Who is scraping?,Rank,University name,Wikidata ID,students total,undergraduates,postgraduates,Carnegie,coordinate location,Location,State,info school name,official website,URL scraped from,Generative AI statement text,Notes,Part of the uni
Ashwini,1,Princeton University,Q21578,"8,478 (Fall 2021)","5,321 (Fall 2021)","3,157 (Fall 2021)",R1,"40.345277777778,-74.656111111111",Princeton,NJ,,https://www.princeton.edu/,,"Our existing academic integrity regulations govern how students may use generative AI. As you know, Princeton requires students to state the work they submit in a course is original and only their own (RRR 2.4.3).  Just as students may not turn in someone else’s work as their own, students may not misrepresent as their own work any output generated by or derived from generative AI.

Students who’ve received permission to use generative AI must follow the requirements for acknowledging sources in academic work (outlined in RRR 2.4.6) and must adhere to the scholarly standards for an explicit statement of method.  Students who don’t follow these regulations will be subject to disciplinary action.

            We want to also clarify that, following our current policies about outside tutoring (RRR 2.4.5), undergraduates may not use AI tutoring bots or any tutoring service that isn’t authorized by the Office of the Dean of the College. 

We believe that detection and surveillance tools are not an effective means to identify or deter the use of generative AI.  They’re not reliable, and they appear to be biased.  We don’t recommend that faculty use these tools.

Instead, we hope that instructors and advisers will engage students in active conversation about the importance of academic integrity, original work, and source acknowledgment to Princeton’s academic mission.

We believe that the powers and risks of generative AI should only deepen the University’s commitment to a liberal arts education and the insistence on critical thinking it provides.  We will continue to assess the development of generative AI and will generate more comprehensive guidance as the year progresses.",,
Ashwini,3,Harvard University,Q13371,"21,613 (Fall 2022)","7,240 (Fall 2022)","14,373 (Fall 2022)",R1,"42.374443888889,-71.116943888889",Cambridge,MA,,https://harvard.edu,,"Generative AI is a type of artificial intelligence that can learn from and mimic large amounts of data to create content such as text, images, music, videos, code, and more, based on inputs or prompts. The University supports responsible experimentation with Generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data
 privacy, compliance, copyright, and academic integrity.
AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”) or may contain copyrighted material. You are responsible for any content that you publish that includes AI-generated material.

You should not enter data classified as confidential (Level 2 and above, including non-public research data, finance, HR, student records, medical information, etc.) into publicly-available Generative AI tools, in accordance with the University’s Information Security Policy. Information shared with Generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.
Level 2 and above confidential data must only be entered into Generative AI tools that have been assessed and approved for such use by Harvard’s Information Security and Data Privacy office. 
Review your School’s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using Generative AI tools. In the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of Generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.

Generative AI has made it easier for malicious actors to create sophisticated phishing emails and “deepfakes” (i.e., video or audio intended to convincingly mimic a person’s voice or physical appearance without their consent) at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu.

Additionally, the University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds.

 If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu or read more about tools currently available or coming soon.
 Vendor generative AI tools must be assessed for risk by Harvard's Information Security and Data Privacy office prior to use.",,
Ashwini,3,Stanford University,Q41506,"17,246 (Fall 2021)","7,858 (Fall 2021)","9,388 (Fall 2021)",R1,"37.4275,-122.17",Stanford,CA,,https://www.stanford.edu/,,"The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.  

While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives.

To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework:

Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt.

Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification.

The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy.",,
Ashwini,5,Yale University,Q49112,"12,060 (Fall 2020)","4,703 (Fall 2020)","7,357 (Fall 2020)",R1,"41.311111111111,-72.926666666667",New Haven,CT,,https://yale.edu,,"Publicly available generative Artificial Intelligence (AI) tools such as ChatGPT, Bing, Midjourney, and Bard have garnered tremendous attention in the past year. The field of generative AI is developing rapidly. While its precise impacts are unknown, this technology will transform how we learn, teach, conduct research, and carry out daily tasks. We encourage you to experiment with AI tools. As you explore AI’s potential, please adhere to the following general guidelines, which align with existing university policies and uphold our institutional commitment to safety, security, and academic integrity. 
 

1.    Protect Yale’s confidential information and your own. Do not enter confidential or legally restricted data or any data that Yale’s data classification policy identifies as moderate or high-risk into an AI tool.  If you are not sure whether you should share certain data, please review Yale’s data classification policy. 
 

2.    Assume all information shared will be made public. Treat all information shared with an AI tool as if it will become public. Do not share information that is personal or sensitive, and be mindful that the information you input into an AI tool may be retained. 
 

3.    Always follow academic integrity guidelines and institutional standards of conduct. All students and faculty are expected to know and adhere to their school’s academic integrity policies. Faculty members are expected to provide clear instructions on the permitted use of generative AI tools for academic work and requirements for attribution. Likewise, students are expected to follow their instructors’ guidelines about permitted use of AI for coursework.
 

4.    Be alert for bias and inaccuracy. AI-generated responses can be biased, inaccurate, inappropriate, or may contain unauthorized copyrighted information. We are each responsible for the content of our work product. Always review and verify outputs generated by AI tools, especially before publication.
 

5.    Protect yourself and your credentials. Never share your Yale NetID and password with AI tools, and always be aware of phishing schemes. For information, tips, and toolkits on cybersafe practices, visit Yale’s Cybersecurity website, which also includes information about security policies and standards. 
 

6.    Seek support. The university is working to support procurement practices that coordinate shared interests and minimize institutional risk. If you are considering acquiring an AI product, please conduct an initial review of the tool to ensure that it conforms to institutional security requirements. Use Yale’s purchasing intake portal or contact purchasing.helpdesk@yale.edu. 

  
Generative AI is evolving quickly, as is institutional support for utilizing these tools. For up-to-date recommendations on teaching and learning, please visit the Poorvu Center’s webpage on “AI Guidance.” Detailed AI guidelines for staff are available on Yale’s data governance website. 

As a university community dedicated to exchanging ideas, disseminating knowledge, and fostering a climate for breakthrough discoveries, we will embrace technological tools and harness their power for innovation. We must do this safely and responsibly. We thank you for your efforts in adhering to these guidelines.",,
Ashwini,6,University of Pennsylvania,Q49117,"23,374 (Fall 2022)","9,760 (Fall 2022)","13,614 (Fall 2022)",R1,"39.95388888888889,-75.19305555555556",Philadelphia,PA,,https://www.upenn.edu/,,"Penn embraces innovations like generative artificial intelligence (AI) models in teaching, learning, research, and the effective stewardship of Penn’s resources.  To this end, this document provides guidelines for members of the Penn community who are using, or interested in using, AI in pursuit of Penn’s mission.

This document is scoped to generative AI, using large language models provided by third parties. Generative AI describes algorithms, such as ChatGPT and other large language models, that can be used to create new content, including text, code, and simulations.

This statement is not intended as legal advice or an exhaustive set of best practices and should not be viewed as a final policy. The AI field is rapidly evolving in terms of technology, deployment models, third-party relationships, terms of service, regulatory landscape, and academic-industry partnership structures. It is anticipated that this document will be updated regularly and interact with other sources of policy, ethics, and governing legal authority.

General Guidance for Penn Community (Educators, Staff, Researchers, and Students)
Transparency. Be transparent about the use of AI. Disclose when a work product was created wholly or partially using an AI tool and, if appropriate, how AI was used to create the work product.

Accountability. The user of AI should endeavor to validate the accuracy of created content with trusted first-party sources and monitor the reliability of that content. Users are accountable for their use of content created by AI and should be wary of misinformation or “hallucinations” by AI tools (e.g., citations to publications or source materials that do not exist or references that otherwise distort the truth).

Bias. When using AI, keep in mind that these tools are often trained on large, unmoderated bodies of text, such as text posted to the internet. This can result in the production of biased and other unintended content. The ability to avoid such biased content is still in the early stages of development.

Privacy & Contracts. Most AI tools and services use input and data from users of the tool to train the model. Additionally, existing tools may incorporate AI features in their service offerings. For this reason, users of AI should avoid sharing personal or sensitive data with the tool and should not input moderate or high-risk Penn data, as defined by the Penn Data Risk Classification, or intellectual property, without: careful consideration and understanding of the tool’s use of Penn data and the service provider’s stated rights to the data, including, but not limited to whether the service provider offers the option to opt-out of using customer’s data to train the AI; a contract in place to protect Penn data; and review by Penn’s Privacy Office and consultation with the Office of Information Security as coordinated by Procurement when moderate or high-risk data is involved. Consultation with the Penn Center for Innovation, where intellectual property is involved.

Patient Privacy Protection. It is not permissible under the Health Information Portability & Accountability Act (HIPAA) or Penn Medicine policy to share patient or research participant information in connection with open or public AI tools and services, such as ChatGPT. This is because, as currently configured, such open or public tools and services can use and share any data without regard to HIPAA restrictions and other protections. Therefore, individual patient data and patient data sets (even if de-identified) may not be exposed to open or public AI tools or services, absent institutional approval.  

Security. When using AI to write computer code or when creating new technology that leverages AI, it is important to be aware of the new kinds of cyberattacks that are being used against AI users.  Review the Office of Information Security guidance on these risks or consult with the Office of Information Security if in doubt.

Data Scraping. The rise of AI models has led to a significant increase in individuals and organizations scraping (i.e., copying) information posted on the internet for the purpose of training new AI models.  Be aware that any data posted publicly will likely be scraped and used in this way by third parties.  Similarly, while these practices are common, their legality and the potential consequences of these actions are currently being developed but remain unresolved at the time this guidance was issued.

Intellectual Property. Members of the Penn community should adhere to established principles of respect for intellectual property (IP), particularly copyrights when considering the creation of new data sets for training AI models.  Avoid uploading confidential and/or proprietary information to AI platforms prior to seeking patent or copyright protection, as doing so could jeopardize IP rights.

University Business Processes. While automating tasks using AI may improve operational efficiency for University business processes, oversight and review of the use of AI and verification of its outputs for these University business processes should be in place to ensure reliability, consistency, and accuracy.

For more information, including additional guidance for educators, students, and researchers, as well as FAQs for AI guidance, visit https://www.isc.upenn.edu/security/AI-guidance.
",,
Ashwini,7,California Institute of Technology,Q161562,"2,397 (2021–22)",987 (2021–22),"1,410 (2021–22)",R1,"34.1375,-118.125",Pasadena,CA,,https://caltech.edu,https://www.imss.caltech.edu/services/ai,"Guidance on the Use of Generative AI and Large Language Model Tools 
Caltech provides this initial guidance to encourage the responsible use of generative artificial intelligence (GenAI) and large language model (LLM) tools and technologies, such as OpenAI's ChatGPT and Dall-E and Google's Bard in research, education, and administrative work at Caltech.

As a research and education institute, committed to advancing the frontiers of science and engineering and expanding knowledge, we support a responsible, measured experimentation with and use of new technologies. While doing this, however, Caltech requires that you follow all existing applicable regulations and Institute policies. These include, but are not limited to, ensuring protection of confidential, personal, or business information and intellectual property, and adherence to the honor code, course requirements, research integrity, and publication ethics.

GenAI and LLM technologies have evolved rapidly in their use and application this past year and are expected to continue to evolve in ways society cannot predict. Likewise, our guidance for the appropriate use of these tools is written for the present moment and will likely evolve alongside the technology. In the interim, however, as you use GenAI and LLM technologies in your work at Caltech, we ask that you apply these four guiding principles to your practice: disclosure, data and information protection, content responsibility, and Caltech's honor code.

Disclosure: When using GenAI, always disclose promptly, or reference the use of GenAI tools and application plug-ins, as applicable. This transparent disclosure ensures that others are aware when GenAI was used to generate content and reduces misunderstandings regarding the source of information, potentially limiting claims of academic dishonesty or plagiarism. When using GenAI to write or publish, please make sure to follow the guidance provided by the course instructor or journal or manuscript publisher/editor. For example, some may require that the GenAI be included as an author, others may simply require acknowledgement.
Data and Information Protection: Federal, state, and local laws as well as Caltech policies may limit data that can be disclosed. Unless you are using a GenAI application that ensures separation of your entry from other entries and confidentiality (usually a paid service), uploading content into GenAI (Open GenAI), is a public disclosure. It is safest to assume data or queries uploaded into Open GenAI tools will become public information, unless otherwise indicated.

In order to protect Caltech data and information, do not enter, contribute, or otherwise input sensitive, confidential, or restricted information into open GenAI tools. This includes, but is not limited to, data covered by regulations such as FERPA and HIPAA, any intellectual property or unpublished research data, export-controlled data, and other sensitive HR, business, or administrative data. Caltech is considering a subscription to a restricted GenAI and will keep you apprised of its progress in securing such a service. In the meantime, Caltech has and continues to reserve the right to disable or limit access to AI companion tools in enterprise business software and applications, such as Zoom and Microsoft Office suites.
Content Responsibility: Remember that GenAI systems are fallible. Responses can be inaccurate, misleading, and even entirely fabricated. Therefore, you should always review and assess all output generated by GenAI tools for accuracy before relying on them or distributing them publicly.
Honor Code: Caltech's honor code underscores the importance of ethical conduct and fairness and extends to the use of GenAI tools and is stated as follows: ""No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.""
Please note that we offer these guidelines in addition to the teaching resources that have already been provided by the Center for Teaching and Learning and Outreach.

The Institute's guidance promotes responsible and ethical use of GenAI and LLM tools at Caltech, and fosters a community that values transparency, integrity, privacy, accuracy, and fairness. Caltech may update its guidance as the technology and regulatory and commercial landscapes evolve. Thank you for adhering to these guidelines. If you have any questions about the use of this dynamic technology, please email gen_ai@caltech.edu.",,
Ashwini,7,Duke University,Q168751,"16,780 (fall 2021)","6,789 (fall 2021)","9,991 (fall 2021)",R1,"36.001111111111,-78.938888888889",Durham,NC,,https://duke.edu/,,"We suggest that faculty make it clear to students what their expectations are regarding the use of AI at the outset of the particular course. If AI is banned outright, explaining how the use of AI can constitute cheating or plagiarism is a must. If you allow AI, be sure to differentiate between acceptable vs. unacceptable use and proper citation. For example, you might allow the use of AI for generating early ideas and drafts and explain to students how to track changes as they edit the original AI text, but you might choose to expressly forbid the use of AI to develop other written assignments.You have the discretion to define how, if, and when generative AI may be used in your courses. For any assignments that are meant to be original work, you can ban the use of generative AI. Sample language might be “If an assignment requires you to use your own critical thinking, solve problems, or practice concepts or skills, do not use generative AI.” You may choose allow students to use generative AI on assignments if instructor permission is obtained in advance and with proper citations. The Duke Community Standard has been updated to include the unauthorized use of generative AI as a form of cheating. Updated on October 30, 2023 to reflect the changes in the Duke Community Standard.",,
Ashwini,9,Brown University,Q49114,"10,425 (Spring 2023)","7,125 (Spring 2023)","3,300 (Spring 2023)",R1,"41.82611111111111,-71.40305555555555",Providence,RI,,https://www.brown.edu/,,"Generative AI tools, such as chatbots (ChatGPT, Bing Chat) and art systems (Midjourney, DALL-E), have great potential for university research.  Brown investigators contemplating using generative AI tools in their own research should be cognizant of the various intellectual property (“IP”) issues related to its use.  

Technology typically outpaces policy and law, and generative AI (“GenAI”) is a particularly dynamic area of technology development.  The guidance on the intersection of generative AI and IP is an evolving area.  Any questions regarding the use of generative AI in your specific research program related to IP may be directed to  tech-innovations@brown.edu. 

Issues Arising from the Use of Generative AI
A. Public Disclosure/Confidentiality.
Public Disclosure. Patentability.  Any given GenAI may not provide sufficient protections to ensure data privacy or confidentiality.  Make sure the GenAI has terms of use stating that any data you provide and your results will not be made public, bearing in mind that any cloud-based GenAI can be vulnerable to public disclosure.  Public disclosure of an invention can preclude Brown’s ability to secure patent rights for that invention. 

Misappropriation of Research Results.  Make sure the GenAI terms of use prevent the GenAI company from using your research results for its own commercial purposes. Brown researcher’s data and results are typically considered confidential and proprietary until a decision is made to publish a manuscript. 

B. IP Ownership.  
Ownership issues pertaining to three distinct types of properties are addressed:  researcher developed generative AI models (the software algorithms designed to “learn” from training sets and generate outputs); researcher-curated training sets (the large data sets “learned” by GenAI systems); GenAI output results.  

Ownership of generative AI models.  Brown researchers may be involved in developing novel GenAI software and algorithms.  As a reminder, software source code is protectable by copyright and algorithms may be protectable by patents. 

Ownership of Brown Researcher-Curated Training Sets.  Often, the value of a GenAI tool is derived from its training sets that yield superior results. BTI works with researchers to protect proprietary rights in commercially valuable training sets, while preserving the ability to make it available for nonprofit research. 

Ownership of GenAI Results.  Make sure the GenAI tool has terms of use that state that the user will own output results.  Not all GenAI tools have terms of use that are clear or sufficient to ensure ownership. 

Infringement Risks in GenAI Results.  Safe practice is to only use GenAI systems that disclose the sources of its training sets.  Many GenAI tools do not disclose their training set sources and may contain material scraped from the internet without permissions, which amounts to copyright infringement. The output results of such GenAI tools may contain or reflect copyright infringement.",,
Ashwini,9,Johns Hopkins University,Q193727,"30,549 (2022)","5,318 (2022)","25,231 (2022)",R1,"39.32888888888889,-76.62055555555555",Baltimore,MD,,https://www.jhu.edu/,,"As the availability of generative AI grows and the quality of these tools improves, student and faculty perspectives on the use of generative AI become more diverse—with some expressing academic integrity concerns and others advocating for thoughtful ways to incorporate these tools into learning. The Whiting School of Engineering (WSE) recommends communicating with students about their perspectives and the various approaches that can be taken when using these tools in higher education. By engaging in these discussions, collectively, we can learn more about diverse perspectives and better understand the benefits and drawbacks of generative AI, whether it is permissible to use in your courses, and how these tools support or hinder learning and achievement of course objectives.

Policies in Higher Education
To address these issues, provide explicit guidelines within course syllabi that clearly outline whether and how AI tools can be incorporated into activities or assignments. If applicable, students should also be informed about the appropriate way to acknowledge the use of such tools in their submitted work. 

 Example syllabus guidelines: 

Students are encouraged to explore and experiment with generative AI tools for learning purposes, but any use in assessments must be clearly indicated and appropriately attributed.
The use of generative AI tools is strictly prohibited in all assessments to ensure fair evaluation of individual student performance.
Generative AI tools may be used in activities and assignments, but students must include a reflection on their use and how it impacted their understanding of the material. ",,
Ashwini,9,Northwestern University,Q309350,"23,410 (Fall 2021)","8,816 (Fall 2021)","14,684 (Fall 2021)",R1,"42.054853,-87.673945",Evanston,IL,,https://www.northwestern.edu/,,"In most cases, the data you share as part of your queries in generative AI tools will be accessible by others using the same tools. This is because generative AI learns by collecting, analyzing, and storing user-provided information. Therefore, University faculty, staff, students, and affiliates should not enter institutional data into any generative AI tools that have not been validated by the University for appropriate use and have explicit permission of the data provider.If your data is Level 1 (non-confidential and public data), uploading it to generative AI tools is permissible. To process data above Level 1, any generative AI tool must have been approved through Northwestern IT’s procurement and security review processes.At this time, the University has not approved additional applications or services for use at Northwestern with Level 2 and above data; however, several (including Bing Chat Enterprise and Microsoft Co-Pilot) are being evaluated for safety, security, and supportability. This page will be updated as products are approved for use.Use of generative AI for teaching and learning purposes is governed by the Provost’s Committee on Generative AI, in tandem with Northwestern IT. Guidance on generative AI tools and the impact on teaching and learning can be found on the Office of the Provost website.",,
Ashwini,12,Columbia University,Q49088,"34,782 (Fall 2022)","6,668 (Fall 2022)","25,880 (Fall 2022)",R1,"40.8075,-73.961944444444",New York,NY,,https://www.columbia.edu/,,"The use of generative AI without faculty permission will be considered a violation of the CBS Honor Code. Suspected violations of this nature will be reported to Student Conduct in the Center for Student Success and Intervention (CSSI). 

The use of generative Artificial Intelligence (AI) tools to complete an assignment or exam is prohibited unless students have a written statement from the course instructor granting permission. Unauthorized use of AI shall be treated similarly to unauthorized assistance and/or plagiarism and is subject to Dean’s Discipline. ",,
Ashwini,12,Cornell University,Q49115,"26,284 (Fall 2023)","16,071 (Fall 2023)","10,207 (Fall 2023)",R1,"42.447222222222,-76.483055555556",Ithaca,NY,,https://cornell.edu,,"As Cornell continues to explore artificial intelligence (AI), particularly generative AI, we are providing some preliminary guidelines for using these rapidly evolving technologies in ways that uphold our core values of purposeful discovery and free and open inquiry and expression.

This communication summarizes the spirit of more extensive and formal information, regularly updated on Cornell’s new general webpage about AI and including links to reports of university committees.

Generative AI, offered through tools such as ChatGPT, Claude, Bard, Bing AI and DALL-E, is a subset of AI that uses machine learning models to create new, original content, such as images, text or music, based on patterns and structures learned from existing data. 

Cornell’s preliminary guidelines seek to balance the exciting new possibilities offered by these tools with awareness of their limitations and the need for rigorous attention to accuracy, intellectual property, security, privacy and ethical issues. These guidelines are upheld by existing university policies.

Accountability: You are accountable for your work, regardless of the tools you use to produce it. When using generative AI tools, always verify the information for errors and biases and exercise caution to avoid copyright infringement. Generative AI excels at applying predictions and patterns to create new content, but since it cannot understand what it produces, the results are sometimes misleading, outdated or false.

Confidentiality and privacy: If you are using public generative AI tools, you cannot enter any Cornell information, or another person's information, that is confidential, proprietary, subject to federal or state regulations or otherwise considered sensitive or restricted. Any information you provide to public generative AI tools is considered public and may be stored and used by anyone else. 

As noted in the University Privacy Statement, Cornell strives to honor the Privacy Principles: Notice, Choice, Accountability for Onward Transfer, Security, Data Integrity and Purpose Limitation, Access and Recourse.

Use for education and pedagogy: Cornell is encouraging a flexible framework in which faculty and instructors can choose to prohibit, to allow with attribution or to encourage generative AI use. In addition to the CU Committee Report: Generative Artificial Intelligence for Education and Pedagogy delivered in July 2023 and resources from the Center for Teaching Innovation, check with your college, department or instructor for specific guidance. 

Tools and use for research, administration and other purposes: By the end of 2023, Cornell is aiming to offer or recommend a set of generative AI tools that will meet the needs of students, faculty, staff and researchers, while providing sufficient risk, security and privacy protections. 

The use of generative AI for research and administration purposes must comply with the guidelines of the forthcoming reports from the university committees for research and administration. The reports are scheduled to be published by the end of 2023.",,
Ashwini,12,University of Chicago,Q131252,"18,452","7,559","10,893",R1,"41.789722222222,-87.599722222222",Chicago,IL,,https://www.uchicago.edu/,,"AI-generated content may be misleading or inaccurate. Generative AI technology may create citations to content that does not exist. Responses from generative AI tools may contain content and materials from other authors and may be copyrighted. It is the responsibility of the tool user to review the accuracy and ownership of any AI-generated content.
For guidance on how generative AI tools intersect with academic honesty, it is recommended that instructors contact the Chicago Center for Teaching and Learning. (See Academic Honesty & Plagiarism in the Student Manual for University policy.)
Generative AI systems, applications, and software products that process, analyze, or move confidential data require a security review before they are acquired, even if the software is free. This review will help ensure the security and privacy of University data. Please contact IT Services at itrisk@uchicago.edu before acquiring or using any tools, add-ons, or modules that include generative AI technology with University confidential data, even if they are free. For more information, see the Policy on the Use of External Services and the Policy of Procurement and Engagement.  ",,
Ashwini,15,"University of California, Berkeley",Q168756,"45,307 (Fall 2022)","32,479 (Fall 2022)","12,828 (Fall 2022)",R1,"37.87,-122.259",Berkeley,CA,School of Information,https://www.berkeley.edu,,"With the emergence of ChatGPT and other AI tools, many members of our community are eager to explore their use in the university context. This advisory provides guidance on how to use these tools safely, without putting institutional, personal, or proprietary information at risk. Additional guidance may be forthcoming as circumstances evolve.

Allowable Use:
Publicly-available information (Protection Level P1(link is external)) can be used freely in ChatGPT. 

In all cases, use should be consistent with UC Berkeley’s Principles of Community(link is external)

Prohibited Use:
At present, any use of ChatGPT should be with the assumption that no personal, confidential, proprietary, or otherwise sensitive information may be used with it. In general, student records subject to FERPA(link is external), and any other information classified as Protection Level P2, P3, or P4(link is external) should not be used. 

Similarly, ChatGPT should not be used to generate output that would be considered non-public. Examples include, but are not limited to, proprietary or unpublished research; legal analysis or advice; recruitment, personnel or disciplinary decision making; completion of academic work in a manner not allowed by the instructor; creation of non-public instructional materials; and grading. 

Please also note that OpenAI explicitly forbids the use of ChatGPT and their other products for certain categories of activity, including fraud and illegal activities. This list of items can be found in their usage policy document(link is external)

Additional Guidance:
For further guidance on the use of ChatGPT for teaching and learning, please see Understanding AI Writing Tools and their Uses for Teaching and Learning at UC Berkeley(link is external) from Research, Teaching & Learning.

Rationale for the Above Guidance:
UPDATE 7/1/2023: The University of California recently renegotiated the UC systemwide agreement with Microsoft to include Microsoft Azure OpenAI. UC Berkeley is currently working with Microsoft to establish how to use this service under UC’s agreement. [Note: For questions regarding the approved use of Microsoft's Azure's Open AI service, please consult with the Privacy Office at privacyoffice@berkeley.edu(link sends e-mail)]

As of September 2023, the University of California's agreements with the parent companies of ChatGPT and other generative AI services, which include our terms and conditions, Appendix Data Security and other privacy protections, do not cover the use of ChatGPT or other similar generative AI services. The UC Office of the President is working on this issue. We hope to see this addressed in the near future and will update this guidance when additional information is available.

Personal Liability: ChatGPT uses a click-through agreement. Click-through agreements, including OpenAI and ChatGPT terms of use, are contracts. Individuals who accept click-through agreements without delegated signature authority may face personal consequences, including responsibility for compliance with terms and conditions. [1]

Guidance on Appropriate Use
For questions regarding the appropriate use of ChatGPT and other AI tools, please contact privacyoffice@berkeley.edu",,
Ashwini,15,"University of California, Los Angeles",Q174710,"47,518 (Fall 2021)","32,121 (Fall 2021)","13,994 (Fall 2021)",R1,"34.07222222222222,-118.44416666666666",Los Angeles,CA,School of Education and Information Studies,https://www.ucla.edu/,,"Students and instructors are embracing ChatGPT (GPT-4 as of March 2023) and similar artificial intelligence (AI) technologies across disciplines for different learning goals. There is no one-size-fits-all best practice for their use. This document is meant as a guideline for instructors on what to consider as these tools evolve. We will provide strategies for adopting AI technologies in a responsible, ethical manner, and innovating within each discipline, major, and course. Exploring and communicating about the opportunities and limitations to using these tools will allow instructors and students to critically think about how knowledge is created.",,
Ashwini,17,Rice University,Q842909,"8,212 (Fall 2021)","4,240 (Fall 2021)","3,972 (Fall 2021)",R1,"29.716944444444,-95.402777777778",Houston,TX,,http://www.rice.edu,,"In the middle of Rice University’s spring recess dates, 43 faculty and staff members returned to campus to fill the Center for Teaching Excellence (CTE). They wanted to hear three of their colleagues — experts in their fields — discuss a new artificial intelligence (AI) model that can respond to natural (human) language questions with conversational-style text to create paragraphs and papers. The panel, co-hosted by the CTE and the Program in Writing and Communication (PWC), was created to open a dialogue about what ChatGPT might mean for students’ writing assignments. 

Vicente?Ordóñez-Román, an associate professor in the Department of Computer Science, opened the discussion. His research interests lie at the intersection of computer vision, natural language processing and machine learning (ML), and his remarks began with a simplified explanation of how this type of text prediction modeling has advanced over time.  

“Essentially, we’ve been developing neural networks or machine learning models to predict the next word in a sentence or phrase for years. Many of the advancements were made before 2013. We saw another breakthrough in 2017. OpenAI has gotten a lot of press for their GPT models, but Google and other companies have been developing similar chatbots and models. The recent announcements about ChatGPT and other similar chatbots are simply the latest iterations.” 

He said improvements to machine translation can now be scaled to even larger models for more sophisticated prediction of the next word, to the extent that large amounts of text can be generated. But the amount of compute power needed to accomplish the task requires supercomputers that are available only to the largest organizations. 

“For someone like me, who works in the ML field, I see ChatGPT from a different perspective,” said Ordóñez-Román. “This did not just ‘come out of nowhere.’ It has been in the pipeline. What is new is the training that incorporates human feedback. And now, the input is in the form of a human asking a specific question. As you can guess — in academia, we are still working on and discussing the validity of these models. How does the model try to understand the question or sets of questions? 

“As it turns out, we’ve developed models that have become really good at chain-of-thought. With instruction training, the models improve further, and we can now create better models with less data. But it is very hard to know from this current state just how much more capable the models will become. For some cases, being right 90% of the time is adequate, but in most instances that is just not good enough. Remember the prediction that everyone would be in self-driving cars by now? We are just not there yet – not with self-driving cars and not with computer generated text to everyday questions – and it is hard to predict how much longer it will take to get these systems really close to 100%.” 

Rodrigo Ferreira also expressed skepticism at popular media stories claiming that chatGPT represents a critical threat to higher education. He is an assistant teaching professor of computer science and his Rice courses focus on technology and ethics.  

“When we – as instructors – feel the learning outcomes for our students are threatened by new technologies, we tend to respond in three ways: either we retreat or we try to challenge or outsmart them. But this is not the first time there has been pushback to new technologies,” said Ferreira. 

“Going back to ~350 BC, in Plato’s Phaedrus we find a critique of writing as a technological invention. In this dialogue, Socrates expresses concern that written text, in contrast to oral speech, can easily ‘escape’ from the author’s intended audience or can even unintentionally obscure his identity. For these reasons, Socrates viewed writing as deficient; it prevented teachers from effectively reaching students, or as Socrates put it, addressing their ‘soul.’  In context of some of these concerns is that when Plato spoke of the Academy he envisioned a place where students and teachers could properly have the time to contemplate, to learn, and critically reflect on social life away from the pressures and demands of traditional vocations.” 

Ferreira contrasted Plato’s vision of the academy against the disciplinary boundaries in most universities today, creating silos between humanities, science, and business schools, and referred to current work that criticizes the ‘uberfication’ of higher education in the United States, where students increasingly tend to view the experience as transactional and universities are increasingly concerned with rankings and both student and faculty metrics.   

“The real challenge with ChatGPT,” Ferreira said, “is not that of a new technology that may help students cheat, but for us to re-think our educational model, where complex social problems are sometimes framed in ways that lead students to think that they can be exhaustively addressed through a simple technological solution. We can’t blame students for wanting to cut corners to achieve an individually-desired outcome, when so much of their learning environment is precisely grounded on that same technologically-solutionistic and individualist mindset.” 

Ferreira then returned to the divided opinions of how faculty should address student use of technologies like ChatGPT. He said, “What if we instead found ways to collaborate and co-create with ChatGPT? Think about encouraging students to bring in their ChatGPT generated-responses to examine and critique together. Rather than pressuring them to perform mastery of the topic in their first draft, the process could be more focused on iterative practice and peer collaboration. Another way to incorporate ChatGPT and other generative AI models is to help develop speculative images and texts, particularly when imagining a different future. This is something that artists, scholars, and activists have been doing for a long time. As an example, for educators aiming to help students communicate the impact of climate change, AI-generated texts and images have been an excellent narrative tool.” 

David Messmer has been teaching a variety of communication courses at Rice since 2009 and was named director of the First-Year Writing Intensive Seminar (FWIS) program in 2018. Stepping to the microphone, Messmer joked about having asked ChatGPT what he should say in a presentation about ChatGPT. Displaying ChatGPT’s responses quickly revealed the limitations of the model’s current iteration and initiated another round of laughter. 

“What I came away with – after asking it what ChatGPT can and can’t do – was a lot less concern than I had going in,” he said. “Students have always had opportunities to cheat; this is nothing new. Remember Cliffs Notes? And Wikipedia is now older than our students. Grammarly and Google – these are all examples of the tools students have turned to and teachers have been encountering for centuries.  

“The vast majority of students are willing to comply with our guidelines, as long as we are clear about our expectations and why we expect those behaviors. Just because OpenAI has released ChatGPT doesn’t mean that honest students will begin to cheat.” 

Messmer said one of the challenges instructors face is the blurred boundaries between what is and isn’t a student’s original work. With ChatGPT, plagiarism is not limited to copying and pasting something students found in a Google search. If a student poses a question to ChatGPT and a dialogue ensues, what is actually their material and what belongs to the Chatbot? He stressed the importance of doing more coaching up front when teaching students critical thinking and writing skills so that they can know the difference. 

“I gave ChatGPT another test, this time to analyze a sonnet by Shakespeare,” said Messmer. “Now the model’s limitations become very apparent. Every paragraph used exactly two quotes, and even some of the same phrases. If I were an instructor reading this submission by a student, I could immediately tell something is wrong. But if I am a student struggling to understand Shakespeare, this ChatGPT paper looks pretty good. This is where we need to coach students on the dangers involved in using technology to do their work. 

“But where do we start? Rodrigo made the point about critiquing ChatGPT responses and I’ll take that a step further. Have the students write an essay on their own and then write the essay using ChatGPT and compare them. This exercise has a lot of value, but also danger because a less experienced student might look at the ChatGPT essay and think it is superior. The students who need to practice writing most are the same ones that a ‘better’ ChatGPT essay does not serve well.” 

Rather than comparing only their written essay with their ChatGPT essay, Messmer suggested having a group critique where the members look at several student-written essays and the corresponding ChatGPT versions. Examining and discussing several papers should reveal that the papers the students wrote are all different, while the papers generated by AI will be more or less the same. 

“When students see – and teachers emphasize – the value of unique perspectives and the validity of original thought, then the students will come to see how ChatGPT has its limitations,” said Messmer. 

The principle underlying this example of group comparison? “If we want the students to practice and improve, then we must value their perspectives and originality. We have to give them assignments and feedback that supports and values their work and their responses.” 

During the question-and-answer session, the speakers gave a few more examples of the importance of doing their own homework. Ordóñez Roman decided he’d like to learn German. He could cheat on his homework and use a translator, but the end result would be never really learning German. Similarly, a medical student is going to have to recall the material they are learning in a real situation.  

Messmer said when a child asked why they needed to learn multiplication when they could just use a calculator, he tried to explain it was a building block for algebra and future classes. Then he stopped and gave a more visual example. 

“If I want to run a marathon, I have to start by running one or two miles over and over again. I can’t do those miles in my car. As long as instructors are clear about what we are trying to accomplish, then our students shouldn’t have to ask why we are doing this.” 

Rice community members who missed the talk can access the video from the CTE ChatGPT event page. Talks like these are coordinated by the CTE staff throughout the year; watch their website for announcements.  ",,
Ashwini,18,Dartmouth College,Q49116,"6,608 (fall 2019)","4,459 (fall 2019)","2,149 (fall 2019)",R1,"43.703333333333,-72.288333333333",Hanover,NH,,https://www.dartmouth.edu/,,"Generative Artificial Intelligence
While there is no official Dartmouth policy on syllabus statements for Generative AI (GAI), we urge you to include a statement regarding GAI in relation to your course. Your statement should communicate clearly when and how GAI may be used by you and the students in your course. We also suggest discussing this policy with your students in class. Recommended topics to address include: 

appropriate ways to use GAI in the course, if it is allowed;
rationale for any limits or prohibition to using GAI in the course;
which types of uses would violate the Academic Honor Principle;
how students should acknowledge and cite use of GAI in their work. 
To learn more about GAI in teaching and learning to inform your course policies and language:

Read our regularly updated document, Guidance for Dartmouth Faculty on Teaching with Generative AI
See Cornell University's committee report on Generative Artificial Intelligence for Education and Pedagogy, including several Appendix examples of how GAI is being used within courses.
Schedule a 1:1 consultation with a member of the Learning Design and Innovation team (ITC) for support developing your generative AI policies and practices",,
Ashwini,18,Vanderbilt University,Q29052,"13,796 (Fall 2021)","7,111 (Fall 2021)","6,685 (Fall 2021)",R1,"36.148649,-86.804972",Nashville,TN,,https://www.vanderbilt.edu/,,"Artificial Intelligence is a valuable, dynamic, and vital tool. As with all other tools, you must abide by all applicable laws and policies when you use AI, and you should strive to use AI ethically. You should follow guidelines (including prohibitions) provided by your dean, instructor, supervisor, or other individual(s) overseeing your work. You should disclose the use of AI in an appropriate way. You should exercise sound judgment in your use of AI. When AI is used, it should be used in ways that are consistent with university policies on confidentiality and privacy. This applies to all aspects of research, service work, and creative expression. You are the author of content that you produce with AI and responsible for its accuracy, impact, and compliance with relevant laws and policies.

PRINCIPLES/GOALS:

Faculty should decide whether and how generative AI is used in courses.
Faculty should clearly communicate expectations to students.
Faculty should clearly communicate what constitutes academic dishonesty.
Students are responsible for understanding the rules of engagement for using AI in each of their courses and seeking out information if they do not understand or are unsure how to comply.
Faculty and students are responsible for using AI appropriately and ethically.
Faculty and students should disclose the use of AI in their work if such disclosure is expected.
Faculty and students are the authors of content generated by AI and are responsible for that content as they are with content that they author.
AI should be used in ways that respect confidentiality and privacy.
AI should be used legally, ethically, and reasonably.",,
Ashwini,20,University of Notre Dame,Q178848,"12,809 (Fall 2022)","8,874 (Fall 2022)","3,935 (Fall 2022)",R1,"41.7,-86.2388888888889",Notre Dame,IN,,https://www.nd.edu/,,"This new technology offers numerous ways to support your education, such as making study guides or flash cards or providing help with understanding difficult concepts.

However, misuse of generative AI impedes the University’s mission to develop the gifts and talents that you bring to the ND community, and using it as a substitute for genuine engagement with your coursework runs counter to the heart of education itself. Think carefully about the difference between supplementing your education and replacing it.

With this in mind, remember that representing work that you did not produce as your own, including work generated or materially modified by AI, constitutes academic dishonesty. Use of generative AI in a way that violates an instructor’s articulated policy, or using it to complete coursework in a way not expressly permitted by the faculty member, will be considered a violation of the Honor Code.

Finally, Undergraduate Education at Notre Dame and the Office of Academic Standards would also like to support you as you navigate the impacts of generative AI on your education and career path. If you have concerns about academic dishonesty, or about generative AI impacting learning in your classes, email honor@nd.edu and let us know.",,
Ashwini,21,"University of Michigan, Ann Arbor",Q230492,"51,225 (2022)","32,695 (2022)","18,530 (2022)",R1,"42.276944444444446,-83.73805555555556",Ann Arbor,MI,The School of Information,https://umich.edu/,,"Following are some things to consider as you use GenAI-based tools, and how it may affect your usage of them in your day-to-day life:
GenAI is not sentient:  GenAI models or Large Language models (LLMs) might appear to possess sentience or self-awareness as a human would, but are simply systems trained on large and biased datasets. LLMs are designed to output the most likely, or most common results possible based on their data, and will invariably tend to suppress less common or marginalized information.
GenAI is biased:  
GenAI models carry implicit biases in them that make them unsuitable for use in cases of ethical deliberation and decision, and should not be used in those circumstances. Furthermore, this data is from the past, which results in a loss of context for current social changes.
GenAI can mislead:  
GenAI in its current stage will tend to ‘hallucinate’ or make up random data that is not true. Models have no real sense of what is true or false. These models are built to output what is most likely in a verbose manner, even if there might not be enough real information to back it up.
GenAI prefers English:  
LLM models are currently heavily biased toward Standard American English. This means that writing styles and dialects adopted by other cultures and ethnic groups such in cases of African American or Indigenous English are at risk of being penalized for a privileged White-dominated form of writing instead.
With that in mind, here are some basic guidelines that can help you in how you use GenAI in academic use:

Use U-M’s offerings that ensure your data privacy and security.
Talk to your professor about how and where GenAI-based tools can be used in your course. Seek clarity on issues of syllabus wording, citation, and methods of use.
Do not cite information from GenAI as the truth for the information it presents. Always check the citations that it provides, and research them yourself. You can also reach out to the Librarian for assistance in your research.
GenAI-based tools are just that, tools that you wield. Your prompts can determine the quality of information that you get and should assist you in your academic growth. They do not and should not replace your ability for critical thinking and problem-solving as an individual.
If you have concerns about how GenAI is being used or limited in a course, or have disputes with the course instructor over the usage of these tools, we recommend first contacting your school or college, as they may be able to assist you in accessing training material specifically curated for your area.",,
Ashwini,22,Georgetown University,Q333886,"19,005","7,463","11,542",R1,"38.907222222222,-77.072777777778",Washington,DC,,https://www.georgetown.edu,," The policy also states that students using AI-generated text and representing it as their own work constitutes a violation of academic integrity. If your students are permitted to use AI tools in your course, in any capacity, be sure to discuss your expectations with them. ""New AI text generation tools may be helpful tools for this course and indeed your future professional lives. Some assignments in this course will require the explicit and intentional use of generative AI. When using and AI generator in your work, including in those assignment, the AI generator used to support your work must be included as a source/citationxNew AI text generation tools may be able to generate ideas/topics for your assignments, formulate structure for your written work, and can even help you find existing research on the topic. These uses are permitted in this course. The writing and revising, however, must be your own; you may not use AI text generators to write any portion of the paper itself, and using AI tools in this way will be treated as a case of plagiarism and referred to the Honor Council. The AI generator used to support your work must be included as a source/citation.",,
Ashwini,22,University of North Carolina at Chapel Hill,Q192334,"31,705 (Fall 2022)","20,029 (Fall 2022)","11,676 (Fall 2022)",R1,"35.908611111111,-79.049166666667",Chapel Hill,NC,School of Information and Library Science,https://www.unc.edu,,"Accuracy: Generative AI may invent both facts and sources for those facts. Verification is your responsibility, whether the source of the error is you or the AI makes no difference. You need to check the facts, the quotes, the arguments, and the logic, and document what you did to validate your material.
Attribution: All ideas that are not originally your own have a source and that source must be attributed. Please be aware that generative AI tends to invent sources. You have a two-fold obligation with respect to attribution:
(1) If a source is identified, find and attribute the original source of the idea, identify the location of the text within the source, and provide a working link to the location (if the source is available online). If you are not able to locate the source, delete that content.
(2) Document the process by explaining how you used generative AI in a work statement that will accompany your submission of major projects in the class. As you submit a project, develop, and include an appropriate version of the below statements:",,
Ashwini,24,Carnegie Mellon University,Q190080,"16,779 (2022)","7,509 (2022)","9,270 (2022)",R1,"40.4425,-79.94333333333333",Pittsburgh,PA,College of Information Systems and Public Policy,https://www.cmu.edu/,,"We want to take a moment to call attention to the rapidly evolving role of generative AI and its place in higher education, especially in relation to academic integrity. Carnegie Mellon has always been a first mover in the AI space and remains deeply engaged in shaping the future of these technologies and their uses. As we all continue to learn about these exciting tools, our goal is to address their challenges and lean into the opportunities they make possible by ensuring you have clear guidance and expectations about this technology. With that in mind, we encourage you to please: 

Review CMU’s existing Academic Integrity Policy, which prohibits ""unauthorized assistance,"" which would include generative AI tools unless explicitly permitted by the instructor. 
Carefully review your course syllabi to understand what is and is not acceptable in each of your courses in terms of using generative AI tools in your coursework.
Talk to your instructors if you have questions about these policies or syllabi to clarify expectations and ensure you are adhering to our highest academic standards.
Refer to Computing Services’ Generative Artificial Intelligence Guidelines and Guidelines for Data Classification to learn more about generative AI best practices. ",,
Ashwini,24,Emory University,Q621043,"15,909 (Fall 2022)","8,155 (Fall 2022)","7,754 (Fall 2022)",R1,"33.791111111111,-84.323333333333",Atlanta,GA,,https://www.emory.edu/,,"AI tools can be effective for some research related activities in well-defined tasks, that could produce un-biased results. For example, AI is being used in a CT to calculate Target zoned for head and neck radiotherapy. The hypothesis is that the end-result will yield more accurate and quicker results.?

AI can also assist researchers with tedious tasks, such as researching data patterns or even literature review?s.  Elicit.org is an AI platform that assist in publication finding for research.?

AI tools can be used to write grants, assist in the peer review process, and other tasks that need repetitive, long, or unbiased review.

AI also brings many challenges for researchers to navigate and avoid:

Data Integrity Concerns? due to –

Lack of data transparency?
Data protection and security? measures
Erroneous data?
Lack of tools to identify erroneous data
Authorship Concerns?

Plagiarism in grant proposals? and publications
Fairness in data selected for AI articles as bias prevention isn’t possible as data diversity is lacking due to the newness of the technology.?
Improper or inadequate citations?
         Funding Risks?

NIH cautions researchers that using AI tools may introduce several concerns related to research misconduct, such as, including plagiarized text from someone else’s work or fabricated citations. Plagiarized, falsified, or fabricated information in a grant write-up, may result in noncompliance with NIH and/or other funding agency policies.
There are several AI tools that can be used to write grants:?
https://www.fundwriter.ai/?
https://www.grantwritingmadeeasy.com/?
https://grantable.co/?
Privacy & Information Security

Open-source AI tools are not Health Insurance Portability and Accountability (HIPAA), General Data Protection Regulation (GDPR), or the Family Educational Rights and Privacy Act (FERPA) compliant.

Inputting data into ChatGPT or similar AI tools is equivalent to disclosing that data to the public and could be considered a violation under FERPA, HIPAA, PCI, GLBA or other federal or state laws.

Organizational confidential and proprietary information are at risk due to the sharing of AI responses in Chatbots. ChatGPT also poses a potential breach of contextual integrity, which dictates that individuals’ information is not revealed outside of the context in which it was originally created.

ChatGPT is NOT HIPAA compliant. Therefore, it is critical not to input any PHI (or any Individually identifiable Health information used for research data) into ChatGPT.

De-identifying or anonymizing data is key to minimizing the risk of a data breach. However, AI models have demonstrated to be particularly adept at re-identification of data subjects even when the source data set was supposedly de-identified in accordance with existing standard.",,
Ashwini,24,University of Virginia,Q213439,"23,721 (Fall 2022)","16,793 (Fall 2022)","6,928 (Fall 2022)",R1,"38.035555555555554,-78.50333333333333",Charlottesville,VA,,https://www.virginia.edu/,,"The myriad uses of generative AI can often seem to offset the potential pitfalls. However, AI content cannot be used uncritically; a thoughtful interrogation of the source material is essential. There are a number of variables to evaluate, including knowledge gaps, currency, and the specific prompt used to generate the content. In addition to the risks of plagiarism and perpetuating misinformation, complex concepts of bias, privacy, and equity should be considered. 

 

Given the abundance of generative AI tools available to explore and use, determining the appropriateness of their use, how to successfully attain a useful response, and whether that response is accurate and appropriate for your needs can be challenging. While we can employ various strategies to evaluate the output provided by a given tool, it's essential to understand where the information is coming from and to have sufficient proficiency in the subject matter to be able to assess its accuracy. Among other things, you should consider whether the information the AI is producing is accurate, if the tool is drawing from a diverse range of data, and monitor the information returned by the tool for bias. Sarah Lebovitz, Hila Lifshitz-Assaf, and Natalia Levina write in the MITSloan Management Review that it is critical to find the ground truth on which the AI has been trained and validated."" (Lebovitz et al., 2023) Digging in further, you can consider who the owner of the AI tool is and determine if that ownership reflect bias in the results. Consider reviewing the resources maintained by the DAIR (Distributed AI Research) Institute. DAIR examines AI tools and issues through a community-rooted lens; maintains a list of publications related to social justice, privacy, and bias; and, conducts research projects free from the influence of Big Tech. ",,
Ashwini,24,Washington University in St. Louis,Q777403,"16,244","7,803","8,441",R1,"38.648,-90.305",St. Louis,MO,,https://wustl.edu/,,"Do not enter confidential or protected data or information, including non-public research data, into publicly available or vendor-enabled AI tools.
Information shared with public AI tools:

Is not considered private.
May be added to the tool’s knowledge base and provided to other users.
Is usually claimed to be the property of the vendor.
These Tools Can be Inaccurate
Each individual is responsible for any content that is produced or published containing AI-generated material.

AI tools sometimes “hallucinate,” generating content that can be highly convincing, but inaccurate, misleading, or entirely fabricated.
It may contain copyrighted material.
All AI-generated content should be reviewed carefully for correctness and cited properly before submission or publication.
Adhere to Current Academic Integrity Policies
Review university, school, and department handbooks and policies.

Schools will be developing and updating their policies as we learn more about AI tools.
Faculty members should teach and advise students about policies on the permitted uses of AI in classes and on academic work.
Students are encouraged to ask their instructors for clarification about these policies.
AI may contribute intentional and unintended forms of plagiarism and falsification of data.
Be Alert for AI-Enabled Phishing
AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to?follow security best practices and report suspicious messages via the Phish Report button in Outlook or to phishing@wustl.edu.",,
Ashwini,28,"University of California, Davis",Q129421,"40,031 (Fall 2020–21)","31,162 (Fall 2020)","8,869 (Fall 2020)",R1,"38.54,-121.75",Davis,CA,,https://www.ucdavis.edu/,,"The use of generative AI writing tools (such as ChatGPT, GrammarlyGO, GPT-3, GPT-4, Elicit, BERT, or others) to support you as a writer (e.g., for brainstorming, finding search terms for research, translating, getting feedback for revising and editing) is allowed in this class. Such use must be properly acknowledged in references, bibliographies, or other formats as required by the instructor and must be discussed in reflective writing (e.g., assignment memos).

Learning to use AI in productive, responsible, and ethical ways to support writing processes is an emerging literacy skill, one that we will begin to cultivate in this course through readings, guided activities, discussions, and assignments. We will review and practice as a class how to properly acknowledge use of AI writing tools and properly cite and reference AI generated content.

As we learn about AI and how AI writing tools can support different aspects of the writing process, we will be guided by two core principles:

Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.
Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. (UC Davis University Writing Program)
 

In principle you may submit AI-generated code, or code that is based on or derived from AI-generated code, as long as this use is properly documented in the comments: you need to include the prompt and the significant parts of the response. AI tools may help you avoid syntax errors, but there is no guarantee that the generated code is correct. It is your responsibility to identify errors in program logic through comprehensive, documented testing. Moreover, generated code, even if syntactically correct, may have significant scope for improvement, in particular regarding separation of concerns and avoiding repetitions. The submission itself should meet our standards of attribution and validation. (University of Michigan)


Generative AI (Artificial Intelligence that can produce contents) is now widely available to produce text, images, and other media. We encourage the use of such AI resources to inform yourself about the field, to understand the contributions that AI can make, and to help your learning. However, keep the following three principles in mind: (1) An AI cannot pass this course; (2) AI contributions must be attributed and true; (3) The use of AI resources must be open and documented. (Sentient Syllabus Project)",,
Ashwini,28,University of Florida,Q501758,"55,211 (fall 2022)","34,552 (fall 2022)","20,659 (fall 2022)",R1,"29.6475,-82.345",Gainesville,FL,,http://www.ufl.edu/,,"While the potential of generative AI in education and the workplace is extremely exciting, integrating this tool into your classroom requires careful consideration. It is essential to be aware of the implications and challenges that come with using generative AI in your course as you work towards building your learning community. 

There are ethical considerations that cannot be overlooked. First, as AI models learn from various datasets, they may inadvertently promote bias present in the data. In addition, AI-generated content may lack factuality and authenticity. Sometimes, generative AI models create facts, called hallucinations, that give the appearance of correctness. Unless the user is knowledgeable about the subject matter, the factual tone that the generative AI models use creates a sense of legitimacy for the false information. It should also be noted that AI-generated content does not acknowledge the original source of the information and struggles with properly citing sources when asked. Finally, privacy is a concern, especially given FERPA (Family Educational Rights and Privacy Act). You would not want to share protected information with a generative AI application as it can store that information as part of its learning process and inadvertently reveal the confidential information through the generated responses. Consent and Data privacy need further consideration, and measures should be taken to inform students about the nature of the technology being used and ensure that their privacy rights are respected. 

The use of generative AI in the classroom creates new challenges in navigating your instructional practice. Whether you choose to integrate, limit, or ban the use in your course, generative AI needs attention in your instructional practice. 

Strategies 
Explore Generative AI 
If you have not already done so, take some time to explore a generative AI. ChatGPT seems to be the more commonly known generative AI; however, a different program may be better aligned with your course. Check out this list of generative AI tools because you know your students are exploring these tools as well. 

Start Strong from Day 1 

Start strong by making sure your syllabus uses learner-friendly language and clearly states your policy for using generative AI in your course.  
Consider integrating an academic integrity quiz that includes language about using generative AI into your first day module. 
Sample academic integrity quiz language 
The Student Centered AI Syllabus 
Plan time to talk with your students about the use of generative AI in your course. Include an interactive component such as a demonstration or brief student task to encourage a more robust conversation on the strengths and weaknesses of using generative AI in your field. If you are choosing to integrate the use of generative AI in your coursework, consider continuing the conversation throughout the semester. 
Incorporate Guest Lectures and Industry Perspectives 

Invite guest speakers, including AI researchers, industry professionals, and fellow instructors who have experience with generative AI in your field to share their expertise and professional applications of AI in the field. 

Citing the use of Generative AI in coursework 

Include a resource section to help students understand how to properly cite the use of generative AI in their assignments. You might want to consider the requirement of including a statement in the assignment indicating the use of generative AI in completing the assignment. 

Example of language to be included in coursework completed with the assistance of generative AI:

“I, [student’s full name], hereby declare that in the completion of this assignment, I have utilized generative artificial intelligence (AI) as an academic tool. The content generated by the AI has been properly cited and attributed to [specific AI tool used]. However, I affirm that the overall completion of this assignment reflects my understanding, critical thinking, and creativity, and that the ideas presented are my own. I understand and acknowledge the college’s policy on academic integrity and attest to the compliance with the guidelines set forth in this policy. Any violations of this policy will result in appropriate disciplinary action.” 
Citing ChatGPT in documents: University of Northwestern Information Guide, AI, ChatGPT, and the Library 
Chicago Manual of Style Guide for citing ChatGPT: Citation, Documentation of Sources ",,
Ashwini,28,University of Southern California,Q4614,"49,318 (2021)","20,790 (2021)","28,528 (2021)",R1,"34.021667,-118.285278",Los Angeles,CA,,https://www.usc.edu/,,"We suggest that that would be akin to standing on the
shore hoping to block a rising tide. Generative AI is here and is not going away. That said, if you
wish to discourage student use of generative AI, let your students know this expectation
both in your syllabus and in class. The guidelines listed at the beginning of this document
should serve as a good starting point.
You may also consider adapting and adopting something like Science Journals' artificial
intelligence (AI) policy:
Text generated from AI, machine learning, or similar algorithmic tools cannot be
used in papers published in Science journals, nor can the accompanying figures,
images, or graphics be the products of such tools, without explicit permission
from the editors. In addition, an AI program cannot be an author of a Science
journal paper. A violation of this policy constitutes scientific misconduct.
Another approach is Nature Journals' large language model guidelines:
Large Language Models (LLMs), such as ChatGPT, do not currently satisfy our
authorship criteria. Notably an attribution of authorship carries with it
accountability for the work, which cannot be effectively applied to LLMs. Use of
an LLM should be properly documented in the Methods section (and if a Methods
section is not available, in a suitable alternative part) of the manuscript.
In class, you can discourage student use of generative AI by encouraging students to use other
tools and techniques instead. In fact, many of the teaching and assessment techniques
recommended by the CET – ask more nuanced questions, have students complete assignments
and assessments during class, require students to submit drafts, augment written papers with
other activities that demonstrate students’ content knowledge – work equally well if you want to
embrace and enhance or discourage and detect student use of AI generators.
However, we do not consider requiring handwritten assignments to be an effective technique to
discourage or detect. Students who have academic accommodations may need to use assistive
technology in your class. Prohibiting student use of technology or requiring that all students
handwrite their work may create a situation that singles out students with accommodations if
they can use technology while others cannot.",,
Ashwini,32,University of Texas at Austin,Q49213,"52,384 (Fall 2022)","40,916 (Fall 2021)","11,075 (Fall 2021)",R1,"30.28614,-97.73942",Austin,TX,School of Information,https://www.utexas.edu/,,"With the emergence of ChatGPT, Bard and other large language model generative artificial intelligence tools, hereinafter collectively referred to as “AI Tools”, many members of our community are eager to explore their use in the university context. This advisory provides guidance on how to acceptably use these AI Tools safely, without putting institutional, personal, or proprietary information at risk. Additional guidance may be forthcoming as circumstances evolve.

Allowable Use:

Data that is publicly available or defined as Published university information (UT Data Classification Standard) can be used freely in AI Tools 
In all cases, use should be consistent with the Acceptable Use Policy.
Prohibited Use:

At present, any use of ChatGPT or similar AI Tools cannot use any personal, confidential, proprietary, or otherwise sensitive information. In general,  student records subject to FERPA, health information, proprietary information, and any other information classified as Confidential or Controlled university data must not be used with AI Tools. 
Similarly, ChatGPT or similar AI Tools must not be used to generate output that would be considered non-public. Examples include, but are not limited to generating proprietary or unpublished research; legal analysis or advice; recruitment, personnel or disciplinary decision making; completion of academic work in a manner not allowed by the instructor; creation of non-public instructional materials; and grading. 
Please also note that the company that owns ChatGPT, OpenAI ,explicitly forbids the use of ChatGPT and their other products for certain categories of activity, including fraud and illegal activities. This list of items can be found in their usage policy. AI Tools of any sort may not be used for any activity that would be illegal, fraudulent or a violation of any state or federal law, or UT Austin or UT System policies.
Additional Guidance:

For further guidance on the use of ChatGPT or other AI Tools for teaching and learning, please see the following guidance from the Center for Teaching and Learning.

Rationale for the Above Guidance:

No UT Agreement, No Privacy and Security Terms: All content entered into, or generated by, ChatGPT is available to ChatGPT, its parent company, OpenAI, and their employees. There is currently no agreement between UT Austin and OpenAI,   Microsoft or other AI Tools that would provide data security and privacy protections required by UT policy with regard to ChatGPT,OpenAI’s , or other AI Tools’ programming interface. Consequently, the use of ChatGPT or other AI Tools at this time could expose individual users and UT to the potential loss and/or abuse of sensitive data and information. 
As of May 2023, the UT Austin Business Contracts Office is working on this issue. We hope to see this addressed in the near future and will update this guidance when additional information is available.
Personal Liability: ChatGPT and other AI Tools use click-through agreements. Click-through agreements, including OpenAI and ChatGPT and other AI Tools’ terms of use, are contracts. Individuals who accept click-through agreements without delegated signature authority may face personal consequences, including responsibility for compliance with terms and conditions [1].",,
Ashwini,33,Georgia Institute of Technology,Q864855,"39,771 (Fall 2020)","16,561 (Fall 2020)","23,210 (Fall 2020)",R1,"33.77580555555556,-84.39469444444444",Atlanta,GA,College of Computing,http://www.gatech.edu/,,"Generative AI tools are unable to do the work of learning for students, though these tools may be able to support that learning. Generative AI tools cannot themselves meet course outcomes—they cannot “develop knowledge of genre conventions” or “develop flexible strategies for generating, revising, editing, and proofreading,” for example. But when used responsibly, transparently, and with appropriate documentation, generative AI tools may offer support and opportunities for students in their learning.

The MLA-CCCC Joint Task Force notes that “critical AI literacy is now part of digital literacy” (11). As teachers of multimodal composition and practitioners of digital pedagogy, we are encouraged to consider—critically—the ways new technologies (such as generative AI tools) can support student learning and communication practices. Minimally, that means helping students consider the ways generative AI tools and AI-generated content fit within existing expectations for academic and professional work—expectations such as providing process documents, citing sources, and reflecting on one’s work. These expectations are expressed in these three principles:

Responsibility: Students are responsible for the work they submit. This means that any work they submit should be their own, with any AI assistance appropriately disclosed (see “Transparency” below)  and any AI-produced content appropriately cited (see “Documentation” below). This also means students must ensure that any factual statements produced by the generative AI tool are true and that any references or citations produced by the generative AI tool are correct. 
Transparency: Any generative AI tools students use in the work of the course should be clearly acknowledged as indicated by the instructor. This work includes not only when students use content directly produced by a generative AI tool but also when they use a generative AI tool in the process of composition (for example, for brainstorming, outlining, or translation purposes). See the Discussing and Using AI Tools in Class document for guidance about transparently disclosing and reflecting on generative AI tool use.
Documentation: Students should cite any content generated by an generative AI tool as they would when quoting, paraphrasing, or summarizing ideas, text, images, or other content made by other people. See the Discussing and Using AI Tools in Class document for guidance about citing AI-generated content as a secondary source, including in recommended MLA or APA formats.
In general, these principles provide a framework for talking about and using AI tools in the context of writing and communication education. See the Discussing and Using AI Tools in Class document for ways to integrate these principles into your courses.",,
Ashwini,33,"University of California, Irvine",Q868421,"36,505 (2021)","30,222 (2019)","5,849 (2019)",R1,"33.64535,-117.84264166667",Irvine,CA,School of Information and Computer Sciences,https://www.uci.edu/,,"Rapidly evolving generative AI tools include text-generating AI chat services (e.g., ChatGPT, Bard, Claude), as well as image-generating, audio, and video AI tools. These machine learning-powered tools are increasingly integrated into our daily communication apps, word-processing software, and technology broadly. In order to best serve our students, we need to engage with generative AI in meaningful ways across the curriculum.

At one level, generative AI tools are no different than any other tools with regard to their impact on teaching decisions. Faculty will need to consider how to leverage tools to benefit student learning, asking question such as:

How can this help me prepare course materials or handle administrative aspects of instruction?
How, if at all, can they help students master my course outcomes?
Where, if at all, in my course should I be explicitly teaching how to use the tool?
Where, if at all, is it appropriate in my course for students to use the tool independent of goals one and two?
Faculty also need to be aware of pitfalls and dangers, such as:

1) How might use of the tool negatively impact students’ learning in the course?

2) What equity and access issues does the existence of the tool raised for my course?

3) How will I address concerns with data privacy breaches, intellectual property protection, algorithmic biases, and ”hallucinations”, situations where generative AI provides false information?

Ultimately, individual faculty will need to make decisions based on the context of their course, course objectives, students’ academic progression, and disciplinary-specific goals of their students’ learning experiences.

One of the challenges faculty have in making these decisions is that the tools are potentially moving faster than research into the use and impact of the tools. However, UCI has significant expertise in this space, and we will continue to update the information as we learn more from research. The following links provide important resources in this space.",,
Kiley,35,"University of California, Santa Barbara",Q263064,"26,179 (Fall 2020)","23,196 (Fall 2020)","2,983 (Fall 2020)",R1,"34.4163232,-119.8463925",Santa Barbara,CA,,https://www.ucsb.edu/,,"On Behalf of the Office of Teaching and Learning - Guidance Regarding AI-Writing Assistance Technologies
May 1, 2023

This message is distributed to Senate Faculty and Unit 18 Faculty. (Click here to view description of distribution groups.)

The following is being sent on behalf of Linda Adler-Kassner, Associate Vice Chancellor of Teaching and Learning.
**************************************************************

Please find guidance regarding AI-Writing Assistance Technologies from the Office of Teaching and Learning pasted below. The information is also attached as a PDF.

Guidance Regarding AI-Writing Assistance Technologies

UCSB Office of Teaching and Learning 

UCSB’s Office of Teaching and Learning recognizes significant opportunities and challenges associated with the widespread accessibility of AI-assistive technologies, including those that assist with forms of composing like writing, coding, drawing, and completing equations. This document is intended to provide guidance about the ethical use of these technologies by both instructors and students. 

1.     Communicate expectations for student use of AI-writing assistive technologies in courses and/or other documents such as theses, dissertations, research articles, etc.

The use of AI writing technologies falls within the purview of the Student Conduct Code and the Student Guide to Academic Integrity. It states that “Materials (written or otherwise) submitted to fulfill academic requirements must represent a student’s own efforts unless otherwise permitted by an instructor.” Therefore, student use of AI-assistive technology for writing is not allowed in courses, on theses, dissertations, research articles, etc. unless specifically allowed by the instructor or supervisor.
Whatever decision you make as an instructor:
Explain “why.” As AI tools become more integrated with commonly used programs (e.g. GoogleDocs), it becomes increasingly important for instructors to explain why and how AI tools should/should not be used. Considerations may include: the accuracy/credibility of AI generated work, potential bias of AI-generated results, developing students’ critical thoughts/voice/skills, etc.
Consider including a policy statement specifically about the use of AI tools (see examples in Sample Language for Syllabus Policies).
See #5 for ideas about how to use LLM/AI in courses.
 
2.     Use AI-Writing Assistance technologies for plagiarism detection, grading and feedback ethically.

UCSB does not support the use of plagiarism detection software (e.g. Turnitin, ChatGPT Zero) for several reasons: 
Anti-plagiarism software is highly fallible. LLMs are advancing at lightning speed with huge injections of capital. Procuring “anti” LLM software contributes to a virtual arms race, with detection software always one step behind what LLMs can produce. 
Submitting student work to anti-plagiarism software may violate students’ intellectual property rights.  When student work is uploaded into a AI-Writing/plagiarism detector database, the student may lose ownership of their work and the instructor/University unable to safeguard how it is shared and used in the electronic commons. 
Use of anti-plagiarism software can undermine the fundamental relationship of trust that must exist between learners and teachers. To move from “detection” of LLM use to “prevention,” instructors can  consider  how students can use LLMs as a tool to support their work  and/or craft assignments and activities that cannot be produced by LLMs (see Incorporating AI-Writing Assistance technologies into courses, below). While this approach may represent a shift in perspective or assignments, Office of Teaching and Learning instructional consultants offer extensive support for instructors who would like to pursue this approach. 
Instructors and TAs should not use AI-assistive technology for grading and feedback unless the technology is supported by UCSB (e.g. use of GradeScope is permitted, as UCSB has a contract for its use and the technology has been vetted for FERPA compliance), for the reasons outlined above.
3.     Report unauthorized student use of AI-assisted writing technologies.

The Office of Student Conduct adjudicates academic and behavioral violations of the Student Conduct Code. If you suspect unauthorized use of AI technologies, submit an incident report. Be sure to include any samples of earlier/baseline student writing to which the writing in question can be compared.
4.              Avoid issues with student use of AI-assistive technologies.

Scaffold writing assignments, so that students are writing smaller pieces that will be incorporated into larger assignments with opportunities to incorporate feedback.
Add brief reflective writing to assignments that ask students to analyze the choices that they made as they compiled the writing (give examples).
Talk with students about the purpose of writing in the course and work with students to use LLMs in productive ways
Assign topics that require personal reflection or creative thinking. For example, ask students to reflect on a personal experience related to the course material.
5.              Consider incorporating AI Writing Assistance technologies into your course 

Create assignments where students use LLMs as part of the writing/thinking activities. You can get inspiration by browsing 101 Creative Ideas to Use AI in Education and other resources under “Ideas for use” at the end of this document.
Stimulate discussions about writing processes, strategies, and ethics through discussions of AI writing technology. Work with your students to generate diverse examples, compare and contrast them with student work, and examine their strengths and weaknesses collaboratively.
Encourage student creativity and curiosity by leveraging AI writing technology to create prompts, topics, or questions for exploration. Challenge students to interrogate how AI writing technology can help them compose pieces across various genres, styles, and perspectives.
Invite students to utilize AI to generate text in specific genres in order to recognize and identify genre conventions and reflect upon the role of audience, purpose, and context in developing rhetorically effective prose.
Encourage students to compare AI-generated text with human-generated text to see how individual agency, voice, and ethos impact text.
Examine the potential for and risks of integrating AI writing tools into the research process, given that LLMs can ""hallucinate"" and generate false facts, statements, or sources. Urge students to cross-check AI-generated information and develop critical appraisal skills to maintain the credibility and precision of their work.
Examine and address potential biases and fairness concerns that may arise from AI writing technology, including the perpetuation of stereotypes or the exclusion of specific perspectives. Promote critical thinking and discussions to recognize and counteract biases in AI-generated content.
Think about your course objectives. What are the cognitive tasks students need to perform without AI assistance? When should students rely on AI assistance? Where can an AI aid facilitate a better outcome? Are new rubrics and assignment descriptions needed?",,
Kiley,35,University of Illinois Urbana-Champaign,Q457281,"53,271 (Spring 2023)","33,186 (Spring 2023)","20,085 (Spring 2023)",R1,"40.110538888889,-88.228411111111",Champaign,IL,School of Information Sciences,https://illinois.edu/,https://www.vpaa.uillinois.edu/digital_risk_management/generative_ai/students/ ; https://www.vpaa.uillinois.edu/digital_risk_management/generative_ai/instructors/,"Generative AI Guidance for Students
Explore Generative AI
By exploring generative AI, students can discover innovative tools and approaches that can spark creativity, bolster critical thinking, enhance problem solving skills, and prepare for future careers. They should:

Keep current on emerging technologies.
Expect to encounter the unknown and to learn iteratively.
Experiment with various generative AI tools to understand affordances and limitations.
Investigate opportunities for enhancing student learning, personalization, and workload reduction to allow for higher level skills and learning activities.
Provide opportunities to explore and learn from each other.
Consider Diversity and Privacy
To ensure an equitable and safe application of this new technology, students should recognize the limitations associated with newly developing technologies.

Understand training data used to create generative AI models may be inherently biased. Students should be aware of this and think critically about results. As we move forward, we should encourage the use of diverse and unbiased data sets used in training models.
Be cautious of entering personal information. Discuss privacy implications.
Protect data and do not enter proprietary information.
Use Generative AI Responsibly
Writing and research are central ways of learning and understanding material in college. The central tenet of academic integrity is doing your own work. As such, tools that you use in the process of learning, writing, and research should be carefully considered.

Talk with your instructors about what is considered responsible use of generative AI in your discipline and in their course.

If you use a generative AI tool in your work, you should cite it. The APA and MLA both provide guidelines for citing generative AI work. Be sure to note how you used it: “I used it this much, in this way, and this percentage of work was generated by AI.” Open AI, the creators of ChatGPT, have a content co-author policy. The key here is to be ethical and not to deceive or mislead your readers.

In working with a generative AI tool, be certain to verify that the citations it generates actually exist. Many people have found that these generative tools invent articles to cite.

Remember to:

Follow only the specific permitted uses set by your instructor.
Document and attribute all generative AI contributions to your coursework.
Take full responsibility for generative AI contributions, ensuring the accuracy of facts and sources.
Think Critically About Generative AI
The output of generative AI models is not always accurate. It may be misleading, inaccurate, biased, and may even be made up. Use critical thinking skills and question the source and quality of training data used in the generative AI model. Carefully evaluate the results (and citations), fact check, determine accuracy/usefulness of output, and engage in class discussions to identify and address biases or inaccuracies.

Guide to Critical Thinking (Checklist, UIUC)

Developing a Critical Thinking Mindset (Course, 58min, LinkedIn Learning)

Critical Media Literacy and Civic Learning: AI Writing Tools, Politics, and History (Edtech Books)

A Beginner's Guide to Critical Thinking (Blog Post, Medium)

Ways to Use Generative AI
There are good and valid ways to leverage the power of generative AI. Here are a few ways:

Use it as part of the revision process to enhance your own work.
Shorten your own text.
Revise your own text for spelling and grammar.
Create study aids (e.g., flashcards) for quizzes or exams.
Test and practice your knowledge of course topics.
Conduct basic research on course and assignment topics.
Use it to help come up with new ideas or expand upon existing ones.
Develop skills related to usage of generative AI as it will add value to your profile for the workplace.
Generative AI can help people communicate, translate, summarize, and more.
Ways Not to Use Generative AI
There are some things to keep in mind when using generative AI for your coursework. Here are a few:

Don’t rely solely on information provided by generative AI as it may be inaccurate, misleading, biased, and even made-up. So can citations provided by generative AI.
Be transparent in your use of generative AI and cite as appropriate.
Don’t use generative AI to replace your critical thinking and problem-solving skills.
Use generative AI to supplement your learning, not to replace it. Generative AI Guidance for Instructors
LEARN
Explore Generative AI
By exploring generative AI, instructors can discover innovative tools and approaches that can improve student engagement, promote personalized learning, and provide valuable insight, ultimately enhancing the educational experience for students. They should:

Keep current on emergent technologies.
Expect to encounter the unknown and to learn iteratively.
Experiment with various generative AI tools to understand affordances and limitations.
Investigate opportunities for enhancing student learning, personalization, and workload reduction to allow for higher level skills and learning activities.
Provide opportunities to explore and learn from each other.
Support Inclusivity, Diversity, and Privacy
To ensure an equitable and safe learning environment, instructors should recognize the limitations associated with newly developing technologies.

Provide equal access to generative AI tools and resources for all students. Beyond the AI models, instructors should promote inclusivity and diversity in the classroom and support all students.
Understand that training data used to create generative AI models may be inherently biased. Instructors should be aware of this and think critically about results. As we move forward, we should encourage the use of diverse and unbiased data sets used in training models.
Be cautious of entering personal information. Discuss privacy implications.
Protect data and do not enter proprietary information.
REFOCUS
Recognize Limitations of AI Detection Tools
Don’t depend on AI detection tools to address the use of generative AI in your assignments. They are unreliable and easy to circumvent with small amounts of editing. They also generate many false positives. If you do use an AI detection tool on student writing, we suggest using at least two AI detection tools and comparing the results. Consider using these results as a conversation starter with students. Falsely accusing students of cheating with generative AI can quickly damage a learning community.

ASU & UCLA have both opted out of TII’s AI detection for now. Others like the University of Kansas caution faculty. The UM System has summarized their concerns and includes videos from TurnItIn on how the detection works. Here’s a Washington Post article on the efficacy of TurnItIn’s detection and how the company views the feature as a means to prompt a conversation with students instead of an accusation.

Adapt Assignments and Assessments
A careful review of your course from the lens of the impact of generative AI will help you know where to focus your attention. Identify assignments at risk for cheating with generative AI tools. Basic essays on fact-based topics and essays in which only the final draft is submitted are at high risk for cheating. Some strategies to revise at-risk assignment prompts include:

Focus on formative assessment and the process of writing (and thinking). Break down an assignment into smaller parts that help students form their ideas on a topic and receive feedback from peers and instructors. Compile these smaller assignments for the final project/essay.
Ask students to analyze, evaluate, or synthesize information obtained from generative AI and ask them how it could be improved, adapted, or enhanced. Students can drill deeper into the subject by refining or asking more specific questions.
Require that students incorporate personal reflections, local or course-based data, or require known specific sources.
Develop authentic assessments: (inquiry-based learning, problem-based learning, scenario-based learning, project-based learning, design-based learning).
Ask students to report on independent research activities.
Require students to adopt a particular moral or ethical stance.
Utilize progressive/reflective portfolio-style assignments that are iterative and episodic.
Create interactive synchronous oral or asynchronous video-based assessments.
Require students to ""show their work"" by submitting drafts or notes or using digital versioning.
Use an honor statement where students certify that work is their own, all sources are correctly attributed, and/or the contribution of any generative AI technologies is fully acknowledged.
TEACH
Establish Framework for the use of Generative AI in your Course
It is important to discuss academic integrity and the use of AI tools with students. The core principle of academic integrity is doing your own work. As such, when students cheat on assignments, they are cheating themselves of the opportunity to learn. The method of cheating — plagiarism, paying for an essay, generative AI tools, and others — doesn’t change the breach of academic integrity.

You can collaborate with your students to create a framework and generate parameters around the use of generative AI in the course. Cover the affordances and limitations of generative AI. Students may not have fully considered the implications of using generative AI. Discuss writing as a process for forming an understanding of topics in your discipline and course.

Be transparent about your learning outcomes and the teaching methods that inform your assignments. What are you hoping students learn through the work? When is using an AI tool required? When is it appropriate and acceptable? When is it out of bounds? How will use of generative AI be cited in classwork?

You may also want to update your academic integrity statement and syllabus to reflect generative AI tools.

Ways to Use Generative AI
There are good and valid ways to leverage the power of generative AI. Here are a few key points to consider:

Generative AI should be introduced in the course emphasizing its strengths and weaknesses.
To prepare students for future workplace exposure and expectations, they should be introduced to and supported in appropriate use of generative AI in their course(s).
Emphasize developing student understanding that generative AI is a tool to enhance problem solving and critical thinking.
Create assignments where students do their own work and then match it with a response from generative AI. Comparison of the two products can further enhance the learning process as students see how and what the generative AI is providing.
Educate students on the need to check the validity and reliability of the citations from generative AI as those citations might not exist or are not reliable.
Ask students to review and acknowledge the Academic Integrity policy of the university.
Ways Not to Use Generative AI
While it may be tempting to ignore the latest developments in artificial intelligence, it is important to recognize that the world as we have known it has changed. We cannot ignore its capabilities and potential. Generative AI is not a replacement for human input, feedback, and guidance. Rather, these advances give us the opportunity to expand our human capabilities and we should embrace it in an informed and hopeful way.",I put both the guidelines for students and the guidelines for instructors in the same cell even though they were on different webpages. Omitted the lists of resources.,
Kiley,35,"University of Wisconsin, Madison",Q838330,"49,886 (fall 2022)","37,235 (fall 2022)","12,651 (fall 2022)",R1,"43.07527778,-89.40972222",Madison,WI,School of Library and Information Studies,https://www.wisc.edu/,https://it.wisc.edu/statement-on-use-of-generative-ai/,"Statement on use of generative AI
Use of ChatGPT, Google Bard and other generative artificial intelligence (AI) tools and services is growing rapidly within higher education, including at UW?–?Madison. Although AI offers new and powerful capabilities for research and education,  it is clear it also poses a potential risk to institutional data that UW?–?Madison is legally and ethically obligated to protect.

University faculty, staff, students and affiliates must not enter institutional data into any generative AI tool or service unless that data is classified as public (low risk). Providing any data to generative AI tools or services as part of a query is equivalent to posting the data on a public-facing website. That is because generative AI “learns” by collecting and storing user-provided data. This data may then be used as output provided to others.

Use of generative AI by UW?–?Madison faculty, staff, students and affiliates is subject to UW?–?Madison, UW System Administration (UWSA) and Universities of Wisconsin Regent policies. Examples of generative AI use prohibited by these policies include, but are not limited to, the following:

Entering any sensitive, restricted or otherwise protected data – including hard-coded passwords – into any generative AI tool or service (see UW-523 Institutional Data and SYS 1031 Data Classification and Protection);
Using AI-generated code for institutional IT systems or services without review by a human to verify the absence of malicious elements (see UW-503 Cybersecurity Risk Management);
Using generative AI to violate laws; institutional policies, rules or guidelines; or agreements or contracts (see Regent Policy 25-3 Acceptable Use of Information Technology Resources).
In addition to violating UW policies, some of the above uses may also violate generative AI providers’ policies and terms.

Researchers who may need to enter higher risk data into a generative AI tool or service as part of their research programs should consult the policies referenced above. For additional guidance, researchers may contact Chief Technology Officer (CTO) Todd Shechter (todd.shechter@wisc.edu).

For uses of generative AI that are not prohibited, UW–Madison faculty, staff, students and affiliates can help protect themselves and others by choosing tools and services that exhibit the National Institute of Standards and Technology’s (NIST’s) characteristics of trustworthy AI.

You can learn more about the benefits and risks of using ChatGPT and other generative AI tools by attending the summer 2023 “Exploring Artificial Intelligence @ UW–Madison” webinar series. Sponsored by the Division of Information Technology (DoIT) and the Data Science Institute, the series aims to provide a platform for experts and visionaries in the field of AI. Speakers will share their insights, research and experiences in the classroom, research lab and wider academic community.

Also instructive is the “Generative AI @ UW–?Madison: Use & Policies” page.

If you have questions about classifying data, contact the relevant data steward.",This page https://it.wisc.edu/generative-ai-uw-madison-use-policies/ has a collection of policies specific to different topics regarding AI usage ; this would be tough to scrape because it's in a table format and the policies are all hyperlinked,
Kiley,39,Boston College,Q49118,"14,890 (Fall 2020)","9,445 (Fall 2020)","5,125 (Fall 2020)",R1,"42.335083333333,-71.170361111111",Chestnut Hill,MA,,https://www.bc.edu/,https://cteresources.bc.edu/documentation/artificial-intelligence-in-teaching-and-learning/,"ChatGPT in Teaching and Learning
There are many artificial intelligence (AI) tools that can generate “human-like” responses to a wide range of questions and statements. Among the most popular generative AI (or GenAI) tools is ChatGPT, a text-based tool that can produce essays, reports, lesson plans, and more. Boston College students will likely use content from these tools in a variety of ways, including as substitutes for their own thinking and writing. Like other technologies that have created new opportunities for academic dishonesty (e.g. Wikipedia, calculators, etc.), ChatGPT invites instructional responses that promote academic integrity and authentic student learning without sacrificing trust in instructor-student or student-student relationships.

ChatGPT’s parent company, OpenAI, has provided some responses to the most commonly asked questions about the tool on its website.

Limitations of ChatGPT
While ChatGPT can produce text that can pass for human-created work, it does have many significant limitations:

ChatGPT does not use any information after 2021. Current events and trends are not a part of its database.
ChatGPT does not access the internet. It relies completely on the information already present in its database.
ChatGPT is not “unbiased.” It is limited by the culturally-specific and language-specific information that was used to train it.
ChatGPT will often make up facts, produce misleading information, and include content unrelated to a user’s question. This is because the technology is constantly changing and adjusting to how users interact with it. The technology has no native way to differentiate fact from fiction.
Faculty Concerns
Faculty have raised several questions and concerns since ChatGPT was released in November 2022:

How will we know if students are submitting original work?
Should we ban the use of ChatGPT and other AI tools?
If we assume students will use ChatGPT, how can the content it produced be cited?
Are there ways that ChatGPT can be used to help students learn?
Can we use ChatGPT for our own course design?
Tools for Detecting AI
Several tools do exist that claim to detect AI-generated content. For-profit services like Originality.AI claim to have 94% accuracy in identifying text produced by ChatGPT. Among the free tools are:

openai-detector
Gltr.io
GPTZero
CrossPlag
While these tools may be helpful in some cases, there are also significant downsides to using these tools:

It is unsustainable for faculty to run every one of their students’ submissions through these services. It is not only an enormous time-consuming process, it is also likely that the services will be inaccurate.
Using these tools creates an adversarial relationship with students wherein they are taught that being caught cheating is worse than choosing to use the service in the first place. 
As of January 2023, it is unclear whether or not there are FERPA violations in submitting student work through these plagiarism detectors. 
Below are alternative instructional responses to ChatGPT, including short- and long-term interventions. 

Instructional Responses to ChatGPT
Instructional responses to the use of ChatGPT and similar tools vary depending on how much time and energy faculty have to make course-level or assignment-level changes. The short-term solutions provided below are meant to serve as an immediate response that should be revisited when time permits. The long-term approaches, on the other hand, are meant to prompt students to think more critically about the technology and their own intellectual formation.   

While the advent of ChatGPT poses new questions to address, strategies that limit academic dishonesty in general remain as relevant and effective as they have been before AI.

Course Design: Short-Term Interventions
Update Your Syllabus
The following sample statements should be taken as starting points to craft your own policy. As of January 23, 2023, the Provost’s Office at BC has not issued a policy regarding the use of AI in coursework. When adding an AI-specific policy to a syllabus, consider how to personalize the policy depending on the norms of each department and course, along with BC’s institutional policies and protocols.

Syllabus Statement 1 (Discourage Use of AI)

Artificial Intelligence (AI) Tool Usage: AI tools can generate text, images, and other media very quickly. Since a central goal of this course is to help you become independent and critical thinkers, you are discouraged from using AI tools to create text, video, audio, or images that end up in your work (assignments, activities, responses, etc). Any work submitted using AI tools will be treated as though it was plagiarized. 

If any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work.  

Note: This statement assumes the syllabus has an academic integrity policy and/or statement about how plagiarized work will be treated. 

Syllabus Statement 2 (Treat AI-generated text as a source)

Artificial Intelligence (AI) Tool Usage: AI tools can generate text, images, and other media very quickly. Since a central goal of this course is to help you become independent and critical thinkers, you are discouraged from using AI tools to create text, video, audio, or images that end up in your work (assignments, activities, responses, etc). 

If any AI-generated content is used for your assignments, you must clearly indicate what work is yours and what part is generated by the AI. In such cases, no more than 10% of the student work should be generated by AI. Any AI-generated work not cited and/or used for more than 10% of your assignment will receive ____. 

If any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work.  

Note: This statement assumes that students are told which citation styles to use for secondary sources. The instructor would indicate the penalty for not following the policy. 

As of January 15, 2023, a regularly-updated list of existing policies for the use of AI tools at various institutions is available online. BC faculty choosing to adapt these policies for their own syllabi may need to edit them in light of BC institutional policies and protocols, as well as departmental and course context.

Discuss ChatGPT With Students
Allocate some time in class to discuss ChatGPT. The conversation can be a way to discuss ethics in education, your expectations, as well as student perspectives. Discussion prompts and questions can include:

What are some reasons students would want to use ChatGPT?
In what ways does ChatGPT impact the course goals? In what ways can AI prevent or enable students in meeting these goals?
What are the differences between plagiarism, remixing, influencing, and originality?
How does the use of Wikipedia compare to the use of ChatGPT?
What is Academic Integrity and why is it so valued at Boston College?
CTE’s resource on how to make Academic Integrity transparent can be a useful guide for these conversations. 

Collect a Writing Sample
Towards the beginning of the semester, ask students to write in class a short version of what you might ask them to write later in the semester. This “diagnostic” can be a source of comparison if a student later submits work that seems significantly different.

Additional Short-Term Course Design Interventions
Additional short-term approaches to course design can be found on Ryan Watkins’ educational technology blog.

Course Design: Long-Term Approaches
Cultivating a learner-centered course climate is fundamental to designing a course that de-incentivizes the use of tools like ChatGPT to cheat (see the CTE’s resource on Underlying Reasons for Academic Dishonesty). In general, the more that assessments can make thinking visible, the less likely a tool like ChatGPT will be able to be used to replace a student’s own problem-solving techniques.

Break Up Major Assignments
Divide major assignments into smaller graded components that build on each other. This “scaffolding” approach requires students to incorporate feedback on earlier assignments to improve their later assignments. 

The CTE’s guidance on assignment design explains how making such changes can positively harness student motivation and deter academic dishonesty.

Distribute the Grading
When students are asked to produce one or two exemplar assignments for a semester, they are often pressured to focus on the product rather than the process of learning. By creating more lower-stakes graded assignments, instructors encourage students to demonstrate learning as a continuous activity. Faculty can see the progress of their students and easily discover how thinking on a subject has evolved.

Assess Student Workload
As indicated in the resource, Underlying Reasons for Academic Dishonesty, one common reason students might use a tool like ChatGPT to cheat is because the overall workload expected of them seems excessive. What is considered excessive, of course, is relative. One good way to measure the workload is by assuming that a student will take at least three times longer to accomplish a task than it would for a faculty member. Rice University has also produced a workload estimator that can be used to measure the time it might take a student to complete certain kinds of tasks.

Assignment Design: Short-Term Interventions
Try running your assignment prompts through ChatGPT itself. If you are finding the responses to be on par for what you would expect, make small changes to make it more challenging for the tool to be used inappropriately, and plan for longer-term adjustments to how the technologies should be engaged, if it all.

Require Recent References
ChatGPT is only able to use information prior to 2021 in its responses. By asking students to use current events, recent newspaper sources, or very recent academic articles, faculty can more reliably assume that the analysis will be the student’s own.

Since ChatGPT does not use materials behind paywalls or require institutional access in its output, faculty may also want to ask students to refer to specific journals or resources that are accessible to them by virtue of being a Boston College student.

Add Reflection to Assignment
Ask students to provide a reflective essay in addition to the paper or exam itself. The objective would be to allow students to show their thinking and provide explanations for why they made the content and stylistic choices they did. Sometimes called “exam wrappers,” these reflections personalize each submission and give students a chance to explain in detail how their thinking led to the product. 

Show Students How to Cite
If allowing students to use ChatGPT to some degree, or assuming students will do so whether it’s allowed or not, demonstrate how to attribute content produced by the AI. 

MLA Guidelines
Assignment Design: Long-Term Approaches
Teach with ChatGPT
Ask students to analyze the output of the AI for a question that could easily be asked as part of an assignment. In the analysis, see if students can differentiate the output with something that a human would produce. For example:

Does the output have a “style” of writing that makes it distinct?
Does the AI rely on clichés or casual speech in a manner that is inappropriate for the topic?
How would a student rewrite the AI output to be more accurate or more distinct?
A thoughtful framework on ways to teach students to write with AI from Glenn Kleiman can be referenced for further reading.

If choosing to teach with ChatGPT, privacy concerns should be discussed openly. See Georgetown’s resource on the topic under the section, Privacy and Data Collection.

Review ChatGPT’s privacy policy before asking students to sign up for the service. Students should be made aware that signing up authorizes ChatGPT to share “Personal Information” with third parties without notice and that they must provide their cell phone number to the service. In addition, ChatGPT’s parent company OpenAI discloses that:

It can access any information fed into or created by its technology
It uses log-in data, tracking, and other analytics
The technology does not respond to “Do Not Track”
Plan for Social Annotation of Text
Tools like Perusall can be enormously useful in not only ensuring students read a given text, but also in prompting critical engagement with the readings. 

If interested in using Perusall, you can review CTE’s resources on Persuall and set up a consultation with us if needed by emailing centerforteaching@bc.edu.

Create Assignments that Require Multiple Modes
Since ChatGPT is text-based, it can only render output that uses traditional sentence structure and syntax. In place of written essays, faculty can ask students to create multimodal submissions: podcasts, posters, mind maps with annotations, short videos, etc.

A very helpful — if overwhelming — resource for alternative assessments can be a useful guide when rethinking assignment types. The “Guide to Alternative Assessments 2.0” PDF is particularly salient when thinking of multimodality.

Future Impact of AI on Teaching and Learning
The strategies suggested above respond to what we know about AI tools today; however, it’s clear that such technologies will improve and so our strategies will have to improve with them.

In the long run, instructional responses that engage the technology and its limits — rather than seek to simply ban them — promise to be more effective ways to meet learning goals across disciplines. Such strategies may also help faculty find new ways to respond to other persistent challenges in higher education, such as:

How can AI tools be used ethically and strategically for our curriculum?
How can they be used to teach students about information literacy, data privacy, and intellectual property?
Can AI be used to promote more equitable learning experiences for students, especially those who have faced structural barriers to resources? See Equity and Academic Integrity for a reference on how accusations of cheating are disproportionately aimed at underrepresented learners.
Should we find more ways to assess the process of learning and not just the product?
The CTE will continue to update this resource with suggestions, strategies, and perspectives that can inform faculty decisions on these questions.",Did not see one for students.,
Kiley,40,Tufts University,Q49120,"12,648 (fall 2021)","6,559 (fall 2021)","6,089 (fall 2021)",R1,"42.406949,-71.11982",Medford,MA,,https://www.tufts.edu/,https://it.tufts.edu/guidelines-use-generative-ai-tools,"Guidelines for Use of Generative AI Tools We are providing initial guidelines on the use and procurement of generative artificial intelligence (AI) tools—such as OpenAI’s ChatGPT and Google Bard—that can generate content in response to prompts. We support responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.
 
Generative AI is a rapidly evolving technology, and we will watch developments and incorporate feedback from the community to adjust our approach as needed.

Initial guidelines for use of generative AI tools

AI use is subject to existing Policies and Procedures: using AI does not allow an exception to existing requirements and limitations.

1.  Protect confidential data

You should not enter data at Levels 2-3 (Pretty much anything that should not be on a public webpage) into unapproved generative AI tools. The approval procedure is described here.  Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. 

2.  You are responsible for any content that you produce or publish that includes AI-generated material

AI-generated content can be false, misleading, or worse—AI sometimes makes up events and facts. In addition, AI can infringe Intellectual Property rights, and your use of the results can be considered infringement—subjecting you to lawsuits and damages. On the other side of the Intellectual Property coin, AI results are not well-protected by Intellectual Property laws. All of these risks require you to read and understand the terms that come along with the AI you are using.

3.  There is almost never “no contract.” 

There may be no charge, but Intellectual Property law requires an End User License Agreement, and most producers will have Terms of Use. These are usually very favorable to producer and very unfavorable to you and Tufts. That said, somebody needs to read them, comprehend the risk, and make a responsible decision to accept it. If you are using this AI for a Tufts matter, you should be reaching out to TTS for a technology review (see No. 5 below).

4.  Adhere to current policies on academic integrity

Review your School’s student and faculty handbooks and policies. For example, misuse of ChatGPT already violates our Policies Regarding Student Behavior.  We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses and required disclosures, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed.

5.  Connect with TTS before procuring generative AI tools

The University is working to ensure that tools procured on behalf of Tufts have the appropriate privacy and security protections and provide the best use of Tufts funds.

If you have procured or are considering procuring generative AI tools or have questions, please contact TTS using these instructions.  
Vendor generative AI tools must be approved by TTS as described here.
It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies.",There is also a page on creating syllabus statements https://provost.tufts.edu/celt/online-resources/artificial-intelligence/ai-syllabus-statements/,
Kiley,40,University of Washington,Q219563,"49,025 (2021)","32,779 (2021)","16,246 (2021)",R1,"47.65416666666667,-122.30805555555555",Seattle,WA,Information School,https://www.washington.edu/,https://teaching.washington.edu/course-design/chatgpt/,"ChatGPT and other AI-based tools
Tools that use artificial intelligence (AI) and large language models to generate text or images are increasingly at the forefront of teaching and learning conversations. Many students now use these tools, sometimes with the encouragement of their instructors, to complete assignments. Other instructors would prefer that students not use AI-based tools to complete assignments.

Because generative AI tools are constantly evolving, it is very difficult to develop technology that can reliably identify when a student uses AI to complete an assignment. Thus, a policing approach to student use of AI has the potential to be both time-consuming and unsuccessful. Instead, it is important to help students understand the issues associated with AI and its relationship to learning in general and your class in particular.

The following strategies can help instructors think about how to communicate with students, set expectations, and design assignments that increase students’ motivation to develop their own skills and ideas.

Set expectations – Establish a policy for your course around the use of AI-based tools (e.g., ChatGPT) and communicate this with students through the syllabus and/or assignment prompts. Discuss how you will proceed if you discover that a student has turned in AI-generated work.
Communicate the importance of college learning – Many students are focused only on learning that seems related to their intended career track. However, the vast majority of them will change careers at least once in their lives. Talk with students about how the relevance of your course may only become apparent years from now. The skills they are learning will likely transfer to other careers – even careers that do not yet exist!
Acknowledge that struggle is part of learning –  Talk with students about how intellectual struggle is an inherent part of learning. Learning happens only when we move outside what we already know. Seeking a shortcut or workaround through AI tools only prevents them from learning. The short-term consequence is that they pay for a benefit they never receive. The long-term consequence is that they miss the opportunity to become better, more effective thinkers, writers, researchers, and creators.
Discuss the social, ethical, and practical issues surrounding AI – The processes that support the development and functionality of AI-based tools raise issues related to privacy, disinformation, environmental impact, bias, exploitation, and academic integrity, among other things. In addition, although AI-generated output appears authoritative and factual, it is frequently riddled with inaccuracy. Discussing these issues with students can help them see the social context of AI and can position them to make thoughtful decisions about their own use of AI-based tools.
Assess process as much as (or more than) product – Lowering the stakes of individual assignments reduces students’ motivation for cheating and encourages them to build their own skills and competencies. Low- or no-stakes formative assessments reinforce the notion that learning is a process and demonstrates to students that what’s valuable is the learning, not the grade.
Design assignments that ask students to connect course content, class discussion, and lived experience. It’s harder for AI-based tools to effectively connect the dots between these sources of knowledge.
Consider teaching through AI-based tools. Think about how using AI-based tools might facilitate students’ learning and prepare them to thoughtfully engage these tools in their personal and professional lives. How can students use AI-generated output to think critically and analytically? How can these tools help them ask questions about digital literacy and information accuracy? Further down this page we’ve provided some examples of how to integrate AI into assignments.
What to do if you suspect academic misconduct
Students are expected to practice high standards of academic and professional honesty and integrity. The University communicates with students about the importance of knowing and understanding the expectations of both the University and specific instructors regarding academic standards. If you have prohibited the use of AI-based tools and suspect that a student has engaged in academic misconduct, you can make a report to your campus Student Conduct office.

UW Bothell Student Affairs
UW Seattle Community Standards and Student Conduct
UW Tacoma Student Conduct
Information that is communicated to students regarding academic standards and the Student Conduct Code is available on the Office of Community Standards and Student Conduct Academic misconduct page.

Teaching with AI
Below are some examples of how instructors might use AI to facilitate learning. Many of these examples familiarize students with AI-based tools, but also prompt critical examination of their value, accuracy, strengths, and shortcomings.

Note: This list is a work in progress. If you are using AI in your own teaching and would like to share what you’re doing on this page, we invite you to complete this form.

Think-pair-AI-share. Students think (as individuals) about a question/concept, then pair up with a peer to discuss. The pair then plugs the question/concept into an AI tool (e.g., ChatGPT, GPT4, Bing Chat) and discusses or analyzes the output.(1)
Evaluating AI output. Co-develop a rubric with students that describes the components of an effective essay, lab report, précis, technical manual, blog post, etc. Students prompt an AI tool to generate three versions of the assignment on a given topic and then use the rubric to evaluate the quality of the AI-generated versions.(2)
Improving upon/adapting AI-generated output. Students use an AI tool to draft text or code in response to a prompt. Students must then improve upon the AI-generated output. When students turn in their assignment, they must include both the AI-generated text and their improved version.(3)
Explaining the steps in an AI-generated solution. Students use AI to solve a math problem. Working from the AI-generated solution, they then work in groups to explain or analyze the steps that the AI tool used to arrive at the solution.(4)
Visualizing concepts with AI. Students select a concept covered in lecture or course readings. Students then prompt an Al image generator to create an image that represents the connection between the concept and daily life. They must then explain how the Al-generated image conveys the concept and its relationship to daily life. Students might also analyze the strengths and shortcomings of AI image generators.(5)
Exploring AI in your field. Students explore current applications of AI in the discipline of the course or in their major. Within the context of the discipline (or their major), students examine both AI’s advantages and limitations.(6)",Omitted the resources cited at the end of the page,
Kiley,43,Boston University,Q49110,"36,729 (2022)","17,590 (2022)","17,937 (2022)",R1,"42.34888888888889,-71.10027777777778",Boston,MA,,https://www.bu.edu/,https://www.bu.edu/cds-faculty/culture-community/gaia-policy/,"Using Generative AI in Coursework
CDS Generative AI Assistance (GAIA) Policy
Intent
Students should learn how to use AI text generators and other AI-based assistive resources (collectively, AI tools) to enhance rather than damage their developing abilities as writers, coders, communicators, and thinkers. Instructors should ensure fair grading for both those who do and do not use AI tools. The GAIA policy stresses transparency, fairness, and honoring relevant stakeholders such as students eager to learn and build careers, families who send students to the university, professors who are charged with teaching vital skills, the university that has a responsibility to attest to student competency with diplomas, future employers who invest in student because of their abilities and character, and colleagues who lack privileged access to valuable resources. To that end, the GAIA policy adopts a few commonsense limitations on an otherwise embracing approach to AI tools.

Students shall
Give credit to AI tools whenever used, even if only to generate ideas rather than usable text or illustrations.
When using AI tools on assignments, add an appendix showing (a) the entire exchange, highlighting the most relevant sections; (b) a description of precisely which AI tools were used (e.g. ChatGPT private subscription version or DALL-E free version), (c) an explanation of how the AI tools were used (e.g. to generate ideas, turns of phrase, elements of text, long stretches of text, lines of argument, pieces of evidence, maps of conceptual territory, illustrations of key concepts, etc.); (d) an account of why AI tools were used (e.g. to save time, to surmount writer’s block, to stimulate thinking, to handle mounting stress, to clarify prose, to translate text, to experiment for fun, etc.).
Not use AI tools during in-class examinations, or assignments, unless explicitly permitted and instructed.
Employ AI detection tools and originality checks prior to submission, ensuring that their submitted work is not mistakenly flagged.
Use AI tools wisely and intelligently, aiming to deepen understanding of subject matter and to support learning.
Instructors shall
Seek to understand how AI tools work, including their strengths and weaknesses, to optimize their value for student learning.
Treat work by students who declare no use of AI tools as the baseline for grading.
Use a lower baseline for students who declare use of AI tools, depending on how extensive the usage, while rewarding creativity, critical nuance, and the correction of inaccuracies or superficial interpretations in response to suggestions made by AI tools.
Employ AI detection tools to evaluate the degree to which AI tools have likely been employed.
Impose a significant penalty for low-energy or unreflective reuse of material generated by AI tools and assigning zero points for merely reproducing the output from AI tools.
This policy recognizes that
This policy depends on goodwill, a sense of fairness, and honorable character.
Some instructors may prefer stronger restrictions on the use of AI tools and they are free to impose them so long as care is taken to maintain transparency and fairness in grading.
This policy takes account of the existence of subscription versions of AI tools, which are not affordable for some students; the policy may need to be revised as the differences between subscription and free versions become better understood.
This policy may be revised in light of other policies and novel technological developments in AI tools.",Omitted attribution at the end; this policy was actually created by students in a course and then adopted by the faculty of Data Science as an official policy-- here's the original pdf https://www.bu.edu/files/2023/02/GAIA-Final-2023.pdf and here's a press article about it https://www.bu.edu/articles/2023/student-designed-policy-on-use-of-generative-ai-adopted-by-data-sciences-faculty/ This page has suggestions for syllabus construction https://teaching.washington.edu/course-design/chatgpt/ ,
Kiley,43,The Ohio State University,Q309331,"65,405 (2023)","51,078 (2023)","14,327 (2023)",R1,"40,-83.0125",Columbus,OH,,https://osu.edu,https://oaa.osu.edu/artificial-intelligence-and-academic-integrity,"Artificial intelligence and academic integrity
There has been a significant increase in the popularity and availability of a variety of generative artificial intelligence (AI) tools, including ChatGPT, Sudowrite and others. These tools will help shape the future of work, research and technology — but when used in the wrong way, they can stand in conflict with academic integrity at Ohio State.

All students have important obligations under the Code of Student Conduct to complete all academic and scholarly activities with fairness and honesty. Our professional students also have the responsibility to uphold the professional and ethical standards found in their respective academic honor codes. Specifically, students are not to use “unauthorized assistance in the laboratory, on field work, in scholarship or on a course assignment” unless such assistance has been authorized specifically by the course instructor. In addition, students are not to submit their work without acknowledging any word-for-word use and/or paraphrasing” of writing, ideas or other work that is not your own. These requirements apply to all students — undergraduate, graduate, and professional.

To maintain a culture of integrity and respect, these generative AI tools should not be used in the completion of course assignments unless an instructor for a given course specifically authorizes their use. Some instructors may approve of using generative AI tools in the academic setting for specific goals. However, these tools should be used only with the explicit and clear permission of each individual instructor, and then only in the ways allowed by the instructor.",This page offers teaching resources regarding AI use https://teaching.resources.osu.edu/teaching-topics/ai-considerations-teaching-learning,
Kiley,43,Purdue University,Q217741,"50,884 (Fall 2022)","37,949 (Fall 2022)","12,935 (Fall 2022)",R1,"40.425,-86.92305555555555",West Lafayette,IN,,https://www.purdue.edu/,https://www.purdue.edu/innovativelearning/teaching/module/considerations-for-your-syllabus-and-course/,"Considerations for Your Syllabus and Course

AI and Your Course Syllabus
With the rapid advancement of artificial intelligence-generated content tools, consider updating your course syllabus to clarify your plans and expectations around the use of AI. Below are different considerations, including example language that you might use. 

What to know about AI

Many students know about AI resources, but do not assume that they clearly understand this new evolving technology. Adding a short narrative can provide a good foundation for understanding the basics. Three suggested principles to apply to student usage of AI is: 

Make sure your students understand that ChatGPT and other AI agents are known to fabricate things, and they need to carefully consider that when assessing their output.
Any use of AI-generated content should be documented according to the following practices (list your preferred documentation practice). 
Students are responsible for their learning and will not develop the skills they need to be successful in their careers by relying entirely on AI to do the work for them. 
You can both encourage students to explore the utility of AI for their learning and remind them in your course to document AI production that is not their own. Transparency and clear expectations are most helpful for students.

You may also want to determine the degree of LLM use that is allowable in your course, and for what purposes. For example, you may allow students to use an LLM for creating ideas, but not for specific deliverables. You could also limit a percent of certain work that could be LLM written. Finally, you can declare certain assignments to be entirely off-limits. You can see an example from faculty member Kate Zipay on item #5 in the Creative Pedagogy and AI section, “Getting Started with Writing”

Below we’ve included several examples for syllabus language. 

Sample Syllabus Statements

Purdue’s own Data Mine has some direct guidance on AI use and instructor communication

???????University College London has documentation written directly for students

If you plan to engage students critically in discussions around AI, this language statement from the Universidad del Rosario is helpful.

Finally, Lance Eaton has a curated set of syllabi policies sorted by subject on this Google Sheets document.

Chatbots and Large Language Models can also scan the internet and replicate information. Given the increasing potential for chatbots to produce inaccurate information, your syllabus could include a warning about their tendency towards deceptive data (see Mollick 2023 for suggestions).

Concerns with AI Use and Monitoring

LLMs are trained on specific written material, and curated by individuals within the organizations who created them. The algorithms necessarily reproduce the biases inherent in the training material and reflect specific cultural and societal norms. This cultural reproduction is strengthened when by an AI workforce is overwhelming male and white or Asian. 

The freely available LLMs are neither transparent in their algorithmic process nor documentation. This requires an increasingly critical approach to their output, and warrants caution for any course that mandates student use of these tools. We strongly suggest alternative assignments if you plan to integrate LLMs directly into your course learning, while also guiding students through careful interpretation of responses to student prompts.

The structure of LLMs make their output largely undetectable by automated processes. University of Maryland researchers have outlined the theoretical and practical limitations of LLM-detection software. Further, purported detection tools have disproportionately returned false positives for non-native English speakers. We strongly encourage you to treat the output of any such software as highly suspect.

Instead, consider proactively engaging with students on the value of written work as part of the thinking process in your course and discipline, and be transparent and direct with your expectations for writing in your learning environment.

Assignment and assessment guidelines 

AI has many possible uses to improve and expand students’ learning experience. However, students will need clear guidelines for when it is appropriate to use and how to use AI in the course. Adding guiding statements to each assignment and assessment description is one of the best ways to reinforce your position on student use of these tools to complete their assignments. Personal or reflective assignments are less susceptible to Large Language Models and chatbots. You might also consider asking students to connect course materials to their own life and experience, or document the relevance to their personal learning goals. 

Academic integrity 

As with other aspects of academic integrity, students benefit from clear and repeated guidance on course expectations. If your course includes generative content, such as typed assignments, code, or visual/musical media, you will want to emphasize how students should reference their usage of AI resources. Students will benefit from knowing that AI resources such as ChatGPT pull information and data from across the internet, which can lead to factual inaccuracies and implicit biases. If your course learning outcomes require individual student output that AI tools can mimic, you will want to make explicit whether you expect students to refrain from using these tools. Finally, if you have strong feelings about AI tools and the drawbacks of their use, discuss those issues in class with students. Share your perspectives on how you think these tools can help or hinder their learning, and why you value academic integrity. We suggest focusing on the benefit to students and their learning, and not potential negative consequences to their grade.   

Purdue policy and guidelines

The field of AI, chatbots and large language models (LLMs) is evolving rapidly. While there is no official Purdue policy governing their use in learning environments, the Office of the Provost has encouraged instructors to “carefully and thoughtfully evaluate the effects of Generative AI in their courses.” With that in mind, all information contained below are suggestions.  

You may want to incorporate language that mentions AI technology is evolving, and therefore policies on its use may change during the semester. You will also want to include a communication plan to keep students informed of any changes. 

Finally, be aware that efforts to automatically identify AI-generated text are theoretically and practically limited, and current detection software has significant false positives.

If you have further questions about discussing Artificial Intelligence syllabus or would like additional language examples, please contact innovativelearningteam@purdue.edu.","There are other pages on AI in the classroom, but this one offers the most guidance. There is another guidelines document but it's for a specific set of courses.",
Kiley,46,"University of Maryland, College Park",Q503415,"41,200 (Fall 2018)","30,762 (Fall 2018)","10,438 (Fall 2018)",R1,"38.9875,-76.94",College Park,MD,College of Information Studies,https://umd.edu,https://president.umd.edu/administration/commissions-task-forces-and-councils/generative-ai-at-umd,"Generative AI at UMD
What is Generative AI?
Generative AI (GAI) is a subset of artificial intelligence that focuses on producing novel content by learning patterns from training data. Unlike conventional AI used for specific tasks, generative AI employs complex algorithms, especially deep learning models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), to create outputs resembling the training data.

The central concept is enabling machines to craft original outputs, such as lifelike images of nonexistent objects or unique music compositions, by learning from extensive datasets to grasp patterns. Once trained, the AI generates new data points that bear similarities to the original but are entirely innovative. This technology finds applications in art, creativity, drug discovery, and generating synthetic data for training other AI models, highlighting its broad potential impact across industries and its rapid evolution in the AI domain.


Thoughts on AI from Senior Leadership
Op/ed: AI Has Potential to Solve Grand Challenges

Pines Says Higher Ed Institutions Should Consider the Technology’s Upsides Along With Risks

Read on Maryland Today

The Use of Artificial Intelligence (AI) in UMD courses

Innovations in AI technology are bringing both productivity benefits and unpredictable pedagogical challenges to educational settings like ours.

Read more from Provost Rice

Using Generative AI While Respecting Privacy

UMD's CIO and CDO discuss how to protect the privacy of Student, Research, and Administrative data when using Generative AI

Read Statement

Guidance for Students, Faculty, and Staff
General Guidance for Using GAI

Considerations in Choosing a Generative Artificial Intelligence (GAI) Tool:

Privacy risks: Be cautious as some tools might not guarantee data privacy and could be accessible by external parties. Avoid sharing sensitive information like credit card details or personal identifiers

Misleading costs: Some tools might appear free initially but might require a credit card for a trial, making it difficult to cancel the subscription later. Stay vigilant of such tactics

Learn to use: Familiarize yourself with tools like ChatGPT using available resources, such as training materials from UMD or comprehensive lists of prompts for different purposes

Understand limitations: Recognize the capabilities and constraints of GAI tools, as they are predictive models and not sentient beings. Explore information about their limitations to use them effectively

Use reputable resources: Ensure the tools you choose are from reliable companies like OpenAI's ChatGPT and DALL-E 2, Google's Bard, and Microsoft's Bing AI

Safe computing: Adhere to guidelines provided by DIT to remain safe on the internet

Considerations for the Ethics of Using GAI:

GAI is not sentient: These models don't possess self-awareness but are trained on datasets, leading to inherent biases and limitations

GAI is biased: Due to training on past data, these models may not account for current social changes and could have implicit biases

GAI can mislead: Models might produce information that lacks veracity, relying on verbosity rather than actual evidence

GAI prefers English: Language models can be biased towards Standard American English, disadvantaging other dialects and writing style

Student Guidance for Using GAI

How to Use GAI Responsibly in Academic Settings:

Seek guidance from professors on appropriate use in coursework

Verify information and citations from GAI and supplement with independent research

Remember that GAI tools are only tools and shouldn't replace critical thinking

Ask yourself relevant questions regarding your usage of GAI, its impact on your learning, content accuracy, and its equitable use compared to peers. Use these tools ethically to contribute positively to society

Staff Guidance for Using GAI

As GAI presents transformative possibilities for higher education and beyond, consider the following questions regarding your usage:

Is the GAI-based tool aiding my learning and cognitive processes?

Is the GAI-based tool enhancing or hindering my job performance?

Is the content I generate accurate, verifiable and free from biases that may harm others?

How will I handle content generated by GAI-based tools?

How can my use of GAI-based tools contribute to the greater good of society?

Remember the ethical responsibility associated with using GAI-based tools, as they can impact both personal growth and societal well-being.",I omitted the resources section at the end.,
Kiley,47,Lehigh University,Q622137,"6,953","5,178","1,775",R2,"40.607167,-75.379",Bethlehem,PA,,http://www.lehigh.edu,https://lts.lehigh.edu/generative-artificial-intelligence-ai,"Generative Artificial Intelligence (AI)
Exploring artificial intelligence and its role and impact on education, the world, and at Lehigh
Introduction
Faculty and students at Lehigh are focused on artificial intelligence (AI) and its place in university classrooms. AI as an aid to productivity. AI as a coding assistant and teaching consultant. AI for improvisation and shared humor. Also: students’ use of AI as incongruent to academic integrity?

 This web page on AI and approaches to teaching and learning will:

help faculty talk with students and set policies regarding the use of AI; and

aid faculty in the design of assignments and assessments related to AI.

Further reading and resources
You can continue your study with Lehigh-specific Resources on generative AI for Fall 2023 and guides on Generative Artificial Intelligence (AI), Resources for directed learning about AI,and AI Tools and Library Research.

What is Generative AI?
In a few words, “AI can be defined as ‘automation based on associations.’”1  In longer form: “AI is a branch of computer science. AI systems use hardware, algorithms, and data to create ‘intelligence’ to do things like make decisions, discover patterns, and perform some sort of action.”2  Generative AI refers to AI systems that produce text and images, among other possibilities. 

What is ChatGPT?
ChatGPT is an AI-powered text generator. It is also a chatbot and an application on the Internet. ChatGPT is powered by a large language model, or LLM. At their inception, LLMs depend on text mining and/or web scraping to build a corpus for study by a computer. An LLM then learns based on rules, or algorithms, set by developers, as well as training by humans.

LLMs “learn a probability distribution of the next word/pixel/value in a sequence.”3 This means that they do not output whole sentences from a text-mined foundation based on a user's prompt. Instead, LLMs and their chatbot intermediaries build outputs word by word, based on machine learning. This is one reason why proper citation becomes an issue for people who rely on text generators. Outputs from ChatGPT are probable, plausible but they do not draw from specific, reliable sources.  This is also why text generators make up references, or hallucinate, even when a user asks for citations within a prompt.4

The impact of AI
In a recent critical literature review, researchers identified a discourse of imperative change surrounding AI in higher education.5 But that imperative should be critiqued and analyzed in specific scenarios of teaching and learning. Faculty bandwidth is wide and limited. Take time to consider the impact of AI on your course and field of study. Know that there will be many opportunities for change in semesters and years to come. 

Speaking to students about AI
Break the ice
Use a quote, image, multimedia, the news, or movie reference to break the ice of a difficult (and technical!) issue such as the appropriate use of AI-powered tools in teaching and learning.6 Ask questions to open up a conversation about students’ use of AI. Are they using AI-powered tools? If not, why?7 If so, how?8 Aim to create a community of inquiry in which you learn from students and students learn from you.

Trust your students
One reason to introduce AI through a movie reference or news coverage is to pique students’ interest while addressing a pressing issue. Thinking about movies helps us identify anxieties related to AI and realize that a productive place, pedagogically, is to trust your students.  

Summarize your policies
Set aside time early in a course to clarify your policies related to AI. At a time of technological change, such policies will be uneven from a student perspective and across their course load. Some faculty will prohibit the use of AI in their classrooms. Others will make AI a constitutive part of the learning environment they create. Consider adding a policy on AI to your course syllabus.  An LU Syllabus Template developed by Greg Reihman provides a few examples. Feel free to select one – or modify the language as you see fit.9

AI detectors
If you are considering use of an AI detection software, read this Lehigh guide on the use of the Turnitin AI Detection feature.10 Also have your students review Undergraduate and Graduate Student Senate Statements on Academic Integrity and Article III of Lehigh's Code of Conduct.

Foreground equity
Foreground discussions of equity and algorithmic biases in your first discussions with students about AI. One example of algorithmic biases within AI systems involves police departments that use facial recognition programs to compare surveillance footage to a database of potential suspects. Algorithmic decision-making is also easily polluted by the racial and gender biases embedded in our society, which are reflected in large training sets and the actions of developers.11  Gender biases – Who wasn't working hard enough?, Who was late? – are prominent in these responses to prompts in ChatGPT.12   

Explore AI tools together
You will be better prepared to lead students through a critical use of ChatGPT and other AI tools with a general understanding of how text-generators are built and operate. You can also practice prompting, together. What is ChatGPT good at? Where does ChatGPT struggle? Have students fact check and refine outputs, identify biases, and compete with ChatGPT on responses to course-related queries. 

Consider students' professional development
The current conversation around AI is market-driven. According to a recent report from Stanford University, “The demand for AI-related professional skills is increasing across virtually every American industrial sector.”13 A contribution of a Lehigh education, however, is to think of the ethical use of AI alongside possible adoption in specific disciplines, to track misuse, and to imagine a better future marked by AI.

Follow the news
One way to stay up-to-date with the ever-changing landscape of AI is with a newsreel created by Future Tools. The AIAAIC also keeps a list of “AI, algorithmic, and automation incidents and controversies.”

The Next Generation
AI-powered tools generate prose, poems, music lyrics, and code. AI for image and video generation is already here. So what's next? Chart a path for the future of AI alongside developers and policymakers. One that considers intellectual property and copyright, ethical labor practices, public safety without over policing, and a cleaner planet. In the words of Jean-Luc Picard: Engage!","I omitted the footnotes at the end of the page ; This article from Nov. 9 2023 says that the provost ""doesn't foresee a blanket policy on generative AI at Lehigh"" https://www2.lehigh.edu/news/harnessing-generative-ai",
Kiley,47,Texas A&M University,Q49212,"72,982 (Fall 2021)","57,428 (Fall 2021)","15,762 (Fall 2021)",R1,"30.610369,-96.344056",College Station,TX,,https://www.tamu.edu/,https://cte.tamu.edu/transform-learning/respond-to-generativeai,"Respond to Generative AI
""As machines get better at being machines, the primary purpose of higher education must be helping humans get better at being human."" 
- Randy Bass, Vice Provost for Education, Georgetown University

Teaching and learning do not happen in a vacuum; educators and learners must contend with emergent technologies (e.g., Artificial Intelligence), social movements, and the evolving demands of industry. The sudden and widespread interest in Generative Artificial Intelligence like ChatGPT in late 2022 belies the fact that we have been teaching with AI for quite some time. 

On this resource page, we provide some timely strategies and basic definitions, and we highlight key modes of thought and areas of skill development relevant to teaching with any technology. We also underscore some of the foundational principles of teaching and learning that bear repeating.

Finally, we invite you to join the conversation. If you are utilizing technology in your instruction, we hope that you will consider sharing what you're using and how you're using it.

Instructional Considerations
 Teaching with Emergent Technologies
Students of today will be expected to master the technologies of tomorrow. Major companies are already acquiring enterprise licenses for emergent technologies like ChatGPT. As such, incorporating responsible use of these tools in the teaching and learning process can provide students with invaluable experiences and opportunities to prepare to assume their responsibilities as professionals and citizens.

Additionally, technologies can be tools to help facilitate and support student learning and success. Instructors can use them to enhance learning by adhering to best practices grounded in the science of teaching and learning. 

When you want to address and/or incorporate generative AI tools in your course, consider the following principles:

Syllabus Statements: Below are two Texas A&M recommended syllabus statement options for instructors to consider adding to their syllabi, informing students of their stance on the use of AI in the course.
OPTION 1: According to the Texas A&M University Definitions of Academic Misconduct, plagiarism is the appropriation of another person's ideas, processes, results or words without giving appropriate credit (aggiehonor.tamu.edu). You should credit your use of anyone else's words, graphic images, or ideas using standard citation styles. Artificial Intelligence (AI) text generators and natural language processing tools (colloquially, chatbots - such as ChatGPT), audio, computer code, video, and image generators should not be used for any work for this class without explicit permission of the instructor and appropriate attribution. This includes, but is not limited to,
                                                               i.      Creating or revising drafts
                                                             ii.      Editing your work
                                                           iii.      Reviewing a peer's work
This excludes pre-existing software additions such as spelling and grammar checkers, which are acceptable.
OPTION 2: With the emergence of artificial intelligence (AI) technologies, the ways in which we define our creative processes continue to transform. AI generators are rapidly evolving from simple editing for grammatical errors and spelling mistakes (Grammarly, MS Word Spell Check) to sophisticated text production (ChatGPT, Google Bard, etc.), as well as image, computer code, and audio generation. The presence of such tools, however, does not replace our need to learn how to draft, revise, and reflect on texts, programs, drawings and how to exercise information literacy and personal responsibility in how we locate, evaluate, incorporate, and cite primary/ secondary sources. For example, the Association for Writing Across the Curriculum states the following: 
Writing to learn is an intellectual activity that is crucial to the cognitive and social development of learners and writers. This vital activity cannot be replaced by AI language generators (AWAC).
 
Engaging in the various aspects of creative pursuits (e.g., writing, coding, drawing) is critical to education in a broad sense. While AI technologies will continue shaping how we approach these creative tasks, the critical work of creativity relies on integrity, originality, and ethical conduct in regard to appropriate representation as an author or creator. Thus, submitting work with a significant percentage of AI-generated content, unless otherwise permitted, can be considered academic misconduct under Texas A&M University Student Rule 20. Students must therefore cite the use of generative AI tools and document what they have contributed to an assignment.
Syllabus Policies and AI Detectors (video)
Other syllabus statement considerations
Clear Personal Perspective: Determine your stance on the use of generative AI tools in your course–will you and your students use it, and if so, what does that look like and what is the rationale? If you choose not to use generative AI tools in your teaching, be sure to communicate your reasons and rationale clearly and transparently.
Explicit Course Policies: Be transparent and clear about expectations and course policies when using AI tools including to what extent and how students can use the tools.
See the Use of AI in POLS 207 example
Generative AI & Academic Integrity (student video)
Pedagogical Approaches to Embracing AI (video)
See the Aggie Honor System Office definitions of plagiarism and cheating for additional help to define instructor and student responsibilities in this area.
Keep in mind that new tools are always emerging. Rules, regulations, and AHSO definitions will not name or characterize every learning assistance tool, so you must make decisions that work for your context and be clear about what your decisions are.  
Relevance to Learning Outcomes: Always closely link any permission or prohibition use of AI tool to learning outcomes and align with appropriate activities and assessments (See Learning with Emergent Technologiessection for potential learning outcome and assessment considerations).
Leveraging Generative AI to Assist Teaching Faculty with the Course Design Cycle
Build Students' AI Literacy Skills
See Briefing Reports AI Worksheet example
Reasonable Integration: Consider building the responsible and ethical use of AI tools into activities, assignments, and assessments. This may also include an honest discussion with students about ethical use of AI tools as well as issues regarding academic integrity. Consider discussing:
How AI is being trained (what data is being used and what data is not being used);
Why AI is being used and what tools use them; and
The outcomes of the tools being developed (e.g., What, if any, bias does AI training introduce? What role does ""prompt engineering"" play in the results produced by Generative AI? How is this tool being used?).
Accessibility of Integrated Tools: Every technological tool has some bias toward normative hearing, sight, and other perceptive sensing. Instructors have a responsibility to ensure a fair and equitable experience for all learners.
Can every student implement the tool with reasonable accommodations? 
Does use of the tool require or assume access to additional tools that need to be vetted for accessibility?

 Learning with Emergent Technologies
This list represents areas of skill development relevant to teaching with technology. While all of these skills are indeed relevant outside of teaching with technology, they are especially important when seeking to develop our students' capacity for the judgment and decision-making unique to human abilities. 

Critical Thinking
Critical thinking involves analyzing and evaluating thinking with a view to improve it (Paul & Elder, 2020). Critical thinking is useful in all domains of learning. Critical thought enables students to ask vital questions, formulate problems, assess relevant information, interpret abstract ideas, identify assumptions, draw conclusions, and create solutions. Self-directed, self-disciplined, self-monitored, and self-corrective thinking are all aspects of critical thought that can be nurtured in the learning environment and can be used to guide students as they responsibly employ emergent technologies.
Paul and Elder Critical Thinking Framework
Intellectual Standards & Traits
AAC&U VALUE Rubric for Critical Thinking
A SOAR Fired Method for Teaching Synthesis Writing (IDEA Paper #74)

Data & Information Literacy
Data & information literacy involves the ability to know when and what data are needed, how to obtain data, how to evaluate and use data, and how to derive and communicate insights from data responsibly. With emergent technologies in view, data literacy is realized not only through healthy skepticism about what information these technologies produce but also the information that is used to create these technologies.
AAC&U VALUE Rubric for Information Literacy

Ethical Discernment
Ethical reasoning involves making assessments between acts that benefit or enhance the livelihood of others and those that diminish the livelihood of others. Applied, ethical reasoning requires that students be able to identify ethical issues in various situations, understand their own ethical values, and be aware of multiple ethical perspectives. As students practice ethical decision making, their personal identification as an ethical decision maker develops. Examples of elements of ethical reasoning that can be developed through a course or program’s curriculum include: ethical self-awareness, ethical issue recognition, and applied ethics implications and consequences. The advent of new technologies or the application of existing technologies to new contexts and use cases provides learners with urgent questions about how these technologies are developed, how they are deployed, and the outcomes that they produce.
The Miniature Guide to Understanding the Foundations of Ethical Reasoning
IDEA Resource
AAC&U VALUE Rubric for Ethical Reasoning

Creativity & Innovation
Strategically appropriate innovation and creativity can be fostered in students. Instructors can take an active role in helping students reframe their views of creativity, specifically as it pertains to what are creative skills and just who can be creative in any given discipline. This section highlights models and methods that can be used to foster creativity and innovation among students as they engage in a wide variety of instructional activities and experiences (e.g., daily assignments, independent projects, collaborative and group work).
Intuitive Techniques
Brainstorming
Round-robin Brainstorming
Analogy
Drawing
Linear Techniques
Reframing Questions - Reframing Questions is an instructional strategy used to find new or alternate viewpoints in understanding a problem. This strategy involves asking overarching, detailed, and discriminant questions such as the ones below to break from typical patterns of thought in solving a problem. 
What is the broadest frame of reference for this problem? What are the “givens”, here-the “obvious” realities of our situation-and how can each be challenged?
What is the ideal state of affairs we are looking for?  
Force Field Analysis
SCAMPER
Attribute Listing
AAC&U VALUE Rubric for Creative Thinking

Teamwork & Collaboration
Our world is not flat. People are interconnected in a variety of ways, including ideas. As such, current and future global citizens should have know how to interact positively and productively with others. We posit that powerful learning occurs when people are connected with others (situated learning). Ideas and learning can spread through the connections we make. Recognizing those rich experiences constitute one of the most vital aspects of higher education, we therefore encourage the design of learning experiences that promote connections while applying collaboration/teamwork skill application is important. AI tools can help to manage groupwork and to make it more productive. And those with generative capabilities can even act as a near peer - one in the zone of proximal development - for learners. With clear guidance and rational boundary setting, emergent technologies can considerably enhance collaborative efforts and even act as a member of the team. 
AAC&U VALUE Rubric for Teamwork",The title is a bit misleading-- the article is some basic guidelines,
Kiley,47,University of Georgia,Q761534,"40,118 (Fall 2021)","30,166 (Fall 2021)","9,952 (Fall 2021)",R1,"33.9558,-83.3745",Athens,GA,,https://www.uga.edu/,https://www.ctl.uga.edu/_resources/documents/ChatGPT-Guidance-for-Instructorsc.pdf,"ChatGPT Guidance for Instructors WHAT IS CHATGPT? ChatGPT is an artificial intelligence (AI) tool that uses natural language processing techniques to respond to user-generated prompts. While other chatbots are typically limited to pre-programmed responses, ChatGPT can produce original responses in real time. ChatGPT can generate everything from essays and emails to poems and lesson plans. It can revise text with improved grammar and spelling or modulate the style or tone of a provided set of text, as requested by the user. ChatGPT is just one of many generative AI tools. ChatGPT may occasionally incorporate fabricated or incorrect information in its responses. Because of the way it operates, it is difficult to trace the source and provenance of information provided by ChatGPT (except when it is connected to the internet, e.g., Bingintegrated GPT). In addition, because ChatGPT was trained on a corpus of text that was originally written by humans, ChatGPT’s responses may reflect the biases present in the text it was trained on. TIPS FOR INSTRUCTORS • Talk to students about your expectations for academic honesty. Many students who engage in academic dishonesty do so impulsively or without significant reflection on the choice. Anticipate this behavior by proactively engaging students in an open discussion about academic honesty in your course, including your expectations regarding ChatGPT. Remind students that they are required to follow UGA’s Academic Honesty Policy and talk to your students about how that policy applies to your course. For example, is ChatGPT off-limits, sometimes OK to use, or always encouraged? See below for sample syllabus language related to ChatGPT, or explore the growing repository of classroom policies related to generative AI available here. Use of artificial intelligence or word mixing software to write your paper or disguise plagiarized work is considered unauthorized assistance in this course. Suspected unauthorized assistance, or other violations of UGA’s “A Culture of Honesty,” will be reported to the Office of Academic Honesty. Or:   ChatGPT Guidance      Page 2 We encourage you to use AI tools to explore the field and help you study. However, you must take full responsibility for any AI-generated materials you incorporate in your course products. If you include AI-generated materials in your course products, it should be cited like any other reference material. All information incorporated into course products must be verified, ideas must still be attributed, and facts must be true. • Do not rely solely on AI detector tools (see the “Additional Resources” section at the end of this document for examples) to identify whether an assignment was created using generative AI. These tools can return false positives and fail to detect some AI-generated material. If you suspect a piece of work was completed using generative AI, carefully analyze the work. If you believe a violation of your course policies may have occurred, instructors should report the incident to the Office of Academic Honesty. A facilitated discussion will be scheduled for you to talk with the student about their process for completing the assignment and whether a violation occurred. • Consider the limitations of ChatGPT when designing course assignments. As of August 1, 2023, ChatGPT (v3.5): o cannot access information that is not already included in its corpus. This includes text or internet resources generated after 2021, notes or other specifics from your course. However, GPT4 and other GPT integrated products do have access to more updated information. o cannot dissect or produce non-text-based responses, although other AIgenerators can produce images and art. o may fabricate references (including contrived DOIs) when prompted to produce citations. • Instructors might deliberately ask students to engage with ChatGPT, or other Generative AI tools, as an educational endeavor. For example, students might analyze how ChatGPT generates text for different audiences, attempt to find the original sources of text used to generate a particular ChatGPT response, compare and contrast ChatGPT output in response to various prompts, or improve upon a given ChatGPT prompt. If you decide to incorporate use of generative AI tools in your course, inform students how/whether you would like them to document or cite their use of generative AI. Discuss with students the importance of transparency and trustworthiness to the process of generating knowledge, the pursuit of a degree, and to the ideals of higher education.  • If you ask your students to use ChatGPT, review the privacy policy and provide an opt out. ChatGPT is currently released as a “free research preview,” meaning that current use of the tool is helping to improve it for future use. OpenAI (the company that designed ChatGPT) collects data from its users which may also be shared with third party entities. If you plan to use ChatGPT in an educational   ChatGPT Guidance      Page 3 setting, it is recommended that you review the privacy policy and terms of use with your students, and provide them with the option to opt out (or alternate means to engage with ChatGPT output) if they do not feel comfortable having their data collected and shared as stated in these policies. ",I didn't include the resources at the bottom.,
Kiley,47,Virginia Tech,Q65379,"36,383 (2020)","29,300 (2020)","7,083 (2020)",R1,"37.225,-80.425",Blacksburg,VA,,https://www.vt.edu/,https://tlos.vt.edu/resources/generative-ai.html,"Considering Generative AI and ChatGPT at Virginia Tech
Last updated: Aug. 21, 2023

You may be hearing an array of opinions about the implications that generative AI (Gen AI) tools might have for the future of teaching and learning. Any time a new technology like Gen AI emerges and captures the public’s attention, initial reactions tend to be polarized. Some see AI as a threat to higher education, while others view it as a positive force for reshaping the way we teach.

In the midst of these competing and conflicting perspectives, TLOS recommends a measured approach. We ask faculty to consider the following suggestions as you make decisions about the courses you teach. Because these tools are changing rapidly, we will continually evaluate their use at Virginia Tech and revise our guidance as the situation evolves.

What is generative AI?
Generative artificial intelligence (Gen AI) describes several types of AI that are capable of creating new text, images, code, audio, or other types of content. Tools such as ChatGPT have been trained using very large collections of existing media to generate human-like responses to user-authored prompts. For text-based content, these large language models can be used to answer questions, write essays or articles, create lesson plans, and much more. The content creation process is limited by the tool’s training media and the user prompt.

Why does it matter for Virginia Tech?
Gen AI tools are becoming increasingly influential across many industries, making it essential for students to learn how to utilize these tools within their specific fields of study. However, the potential for misuse of Gen AI also raises concerns for evaluating student learning. Interrogating the risks and opportunities of these tools in different types of courses and disciplines (course size, delivery modality, topic, etc.) is key to finding ways to leverage their power while limiting their abuse. 

The true impact of these technologies remains to be seen. It is not likely to cause the end of higher education, and it is not likely to be the panacea some expect it to be. Deciding how to apply and constrain these tools is likely to be a nuanced issue that will require thoughtful and informed debate.

Recommendations
1. Become familiar with generative AI tools in order to have an informed perspective.
See the links below for some basic introductions to the technology and its implications.
Attend a workshop or forum discussion on the topic.
Reach out to support staff in TLOS, CETL, the Office of Undergraduate Academic Integrity, and the University Libraries for specific advice.
2. Consider the Honor Code and its applicability to generative AI tools.
The Office of Undergraduate Academic Integrity offers the following guidance: While most students largely engage in honest behavior in the classroom, some may choose to use tools such as ChatGPT to engage in academic dishonesty. Please continue to be clear in your expectations with your student related to the Undergraduate Honor Code and the use of AI software just as you would other websites that may provide students with means to engage in academic dishonesty. The unauthorized use of ChatGPT and other AI software may fall under several definitions of academic dishonesty in the Undergraduate Honor Code. If you believe that a student has engaged in academic dishonesty, please contact the Office of Undergraduate Academic Integrity at 540-231-9876 or email the office at honorsys@vt.edu to discuss what occurred.
The Graduate Honor System reviews each case in its own context. Faculty are encouraged to provide clear and precise guidance about when or if the use of tools such as ChatGPT are allowed, encouraged, or prohibited and candidly discuss with students the learning value of completing an assignment with or without such tools. GHS decisions about any potential violation will be based on what guidance faculty provided. If you believe that a graduate student may have committed an academic integrity violation, please refer the case to the Graduate Honor System; 540-231-9564; ghs@vt.edu.
3. Avoid being drawn into a confrontational mindset regarding these tools.
Given the obvious implications for academic integrity, tools are being developed and promoted that may provide some perspective on whether or not student submissions were written by AI. Due to the evolving nature of AI solutions, these tools are never going to be 100% accurate, and an overreliance on a tool to “catch” anyone using AI is likely to lead to a slippery slope of ineffective and highly stressful interactions with students.
4. Set clear expectations for your students regarding the use of generative AI/ChatGPT.
Directly address the reasoning behind any restrictions you decide to implement.
Consider updating course documentation about academic integrity to include a statement specifically addressing the use of generative AI/ChatGPT (see ideas below).
5. Explore potential changes to your course design and/or assessment strategies.
Consider incorporating course assessment practices aligned with principles of authentic assessment such as seeking to replicate professional experiences, expecting a task-focused demonstration of understanding, and providing opportunities for practice with feedback. (See Teaching Resources - Assessing Student Learning - Authentic Assessment from Indiana University for some more detailed ideas.)
Consider allowing multiple options for action and expression in your assessments. This is a principle of Universal Design for Learning that suggests students be allowed to select from various methods and technology tools to showcase their understanding and support their learning.
Proactively incorporate generative AI/ChatGPT into your instructional strategy, inviting students to use these tools to spark creativity, or to test and evaluate the accuracy of their output.
In the end, our suggestion is that these tools represent an evolution of existing information technologies and that we should carefully consider how to utilize them to improve our shared experience.",I didn't include the resources at the end.,
Kiley,47,Wake Forest University,Q392667,"8,963 (Fall 2022)","5,447 (Fall 2022)","3,516 (Fall 2022)",R2,"36.135,-80.277",Winston-Salem,NC,,https://www.wfu.edu,https://cat.wfu.edu/resources/ai/#20230914140243,"Artificial Intelligence For teaching centers, the weeks before the start of the fall semester are the most wonderful time of the year. We welcome new faculty into our community of teacher-scholars, and faculty of all ranks begin to reach out as they design their syllabi and reflect on changes they would like to make. This year, almost everyone is considering how they want to approach artificial intelligence in their classrooms. Although ChatGPT was released in November, it took some time for the news to spread, and many of us took a wait-and-see approach in the spring. We’re now hoping to develop a more intentional approach. Yet, as is often the case in the life of a teacher, there is too much to read and not enough time.

The resources below aim to ease that burden by summarizing and curating the relevant literature. Given the need for just-in-time guidance, I’ve organized the resources into a series of FAQs with the most time-sensitive questions at the top. As of 8/22, we’ve addressed AI policy, syllabus statements, and strategies for preserving academic integrity. In the coming days, we’ll share strategies for using AI to support student learning, discuss ways AI might support our work as teachers, and recommend readings, podcasts, and videos for those who want to learn more about the nature of artificial intelligence and its broader implications for society.

As always, the CAT is available for consultations if you have any questions or would like a conversation partner as you think through your approach! I’m feeling overwhelmed. Where should I begin?
First, you should know that you are not alone. Given the pace of these developments, very few of us have a complete handle on what these tools can do and why they matter. As one of my favorite comics points out, even the experts who developed these models aren’t quite sure what is happening under the hood. We’re all feeling a bit like deer in headlights, so give yourself some grace.

Second, if we learned anything from our experience of emergency remote teaching in 2020, it is that we are better together. When we joined formal and informal learning communities to ask questions, share ideas, and reflect on what did and did not work, the load felt lighter. So now, as ever, I recommend joining your colleagues in conversations about how they are approaching AI in their classrooms.

This can happen within your departments, but the CAT will continue to facilitate opportunities for interdisciplinary collaboration. In addition to ai consultations and workshops, we’ve created a WFU AI and Education Google Group to allow faculty to share news, questions, and useful examples. We will continue to share resources we have found via our newsletter and blog, and hope to develop larger-scale programs later this year.

If you find yourself curious about what is happening at other institutions, you can also join the “AI in Higher Education” Google Group developed by our colleagues in the POD Network or the “Higher Ed Discussions of AI Writing” Facebook Group, developed by Laura Dumin, Professor of Technical Writing and AI Coordinator for the University of Central Oklahoma. If you enjoy Substack newsletters, I can also recommend signing up for updates from Ethan Mollick at Penn and Marc Watkins at the University of Mississippi. Why do I need to craft my own policy?
The University has not yet established an institutional policy governing AI use on campus. Our leaders have chosen (wisely, I think) to hold off on such a policy until we have a better understanding of these tools and the various ways our faculty and students are using them. Instead, schools, departments, course coordinators, and individual instructors have been encouraged to develop approaches aligned with their pedagogical goals.

As a result, most instructors will be free to take whatever approach they prefer, including no approach at all. Yet an environment that welcomes various–and sometimes conflicting–policies is also an environment that benefits from transparency. Without a “default” approach to AI on campus, communication between faculty and students will be even more essential than it already is. For this reason, my number one recommendation for faculty this fall is to be as transparent as possible about your specific approach to AI and how it may differ from other approaches on campus.

If you’re genuinely unsure of what your approach should be, share this with your students. You might also consider it a pedagogical opportunity. We know policies are more likely to be followed when they are co-constructed with our students, so you could spend the first few weeks working together to develop your approach. What should I consider when crafting a syllabus statement?
Before you can draft your syllabus statement, you must first decide when AI use is and is not acceptable in your course. In an ideal world, you would become an AI expert before making those decisions. In the real world, it is probably enough to ask yourself whether the various uses outlined in this worksheet would be problematic in your course.

From a teaching and learning perspective, the most important question you should be asking is: “Does this particular use of AI support or undermine the knowledge, skills, and dispositions I aim to develop in my course?” Without clearly defined outcomes, this can be a tricky question to answer. But once you’ve clarified your goals, you will immediately understand why we cannot have a one-size-fits-all AI policy. If I hope to develop students’ ability to edit sentences for clarity, my students will need to practice editing sentences independently. But if I want them to develop their ability to explain this process, they could benefit from using AI to quiz themselves and check their understanding. And if I am to develop both skills, I will need to craft a nuanced, conditional policy.

For further practical guidance on drafting statements, see: Do you have any examples of syllabus statements I might consider?
Yes, many!

Lance Eaton, the Director of Digital Pedagogy for College Unbound, curates the definitive archive of sample AI policies. The CAT has also curated a Wake-Forest-specific archive that brings together examples from the School of Law, School of Business, and numerous departments within the College. We hope this list will continue to grow throughout the week, so please send us your policies when you’ve finished them!

I also recommend the suggestions drafted or curated by the following Centers for Teaching & Learning: Is there a way to detect whether our students have used AI?
Cheating has always existed, often at depressingly high levels.1 And for almost as long, we have sought to limit the harm it can cause through various forms of punishment (or, as we like to say now, “accountability”). In the case of cheating, punishment serves at least two functions. It stops the cheater from doing wrong (receiving an unfair advantage over fellow students) and deters other students from attempting something similar. But in both cases, detection is an essential piece of the puzzle. If we don’t know students are cheating, we can neither stop the wrongdoing nor deter others from doing the same.

There has typically been an inverse relationship between the cost of cheating and its ability to be detected. So while it has always been possible to pay another person to write an undetectable essay for you, the cost of doing so has been prohibitive for most students.2 AI presents a unique threat to the accountability approach because it changes the slope of this relationship between cost and detectability. While it’s true students must put forth some effort (they cannot, as some students have done, submit work that begins, “As a generative AI model …”), it no longer takes much work to cheat in undetectable ways.

Given this reality, it makes sense to think of the challenge before us as one of detection. If we could find a way to detect AI-generated output like we detect plagiarized papers, students would be no more likely to use AI than they are to plagiarize, and we could return to business as usual. So we seek tips to improve our ability to spot AI-generated text or, failing that, software that will do this work for us.

Unfortunately, AI detection is both technically and ethically complex, and this complexity is only going to increase with time. But even if we decide AI detection is neither realistic nor ethical, we need not despair. And that’s because accountability is not the only way to shape student behavior. Yes, punishment can be a powerful motivator. But we also know it can also have unintended and unpredictable effects. As a result, experts on Academic Integrity have long sought to expand our toolkit beyond accountability alone. By turning our attention to these approaches, which aim to cultivate students’ positive, intrinsic motivations, it may be possible to escape the ruin of the spring semester without solving the detection problem.

 What should I know about AI detection and AI detectors?
Although we all want to believe we can spot AI-generated text when we encounter it, researchers have known for quite some time that humans struggle to distinguish between human- and ai-generated prose.3 It is, then, unsurprising that numerous start-ups (and OpenAI itself) were prepared to launch AI detection tools within weeks of ChatGPT’s release. And in April, Turnitin released its own secure, LMS-based tool to the faculty of over 10,000 institutions.

Since then, debates about their reliability have raged, OpenAI has quietly removed its detector from its site, Turnitin has updated its reported false-positive rates, and both Vanderbilt and The University of Pittsburgh have decided to disable Turnitin’s AI detection tools, as a result. Despite these criticisms, thousands of instructors–including many Wake Forest faculty–continue to find these tools valuable.

As with most debates, the details are more complicated than the public discourse suggests. Turnitin still maintains a 1% false positive rate for paper-level scores higher than 20%, and they are the only company able to test its tool on a 20-year archive of papers written by college students before AI came on the scene. Nevertheless, they acknowledge they will miss at least 15% of AI-generated text to maintain a false-positive rate of 1%. And this performance only applies to essays written with GPT-3. Students who can pay $20 a month for GPT-4 are far less likely to be detected.

These rates may seem encouraging if we imagine one false accusation for every 100 suspicious cases. Yet this false-positive rate is based on all papers submitted to Turnitin, including those we would have never investigated without the software. Assuming 25,000 papers are submitted to Turnitin each academic year, and 75% of those papers are human-generated, 188 of those human-generated papers would be inaccurately flagged as more than 20% AI-generated each year. To make this more concrete, any instructor who assigns three papers to 50 students would likely encounter 1-2 false positives each term.

Finally, it is worth remembering that AI detectors are themselves AI tools, trained in much the same way. To the extent you find AI problematic because of its propensity to “bullshit” (in Harry Frankfurt’s technical sense of that term), you must also acknowledge that these detectors are just as likely to speak confidently about things they don’t actually “know.” If you’re worried about your students trusting a machine that can hallucinate facts, remember that the same could be true of the reports you receive about student papers.

 What if AI detection is verified with additional evidence?
If you’ve spent any time talking with me about the assessment of teaching, you will know that I am a big fan of compiling multiple sources of evidence when no single piece of evidence is strong. So if we want to use AI detection scores responsibly, we should always verify them with additional evidence. But what else can you use?

Some have argued that there are tell-tale signs of AI-generated text within the prose. I would be hesitant to use style as a marker, but if you discover invented sources or facts, or the passage begins “As a generative AI model, …” you can be relatively confident AI was involved. You can also introduce “verification” assignments that produce additional evidence. You have almost infinite options here, but you might consider:

Collecting in-person writing samples to compare with out-of-class writing.
Scheduling 1:1 conferences with students to discuss their writing process after they submit.
Asking students to discuss their essays and writing processes with their peers.
Giving short, in-person quizzes that ask them questions about essays they have just submitted.
Asking students to use what they’ve learned on their out-of-class assignment to complete an in-class activity.
Although these assignments will allow you to verify the authenticity of student work, they will also be meaningful opportunities for students to practice, reflect, and improve. Can I redesign my assignments to make AI use impossible?
You may be tempted to “AI-proof” your course by designing assignments AI can’t do (or can’t do well). Yet this approach will require you to hit a rapidly moving target as AI tools advance in unexpected ways. We once thought prompts about contemporary events were safe, but now that Bing and GPT-4 have internet access, they can handle them with ease. What is difficult today will be easy tomorrow, so you might as well assume these tools can do anything you ask.

Yet even an all-powerful AI will be useless to students if they don’t have access to it. So those of us teaching residential courses can also shift the most important activities and assessments to an in-person environment with limited access to computers and phones. We can flip our classrooms, introduce oral exams, or opt for blue-book exams instead of essays.

Yet these decisions are not without costs. Although flipped classrooms are more effective than traditional lectures, they ask a lot of the instructor the first semester the course is taught. The skills we assess in a timed writing exam are quite different from what we assess in a long-form paper written and revised over a series of weeks. And oral exams can be both time-intensive for faculty and anxiety-inducing for students. Can I use grades to foster integrity?
Although punishment is a powerful extrinsic motivator, grades can be just as powerful for many of our students. There are at least two ways you can use this to your advantage.

First, you can begin the semester with an activity demonstrating the dangers of using AI to produce graded work. For this to be effective, you want the output to seem impressive. Students will be able to identify obvious hallucinations. But if you can show them they may not know enough to see the mistakes you will see, they may be less willing to rely on AI for graded work.

Second, you can structure your formative and summative assessments to motivate students to complete their take-home assignments on their own. If you expect students to demonstrate certain skills on a proctored final, and earlier take-home assignments are opportunities to practice those skills, they may be more likely to approach these assignments as genuine learning opportunities.

I will be taking the second approach in my class this fall. I know it’s not perfect (I will be giving a high-stakes, in-person exam for the first time in years, and I risk reinforcing an orientation that prioritizes grades over learning), but it gives me the freedom to assign meaningful work outside of class without submitting their work to detectors. Can I really cultivate an intrinsic motivation to learn?
In an ideal world, our students would be intrinsically motivated to adhere to our guidelines, and for the right reasons. Yet they enter our classrooms with a variety of motivations, and not all of them are aligned with our goals. One might reasonably ask how much we can shape these motivations in the course of a single semester. If students don’t want to learn and care little about academic integrity, is there much we can do?

The primary reason I am optimistic about the future of AI in our classrooms is that I believe in the power of teachers. While we may not be able to win over every student, I believe most students want to learn and will do so with integrity if the conditions are right. And thanks to the fabulous work of many brilliant social scientists, we happen to know a thing or two about what those conditions look like.

For starters, we can involve them in the process of thinking through our collective approach to AI. We know that motivation increases when students feel the environment is supportive and aligned with their goals. Giving them a say in the process gives them some ownership over their environment while helping them better understand the reasons for taking a particular approach.

We also know that moral reminders can be powerful tools to motivate students to align their behavior with their values and commitments. So if we ask students to sign on to a co-constructed set of principles, and remind them of the importance of those principles before each assignment is submitted, they may be more likely to give us their best.

If you think back to the times in your life you were learning the most, what was your primary driver? Chances are it was not a desire for an A or a desire to comply with an externally imposed policy. It was, most likely, the joy of participating in activities you found personally or socially meaningful. Likewise, our courses become more meaningful when we connect our material to the interests of our students and develop relevant, authentic assignments.

Finally, it is worth noting that even the most highly motivated students, committed to learning for its own sake, can also be deeply concerned about grades. And insofar as they perceive a threat to those grades, their intrinsic motivation to learn may take a back seat. So it may not be enough to make learning meaningful in the age of AI. We may also need to reduce the power of extrinsic motivators like grades.",I didn't include the resources at the end.,
Kiley,53,Northeastern University,Q37548,"36,806 (2022)","20,980 (2022)","15,826 (2022)",R1,"42.34,-71.08833333333334",Boston,MA,,https://www.northeastern.edu/,https://learning.northeastern.edu/teaching-in-an-era-of-chatgpt-and-other-ai-tools/ ; https://cssh.northeastern.edu/wp-content/uploads/2023/09/An-Insiders-Guide-to-to-Learning-with-AI.pdf,"AI in Higher Ed: Teaching in an Era of ChatGPT and other AI Tools What is ChatGPT?
ChatGPT is an Artificial Intelligence online tool created by OpenAI that can be prompted by a user to produce novel, substantive written work or code that can be indistinguishable from that produced by humans. Additional tools created by OpenAI and other organizations can also produce original artwork and music. ChatGPT has already been shown to successfully pass an MBA exam at Wharton and “hack” the assignments in a graduate level course on learning and cognition. These AI tools are being met with both intrigue and consternation in higher education. Despite significant and understandable concern about inappropriate use, educators such as Northeastern’s Laura Huang observe that “There are going to be huge, huge opportunities for people who know how to write prompts, …massage, and analyze what comes out of things like ChatGPT or the next generation of ChatGPT.”

How is Higher Education Responding?
Recent advances in AI are so new that educators are just beginning to consider what these new tools might mean for their teaching and their students’ learning. While it may be tempting to focus on potential negative consequences, AI also presents an opportunity to make modifications in our teaching that will improve and personalize student learning, and even to make creative use of the tools that will help prepare students to thrive in the technology enhanced world in which they will live and work.

Six CATLR Tips
Address your concerns with positive intention. Phrases like “integrity,” “honesty,” and “cheating” can suggest a stance of suspicion and punishment in relation to students. Talking with students about the benefits of “originality” and your hopes for how their work in your class will help them personally will help them understand your motivation for discouraging inappropriate use of AI.
Consider revising your assignments to emphasize the work development process, and include an in person or recorded component. Set interim deliverables for major assignments that include rounds of instructor and peer feedback (e.g., brainstorming questions/ideas, finding and annotating sources, outline/draft, final written product/presentation). Have students include a written explanation of how they used the feedback they received to improve their work, citing specific changes.
Tap into the uniqueness of students’ lives. Have students share their prior experiences and goals for the future, then ask them to personalize their project work according to those interests. Consider taking a students-as-makers approach. Students will be more engaged because they see the relevance of what they are doing to their lives, and it will be much more difficult to get AI to do their work!
Avoid generic assignments such as term papers that are easy to create with AI, and consider having students present their work in several formats, such as a digital poster in place of or in addition to a written component. Consider adopting an I-Search format in which students get to identify questions that are personally interesting to them, tell the story of what sparked their curiosity, and document their process of finding answers that leverage their interests to address pressing concerns in the world.
Emphasize the importance of questions. The ability to identify and refine compelling questions will become increasingly essential in a world where previously-identified answers can be accessed in a matter of seconds. The Right Question Institute has developed a process for question formulation that is relevant to cutting edge work in most contexts and disciplines. If you choose to lean into AI, this could be a preliminary step to prompting ChatGPT or another AI tool for a response.
Lean into technology by “learning with” AI. ChatGPT and other AI tools challenge us to understand, access, prompt, corroborate, and incorporate information in new ways. Educational experiences that help students develop these capabilities will equip them to use AI effectively. For example, have students generate an AI response to a prompt, then critique and fact check the product. What questions can the tool answer adequately? What are its limitations? In what aspects is the response flat-out wrong? Another option is to have students develop successive drafts of a piece with ChatGPT, documenting how they refined prompts for each draft and reflecting on the strengths or limitations of the prompt in light of the output. They will increase their understanding and critical thinking about the topic while also developing key technology and information literacy skills.
Do you have ideas or tips for teaching and learning with ChatGPT that you would like to share with other educators? Please use this form to contribute to Educator Insights: AI for Learning! An Insider’s Guide to Learning with AI What AI Can and Can’t Do for You as a Learner It takes a lot of work to get into college, and it’s also a significant financial investment. How can you get the most out of the experience? The truth is, the more effort you put into your learning, the more you gain: skills and strengths that serve you well both now and in the future. But this won’t happen if you lean on AI to do your work for you.  Northeastern is all about experiential learning, learning by doing, with you in charge of making connections and charting a special path. In contrast, AI-generated work is generic, even to the point of being boring. It’s also sometimes flat-out wrong! AI doesn’t know about your unique life experiences, thoughts, and goals. When you invest yourself into your work, the products are unique and irreplaceable. It also is true that AI is becoming integral to many aspects of our lives. In the near future most jobs will involve some form of AI, and generative AI tools can be extremely useful when used appropriately. Learning how to work with AI wisely and creatively – becoming AI “literate” – can position you for success in high demand jobs of the future. So there is a legitimate place for AI in learning, but you need to be actively involved to gain the benefit.  As you consider whether to use generative AI in your classes or while on co-op, keep the following 5 tips in mind. AI @ Northeastern: Top 5 Tips 1. Talk with your instructor: Expectations for AI will vary across disciplines and courses, so it’s important to read the syllabus and check with your instructor to find out what’s allowed. If you’re not sure what’s allowed, be sure to ask before using it. 2. When using AI, stay in the driver’s seat: Let AI challenge and aid you, but never let it take over. AI outputs can be inaccurate, incomplete, or even biased. It should never be used as the sole source. Consider also the strengths and limitations of what the tool puts out. Critique what it produces. Refine your prompts to get a result that’s more accurate and closer to what you need. Remember that it’s important to make the product your own, because originality matters. This will almost always involve a lot of revising and refining on your part. 3. Remember that the process of learning is as important as the product: Keep track of how you go about your work, for example prompts and iterations of AI output and how you use or modify what you generate. This will help you explain what you’ve created to others and cite your use of AI properly. Some instructors will even require that you submit this documentation along with the completed assignment. Be prepared to speak to your learning process. 4. Learning is a human endeavor: Don’t be afraid to ask your instructor or TA for feedback on worksin-progress and get feedback from peers if that is also allowed. It’s not just you and the computer on your own! 5. Cite AI Properly: Always cite AI if you use it in assignments. For example: In response to the question “Why is feedback important?”, ChatGPT suggested ""feedback acts as a compass in learning..."" (OpenAI, 2023). The APA and MLA provide style guides for citing Generative AI. 
","I included both the student and faculty guidelines, which were on different webpages. I did not include the list of resources at the top of the faculty page.",
Kiley,53,William & Mary,Q875637,"9,517 (Fall 2022)","6,543 (Fall 2022)","2,974 (Fall 2022)",R2,"37.270833333333336,-76.70833333333333",Williamsburg,VA,,http://www.wm.edu,https://stli.wm.edu/teaching-in-the-digital-age-artificial-intelligence-and-online-writing-aids/,"Generative Artificial Intelligence and
Online Writing Aids




What They Are

Artificial intelligence (AI) writing aids range from the familiar spelling and grammar checkers found in Word, Google Docs, and Grammarly, to the more recent natural language processing bots such as ChatGPT. These tools “learn” from various inputs, human and artificial, to produce writing/grammar tips, prompts, or original responses to user queries. ChatGPT and similar AI powered bots mark a significant step in the power of AI to generate original content.


Why They Matter

Writing in various forms is a keystone approach for learning across many disciplines. Understanding the affordances and constraints of AI and online writing tools is an important aspect of teaching in the digital age. As with most educational technologies, successful exploration and use of online writing aids and content creation bots in teaching and learning is dependent on intentional course planning. 


Teaching Applications

The Studio for Teaching & Learning Innovation (STLI) recommends instructors implement clear course expectations and learning goals to guide the use of online writing aids. This is especially important in courses where students create assignments with new and unique content to show mastery. The following practices will help establish the approach that works best for your class. 

Expectations

Be sure your syllabus includes a statement on academic integrity and references the W&M Honor Code. In class, discuss how improper uses of chatbots or other AI tools are not acceptable.
Syllabus should make clear that any submitted work can be reviewed, as needed, for plagiarism, lack of citation, and improper use of AI-generated content.
Review the importance of academic integrity and the value of the writing process for learning. Additionally, discuss the consequences of cheating.
Assessment approaches

Implement a variety of assessment approaches to provide multiple pathways for learning. Beyond writing assignments, consider traditional exams, take-home projects, oral presentations, digital projects, visual poster presentations, and lower-stakes in-class activities (e.g., discussions, group work, debates)
Consider adding an “oral defense” component to major submitted assignments.
Scaffold larger or higher-stakes assignments to allow students to work on the assignment in “chunks” with instructor feedback along the way.
Consider using a chatbot like ChatGPT as a class activity where students apply their knowledge and understanding to an auto-generated prompt. This can be a good way to discuss genres in academic writing, common structures for thesis statements or integrating quotations, and the difference between risky and safe assertions. 
Using technology aids

Leverage campus resources such as SafeAssign to detect copied or poorly cited work. Tools to detect AI generated content are evolving as quickly as the AI generators.
Consider using one of the tools listed below if you have concerns about a particular submission.
Consider student variability and equity when using or not using technology tools. Who can access the tool? Why might some populations encounter more challenges than others?
Sample Syllabus Statements

Consider using traditional syllabus language regarding adherence to W&M’s Honor code (samples linked below). Additionally, include course-specific language governing the use of AI-generated content:

The use or incorporation of any AI-generated content (from ChatGPT, Dall-e, etc.) in assignments is not allowed. Submitted work may be reviewed, as needed, for AI-generated content. 
All work submitted in this course, whether in draft or final form, must be your own and must be cited appropriately. You may incorporate AI-generated content or ideas in assignments, but you must cite this content, and you must fact-check all material, because AI-generated content often contains falsehoods and fictional sources. Citations must include which AI platform generated the content, and the specific prompts used to generate content. 
In this course, we will explore the use of AI-generated content as a [insert objective – educational/societal/other] tool. You will analyze the [insert here – quality/ethics/bias/etc.] of this content. Ideas and content generated by you, and those that are AI-generated, should be clearly delineated and cited accordingly.",I didn't include the resources at the bottom,STLI: Studio for Teaching & Learning Innovation
Kiley,58,Stony Brook University (SUNY),Q969850,"25,510 (Fall 2022)","17,509 (Fall 2022)","8,201 (Fall 2022)",R1,"40.9173205,-73.1245537",Stony Brook,NY,,https://www.stonybrook.edu/,https://www.stonybrook.edu/celt/updates/chatgptguide.html,"ChatGPT - Guidance on AI Writing Bots 


Since ChatGPT became publicly available in November, 2022, many in academia have been exploring the benefits and challenges of artificial intelligence (AI) writing bots. ChatGPT and other similar tools provide responses to questions and prompts imputed by the user that have vast implications for its use in courses and other academic settings. At SBU we have hosted several relevant events including two panel discussions through CELT and a panel discussion through the Library. Attendees at these events and others who have reached out to CELT directly have requested guidance that faculty and other instructors can consider for their classes. As you prepare for your summer and fall courses, we wanted to share some items that you might find useful. To be clear, while we are actively engaging our academic community in conversations about how we should approach these tools in the future, at the time we have not developed any policy related to these tools, and thus the material below is meant to serve as food for thought as we continue these conversations and as you prepare your classes.



Add an AI writing bot/ChatGPT statement in your syllabus.
Faculty should consider if they want to allow or restrict the use of AI writing bots in their class more broadly, and if allowed, for which assignments and in what capacity. Clarity with both class and assignment expectations is paramount, and it is useful to let students know when they can or cannot use this tool. In addition to your students, engage your teaching assistants in these conversations, making sure they are aware of your expectations so they can share the same message. 

Take an AI writing bot/ChatGPT for a test drive.
The functionality of AI writing bots may vary depending on what it is being asked. All answers given by AI writing bots come from the information it has access to so if the information is biased or incorrect, the answers will be as well; more significantly, the “probable” syntaxes that it “generates” need cross-checking with authentic sources. So test them out with prompts specific to your discipline, context, topic, etc, so you can better address student questions and have a better idea of if and when the tool might be useful. You will also be able to answer student questions if you have explored its abilities as well as its defects and biases that these tools may have.

Teach students the benefits and limitations of using AI writing bots/ChatGPT.
As students begin to use this new tool, they may need help knowing where it may be useful and what its limitations are. It also is beneficial to them to explain when and why it may or may not be appropriate to use it. For example, give a prompt to a writing bot and then annotate what worked well, what could be improved, and why. So, an effective practice is to either show students how AI tools are inadequate or problematic or to help them figure out how it can aid the learning process; avoiding or prohibiting is less likely to prevent problematic use.

Think about AI writing bots/ChatGPT as a tool.
Similar to the calculator, think about how you and your students can use this new tool both inside and outside the classroom. Where will it help your students, and when should it be avoided so as not to impede learning?

Continue to engage with your colleagues and get answers to your questions.
Additional resources can be found on CELT’s website and you can reach out to them if you want to discuss further. They will schedule more conversations around AI writing bots/ChatGPT, please let them know if you have any suggestions or would like more information. You can also reach out to the Library if you have questions about ChatGPT/generative AI research and references. Additionally, our academic leadership will continue to partner with our faculty, staff, and students in developing programming and considering more formal approaches to these questions.",There's a shorter version of this statement here: https://www.stonybrook.edu/celt/teaching-resources/aibot.php,Center for Excellence in Learning and Teaching
Kiley,58,University of Connecticut,Q49206,"32,669","24,371","8,298",R1,"41.80722222222222,-72.2525",Storrs,CT,,https://uconn.edu/,https://cetl.uconn.edu/resources/teaching-and-learning-assessment/teaching-and-learning-assessment-overview/chatgpt-ai-impact-on-teaching-and-learning/,"ChatGPT AI impact on Teaching and Learning
The original communication from the Office of the Provost was sent out on Jan 23, 2023 to guide instructors at UConn.

Addressing AI ChatGPT
Many faculty have reached out seeking guidance surrounding possible academic integrity issues triggered by the recent release of ChatGPT3 from OpenAI[1].

For those of you who are unfamiliar, Chat GPT3 is a Large Language Model tool developed by OpenAI. The tool was released in November 2022, is powered by large amounts of data (~175 billion parameters), and relies on deep neural networks to predict and generate text in response to user prompts. There is much debate about the quality of the text generated but there is both interest and concern related to AI-generated text.  Based on conversations to date, our faculty are simultaneously interested in learning how ChatGPT3 and similar chat bots might transform teaching, learning, and assessment in innovative ways, and concerned about students use of Chat GPT3 to answer test and exam questions and generate content for written papers and assignments. Below, we lay out a number of recommendations for our faculty.

Faculty members might take time to experiment with ChatGPT[1] for themselves and discover the capabilities and limitations for their particular discipline and context. One way to become more familiar with ChatGPT is to submit your own writing prompts or assignment questions and examine the type and quality of text generated. Making multiple attempts can illuminate the range of text generated to the same prompt. In addition, don’t be surprised if you get a message ‘ChatGPT is at capacity right now’.
Faculty are encouraged to engage their colleagues and students in conversation about ChatGPT. This will help us to crowdsource both pedagogical opportunities and assessment concerns.  Some faculty plan to have students use ChatGPT to generate essays and then have students critique and improve those essays[2]. Other faculty are busy revisiting their assessment strategy and assignment particulars. Still others told us they plan to ask students to provide writing samples at the beginning of the semester.  Another indicated they planned to replace some writing assignments with oral presentations with Q&A, or contingently reserve the right to do so. 
Faculty are encouraged to amend their syllabus[3] to add a statement that explicitly addresses student use of ChatGPT or similar tools use that is consistent with their teaching and learning philosophy. Faculty should be transparent about what students are permitted to use and under what conditions. Faculty might consider requiring students to include brief acknowledgement statements with assignments, ones in which they share (and perhaps reflect on) which tools they used in the process of completing their work. Two divergent examples are provided below and additional samples will be accessible on the CETL syllabus template.
Every faculty member should take advantage of this opportunity to reflect on their course and lesson learning outcomes and (re)consider the extent to which their assessment strategy is aligned to measure those learning outcomes, and to identify adjustments needed for existing assessments and opportunities for creating alternative authentic assessments.
We believe maintaining a balanced and realistic perspective of the impacts of AI is more productive and appropriate than a narrow focus on surveillance and detection. While we believe that the vast majority of Uconn students are committed to the highest ideals of academic integrity, we also know that some students may be tempted to use ChatGPT3 and other tools in ways not permitted. For this reason, we wanted to let you know that the University administration is exploring various AI detection software that faculty could use to identify possible AI generated content. Until then, faculty may choose to use open source experimental detection applications such as GPTZero and AI Content Detector, being mindful that interpretation of results should be tempered by the fact that neither provide definitive evidence.

Finally, we encourage all faculty to keep abreast of messaging and developments related to ChatGPT this semester. Check for more workshops and panel discussions on the topic forthcoming in the coming weeks and months. Sample Syllabus Language
Addressing student use of AI ChatGPT
Sample # 1 (Permitting use of AI ChatGPT)

Academic Integrity
In this course we’ll conduct ourselves as a community of scholars and writers, recognizing that academic study is both an intellectual and ethical enterprise. Please build on the ideas and texts of others–that’s a vital part of academic life. You may certainly discuss readings and assignments outside of class, study in groups, share drafts with classmates or friends, and go to the Writing Center with your drafts.

When you use or borrow or closely imitate another’s ideas or language–or even syntax–you must formally acknowledge that debt by signaling it with a standard form of academic citation. This means documenting not just direct quotations but also paraphrases and summaries. In less formal or creative genres, you may show your debt to a source (or classmate!) with a signal phrase (“According to Jose Calabra….”) or acknowledgement statement (“In this essay I drew inspiration from…I got the_____ idea  from Kayla during peer review.”). If you have any questions about when and how to credit the work of others, please come talk to me.

You are welcome to use AI writing tools such as ChatGPT on most assignments (I’ll alert you when you can’t) but whenever you use them, you must include an acknowledgement statement that briefly shares that and how you used them. For example, “I used ChatGPT when I was struck at the start and retained substantial parts of what it produced, including X and Y ideas and most of the wording in paragraphs 3 and 4” or “After I wrote my first 2 paragraphs, I used GPT-3 playground to extend the text for another 200 words but then edited…” Please also note that all large language models still tend to make up incorrect facts and fake citations. You will be responsible for any inaccurate, biased, offensive, or otherwise unethical content you submit, regardless of whether it originally comes from you or an AI tool (these last 2 sentences adapted from the course policies of Ryan S. Baker.pdf, University of Pennsylvania).

If you engage in intentional academic dishonesty–whether plagiarizing or submitting the work of others or copying from others on a test or failing to acknowledge use of AI or other tools–you will fail not only that assignment but the course.

Sample # 2 (Prohibiting use of ChatGPT)
All students are expected to act in accordance with the Guidelines for Academic Integrity at the University of Connecticut. If you have questions about academic integrity or intellectual property, you should consult with me or consult UConn’s guidelines for academic integrity. Posting course material on student tutoring and course sharing websites (e.g. Chegg, Course Hero) may be a violation of my copyright and intellectual property and a violation of academic integrity. Many of you may also be aware of the recent release of ChatGPT3, a Large Language artificial intelligence (AI) model that has the capacity to quickly produce text on a range of topics.  ChatGPT3 aggregates the ideas and insights of many researchers without giving them credit.  Submitting ChatGPT-generated text as your own work would be an act of plagiarism insofar as it would involve passing off the work of others as your own. For these reasons, you are not allowed to use this ChatGPT or other similar tools.to produce essays or other academic work for this class, unless otherwise explicitly permitted to do so.  You should also know that the university has AI detection software that distinguishes between AI generated content and human generated content.

For additional samples, see Lance Eaton's collection of syllabus policies.",I omitted the drop down menu of references and resources,Center for Excellence in Teaching and Learning
Kiley,60,Brandeis University,Q49119,"17,590 (2022)","3,591 (2021)[","1,967 (2021)",R1,"42.36566,-71.25974",Waltham,MA,,https://www.brandeis.edu/,https://www.brandeis.edu/teaching/chatgpt-ai/chatgpt.html,"Preliminary guidelines
CTL’s Evolving Guidelines for Dealing with chatGPT (last updated Feb 6, 2023)
[Note: this page was summarized as a handout for our chatGPT discussions on February 6th, 8th, and 10th.]

The following evolving guidelines are designed to help you address the challenge of teaching in the age of chatGPT while still emphasizing student learning and allowing for meaningful student assessment. Please treat these as preliminary guidelines as Brandeis develops its official policies.

Faculty members will differ on the extent to which they want to use or restrict the use of chatGPT in their courses. We recognize that every course is different and that every discipline will have unique concerns. Please reach out to the CTL at ctl@brandeis.edu with any specific issues you face or your feedback on these guidelines.

What does chatGPT struggle to do?
While chatGPT is pretty good at creating cohesive writing, computer  code, and images, they aren’t able to:

describe how (and why) a student revised their essay from their first draft to final drafts
reflect on how an essay made use of feedback provided during peer review sessions
refer to comments brought up during your class discussion
refer to personal anecdotes
refer to course materials
effectively use sources
provide accurate citations, a works cited list, or an annotated bibliography
include sounds or video
collect data or do interviews
Moreover, chatGPT often returns made-up sources or made-up quotes and often contradicts itself, such that users must always evaluate and verify any outputs generated by chatGPT. (As the program’s developers note in their blog, “ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers.”)

We therefore suggest…

1. When designing individual assignments:
A. Incentivize the process, rather than just the final written product. Include the steps and habits of mind that are associated with deep learning and critical thinking in your discipline in their assignment. Have due dates for individual elements that precede the final submission. For example, ask students:
First, to write a bullet-pointed outline with a thesis statement
Next, to provide detailed notes on sources (research articles, literary critiques, original documents, etc.)
Next, ask for a first draft / first best attempt.
Next, have students provide each other with feedback via peer review.
Then ask for a student to submit a final draft and a reflective paragraph/essay describing how their paper evolved throughout the process and made use of their peer’s feedback.
B. Assign more personalized writing, the more personalized the better. For example, ask for short, personal response essays to weekly readings.
C. Require that your students refer to material specific to your class, such as in-class discussions, LATTE discussions, or other unique materials.
D. Reference current events in your writing prompts/essay topics. For example, ask students to apply a concept or topic for your course to a recent event or discovery.
E. Ask students to use evidence and cite their sources in their papers.
F. Incorporate peer review of drafts and ask students to write reflective paragraphs about how they made use of their peer’s feedback as they finalized their papers.
G. Develop assignments that require original data collection and analysis through interview, observation, fieldwork, archival research, or other methodology.
H. Assign your students to create multi-modal essays that require sound, images, and video.

2. When thinking about your overall course structure and assessment practices:
A. Consider lowering the value of any single homework assignment by offering more frequent, lower-stakes in-class or homework assignments. Students are more likely to consider cheating on higher-stakes assignments.
For example, avoid homework assignments that are worth more than 25% of a student’s final grade.

B. Consider short writing assignments during class (these can eventually become incorporated into students’ longer papers or can be stand alone assignments).
C. Consider how chatGPT intersects with the goals of your course.
D. What intellectual skills do you want your students to develop? Are there ways they can practice and develop those skills without AI assistance?
E. What types of assignments are necessary to measure the extent to which students have learned the desired skills and concepts? Are there ways to ask them to demonstrate their progress towards learning these skills and concepts without AI assistance?
F. Consider changing the modality of your assessments. Can some written assignments be turned into oral presentations or podcasts?

3. Talking with your students about the value of writing in your course and discipline
Some students need a reason to want to do their own writing (rather than have chatGPT do it for them). Find opportunities to explain the value of writing in your course and discipline and to engage your students in discussions about the value of writing.
Make clear that while the written piece they produce matters, the most important aspect of writing is that it facilitates and enables their learning.
4. Addressing AI tools in your syllabus
Different faculty will have different expectations about whether and how students can use AI tools, so being transparent about your expectations is essential. If you want to forbid using AI tools, be explicit about this on your syllabus. If you allow these tools, but want them to be acknowledged in the student’s work, explain that on your syllabus and in class.

Here is some language you may consider including in your syllabus or using to discuss chatGPT with your students:

It is important to remember that chatGPT and other AI tools are not a replacement for your own critical thinking and original ideas. The ultimate goal of this course and any tool used to submit work is to enhance your own learning and understanding, not to undermine it.
As a college student, it is your responsibility to maintain the highest standards of academic integrity. Representing work generated by artificial intelligence as one's own work is considered to be academically dishonest. This includes (a) ensuring that all work submitted for grades is your own original work, and (b) properly citing any sources that you use.
Having AI write your paper constitutes plagiarism. If the source of the work is unclear, I may require you to meet with me to explain the ideas and your writing process.
If you consult with other students or use any sources on an assignment, report this in the work that you turn in. Do not generate new content with prompt-based AI tools like ChatGPT or CodePilot without permission from instructors unless specifically allowed by the assignment. (Using, for example, Grammarly as a language aid is OK.) Instructors reserve the right to request an oral explanation of answers.
If you have questions about what is permitted, please reach out to me.
Please see here for more examples of possible language you can include in your syllabus regarding AI tools.
5. Regarding AI detection tools:
A. While there are emerging tools designed to detect AI-produced content (including Hugging Face, GPTZero, CrossPlag, and Turnitin), we recommend against relying on them for several reasons:
These tools are not perfect.
Their false positives may cause faculty to falsely identify a student’s own work as being AI-generated. Chasing down these false leads can waste time and inspire unnecessary mistrust between faculty and students.
Their false negatives may cause faculty to falsely ascribe an AI’s work to the student.
B. These tools are generally only effective when the entire piece was produced by AI, but if the student has included some AI-generated work interspersed within their own words, it is much harder for these tools to detect.
C. Students are already coming up with ways to beat AI-detection tools: one prominent approach is to ask one AI-generating tool (e.g., chatGPT) to create the first draft of an assignment, and then to ask another AI-generating tool (e.g., CopyGenius and Quillbot AI) to rephrase each paragraph, making it much harder to detect.
D. AI-generating tools will evolve to evade AI-detecting tools, and then AI-detecting tools will evolve to get better at identifying AI-generated work. The arms race to detect AI-generated work and the time faculty spent staying current on the best AI-detection tools may have a higher cost than the benefit these tools provide.
E. Some AI-generated tools, such as chatGPT, may develop “watermarks” that allow the text they generate to be easily identified, but as AI-generating tools proliferate, some students will migrate to tools that don’t include these watermarks when they feel incentivized to do so.

In sum, tools that claim to detect AI-generated work are easy for students to work around if they want to, and it can be easy for faculty to invest large amounts of time into policing their students instead of investing time into fostering student learning. Rather than relying on AI-detection software, we recommend designing assignments that foster student learning that cannot be replicated by AI-tools (such as in-class writing; peer review; small, frequent, personal essays; essays that cite scholarship in the field or classroom discussions; projects that involve interviews or original data original; essays that cite current events; assigning multi-modal assignments, podcasts, explainer videos, or any of the other principles outlined above.)
 
In addition, some faculty may also want to use chatGPT in their course. For example, one can imagine that students can use chatGPT to apply concepts from class to generate initial ideas for papers or research projects, or to analyze and refine early drafts. Moreover, incorporating AI tools into one’s course can also provide training and practice for students entering professional environments where the use of AI will be a relevant skill.",,Center for Teaching and Learning
Kiley,60,Michigan State University,Q270222,"49,809 (fall 2019)","39,176 (fall 2019)","10,633 (fall 2019)",R1,"42.7018637482531,-84.48216117291386",East Lansing,MI,,https://msu.edu/,https://provost.msu.edu/-/media/assets/provost/docs/news/interim-guidance-on-generative-artificial-intelligence-2023.pdf?rev=0377813c1d714ca59115a05dc8f41593&hash=509659608C6C209EEFB603C37AC8CD9B,"Michigan State University Interim Guidance on Generative Artificial Intelligence (AI) in Instructional Settings As you prepare for the Fall 2023 academic semester, our hopes are that instructors will 1) develop a course-level generative AI use policy and actively discuss with students about expectations for generative AI use in the work for your class, 2) promote equitable and inclusive use of the technology, and 3) work with colleagues across campus to determine ethical and scholarly applications of generative AI for preparing students to succeed in an evolving digital landscape. The following materials were assembled using existing MSU policy documents and include framing questions developed by an interdisciplinary team of campus experts.  Develop and actively communicate your course-level generative AI policy 1. Consider how AI technology might compel you to revise your course assignments, quizzes, and tests to avoid encouraging unethical or dishonest use of generative AI. 2. Develop and integrate a generative AI policy throughout the course resources: • Provide clear definitions, expectations, and repercussions of what will happen if students violate the policy. • Explain the standards of academic integrity in the course, especially as related to use of AI technologies, and review the Integrity of Scholarship and Grades Policy. • Be clear about what types of AI are acceptable and what versions of the technology students can use or not use. • Put this policy into D2L and any assignment instructions consistently. 3. Discuss these expectations when talking about course policies at the beginning of the course and remind students about them as you discuss course assignments: • Take time to explain to students the pros and cons of generative AI technologies relative to your course. • Explain the development of your policy and make clear the values, ethics, and philosophies underpinning its development. • Explain the repercussions of not following the course policy and submit an Academic Dishonesty Report if needed. 4. If you want to integrate AI in the classroom as an allowed or required resource: • Consult with MSU IT guidance about recommendations for use and adoption of generative AI technology, including guidelines for keeping you and your data safe. • Determine if MSU already has access to the tools you desire for free, and if not available through MSU, consider the cost and availability of the resources you will allow or require, and go through MSU's procurement process. • If you want to require students to use an AI technology that comes with a cost, put the resource into the scheduling system as you would a textbook, so students know that is an anticipated cost to them. Promote Equitable and Inclusive Use 1. Consider equity and inclusion when making decisions about AI use in your course. 1.1. How does the development and use of generative AI affect identity groups differentially? What biases exist within the development and use of generative AI? What are the potential challenges regarding AI from an equity-lens (e.g., historic issues with facial recognition and BIPOC populations)?  1.2. What data sources does generative AI use to generate a response, and how representative is this data source? 2. Consider how AI content and perspectives can enhance dialogue and collaboration between diverse disciplines, departments, and individuals. 3. Consider how integration of generative AI technologies into the classroom help or hinder students’ success. 4. Consider situations in which some students may have access to more advanced technology than others based on cost or other factors. 5. Consider if generative AI technology provides accommodation for certain populations and how its use may help achieve equity for persons with disabilities. Determine Ethical and Scholarly Use The determination of appropriate uses for generative AI can be facilitated through discussion with colleagues within and among disciplines. Discussions that can help our communities to answer 1. What is the ethical use of AI in society, in a given scholarly discipline, and in instruction? 2. How does generative AI pose ethical challenges to issues such as data security and privacy? 3. What types of information should and should not be inputted into an AI system? 4. When does generative AI-assistance become AI-ownership? What are the limits to using generative AI in support of academic work? 5. How should a student cite or disclose the use of generative AI relative to their academic work? 6. How does the course/instructor define plagiarism and academic dishonesty relate to AI? What are the penalties for not following the policy? Many philosophies and policies outlined on the OSSA Academic Integrity website already apply, and additional specific guidance is available on FAQs specifically addressing generative AI. AI as a complex issue requiring multiple perspectives and dialogue 1. How can generative AI technologies assist with collaborative, integrated, and interdisciplinary work in the classroom? 2. What is the appropriate use of generative AI in the subject-area and/or discipline to advance scholarship and maintain excellence? 3. What are the current uses, concerns, and dialogue relative to generative AI within the subject and/or discipline? 4. How will generative AI affect the unit/major/courses’ content, assignments, instructional objectives, learning outcomes, or assessment models? Might these outcomes need to change to account for AI technology? 5. How will generative AI influence job growth, skills, and responsibilities in the future? To help facilitate this work, the Center for Teaching and Learning Innovation (CTLI) and the Enhanced Digital Learning Initiative (EDLI) will be producing resources, offering workshops, and facilitating discussions throughout the year. You can learn more about generative AI and participate in discussions in the “AI & Education” group on iteach.msu.edu. ","This is linked from a brief statement by the Provost, but interestingly doesn't have its own webpage. There is a separate statement from IT about data risks.",Office of the Provost
Kiley,60,"The Pennsylvania State University, University Park",Q739627,"89,816","74,446","15,370",R1,"40.796111111111,-77.862777777778",University Park,PA,College of Information Sciences and Technology,https://www.psu.edu/,https://aiai.psu.edu/frequently-asked-questions/,"FAQ
Introduction
Artificial intelligence (AI) tools like ChatGPT have  seen rapid advances recently, and there is evidence that students are actively using these tools to assist in academic writing. This document provides an overview of relevant issues and provides some practical recommendations for adapting your assignments and assessments in light of these new capabilities.

There are many options for support available at Penn State to assist in updating your instruction. We encourage you to identify learning designers and faculty development professionals at your college or campus who are knowledgeable on this topic:


Frequently Asked Questions
What are some strategies I can adopt immediately (without significant redesign of my course)?
If possible, require students to provide in-text citations and a list of their sources for written assignments, including discussion posts, projects, and papers.
Consider changing the resulting assignment submission from written to creative and visual or oral. Instead of a paper, require students to create presentations, infographics, concept maps, videos, etc. Also ask students to include a reflection along with their submission, where they talk about the process and what they learned.
Consider varying the types of information that you provide for students to use when completing an assignment. Instead of or in addition to written information that can be found online, include in-class guest speakers, videos shown in class, online videos such as TedTalks, interviews, virtual environments, etc.
Although created during the pivot to remote learning, instructors may find the information on that site helpful for addressing ChatGPT and academic integrity issues
Communicate to students via conversation, a Canvas announcement or email acknowledging your awareness of A.I. tools like ChatGPT and what your expectations are for use of these technologies in your course.
Add a statement about the use of AI to existing assignments. For example:
You must complete this work entirely on your own. You may not help other students or use any online sites, technologies, tools, or sources that are prohibited. If you use any ideas, images, or word phrases created by another person or by generative technology (such as ChatGPT), you must identify their source. You may not share any information about, or from, this assessment with others.  If you have questions about these instructions, you should discuss them with your instructor before you begin.

What kind of assignments are susceptible to the use of A.I.?
Assignments (and rubrics) that stress structure over process through awarding most points for organization, word counts, types of paragraphs, grammar, etc.
Assignments that ask students commonly used questions about topics that are used for a particular subject matter. These questions typically have one correct answer and do not allow for individual interpretation in the students’ responses.  For example, asking the students to summarize a well-known theory, discuss the symbolism of a historical painting, critique a famous speech, state the pros and cons of a political policy, etc.
Assignments that do not require students to use in-text citations and a list of sources.
 

How can I test my assignments to determine if they are susceptible to the use of A.I.?
It is recommended that instructors visit ChatGPT and input the text of the assignment into the chat prompt and use the tips below. Note that although access is free, you must create an account  to use the technology.

Regenerate the response multiple times or slightly alter the initial prompt, as the results will be different each time. This will give you a sense of the variety in the responses.
If your assignment has multiple sub-prompts, input each of them separately during a single chat session to get higher-quality results.
Utilize the back-and-forth dialogue to refine your answers to determine the level of detail that can be obtained by more user-savvy students.
 

How should I modify assignments with a writing component for future course offerings?
Generally speaking, you may take two positions on the use of A.I. writing tools: 1) explicitly prohibit their use, or 2) constructively incorporate their use into your instructional approach. To modify your assignments, consider the recommendations below. Your local support resources can also help.

View writing as a process rather than an outcome.  By using the writing process, the instructor can make assignments more personalized and become familiar with the students’ thought process and writing style. Use unit-long or semester-long assignments that follow the writing process, including brainstorming topics, creating an outline, writing the first draft, revising the draft and reflecting on the writing process. Require students to revise drafts based on the instructor’s review and feedback. If peer review is used, ask the students to submit their peers’ feedback along with their final draft. Consider using fewer writing assignments to ensure the instructor has enough time to carefully evaluate the students’ submissions. 

Teach skills for finding and citing sources. Using in-text citations and a list of sources can assist instructors in checking the accuracy of the students’ responses. Although ChatGPT can include in-text citations in its output,  theyare usually incorrect or contain false information. Penn State’s library offers some excellent resources for students to learn about research, including “How to” guides for research skills and information on commonly used academic citation styles. See the links below:

https://libraries.psu.edu/guides/howto
https://guides.libraries.psu.edu/CitationStyles
Situate your assignment in personal, local, or recent issues.  AI tools like ChatGPT are only capable of providing general information about a topic based on what is in its database, which currently only includes information prior to 2021. By using personal or local issues, a student will be limited to a generic response that they can use as a starting point, but it will require additional information from them to meet the requirements of the assignment. Students may also find this type of assignment more interesting and be more likely to write it without assistance.

Use an Experiential Learning approach: Consider elements of experiential learning, including using authentic, real-world contexts (field trips, case studies, school or work scenarios, virtual environments); presenting the assignment as a challenge; requiring students to make decisions to reach an authentic outcome; and asking students to look back and reflect on their experience.

Create assignments that use a combination of visual and written resources. Provide students with a variety of formats to inform/guide their work. Use guest speakers, videos, podcasts, virtual environments, interactive case studies, etc., in addition to textbook readings and articles.

Use course-based research. Use assignments that require active cognitive learning. Students can create and investigate research questions through interviews, surveys, experiments, and observations. These types of assignments will keep the students engaged and require them to analyze data, arrive at a conclusion that is unique to their situation, and explain how their research changed their thinking.

Consider writing alternatives. Ask yourself if original writing is an outcome of the course. If not, consider what might be the best modality for equitably assessing student learning.  There are myriad ways to assess student learning alongside authoring traditional papers, from portfolios to journals to presentations (live or recorded), to podcasts, meme assignments, and a host of other creative strategies. For now, though, instructors could measure how well a student’s thesis is supported by ideas, evidence, and arguments, and whether optimal organization is used. This could lead to presentations in place of written papers, or even collaborative writing sessions during class, if appropriate for the course outcomes.

Is it possible to detect if a student has used AI?
Faculty should consider the following before using AI detector tools to check students' work: 

AI detectors may flag text that is edited by AI embedded/hidden in commonly used applications (e.g., Grammarly)
The accuracy of a particular detector depends on the version of the detector and both the version and the source of the tool that generated the writing
Instructors who believe that a student used a prohibited tool to generate coursework should discuss their concern with the student and check the student's submission for fabricated information (e.g., quotes, sources)
Once information is submitted to an AI detector, the detector's company can use and share that information freely
Given these considerations, there are several tools which claim to be able to detect A.I. writing (some are free or require payment)  including:

GPTZero
OpenAI Text Classifier
Originality.ai
Content at Scale
Writer.com
You may also direct students to these sites to check their own writing.

NOTE: many of these tools provide a percentage confidence that the submitted writing is generated by A.I. This is not definitive proof that a student used A.I. or otherwise plagiarized their work.

If I suspect that a student has used AI to write a significant portion of an assignment, what should I do?
Consult with the Academic Integrity representative at your College or Campus. The process will be the same as it would be for any Academic Integrity violation.
Are Penn State’s Academic Integrity policies being updated?
The academic integrity issues around A.I. generative technologies are not new and are like those around the use of more constrained generative technologies (e.g., a language translator in a language course) or the use of other prohibited sources, tools, or aids. 

Per policy G-9: Academic Integrity, an academic integrity violation is defined as “an intentional, unintentional, or attempted violation of course or assessment policies to gain an academic advantage or to advantage or disadvantage another student academically.” At Penn State, submitting artificially generated text to gain an academic advantage would qualify as an academic integrity violation if course or assessment policies prohibit use. G-9: Academic Integrityalso provides information about common types of academic integrity violations, including the use of unauthorized/prohibited tools and technologies and the misrepresentation of one’s work, words, results, processes, or ideas, in whole or in part, without attribution.

Beyond writing assignments, are there other types of assignments that are susceptible to the use of A.I.?
Yes,  A.I tools go beyond just writing and impact many different fields, for example:

Arts and Architecture: Image generation tools exist that can create impressive works of art, including DALL-E and MidJourney.
Computer Science and Information Science and Technology: ChatGPT, GitHub CoPilot and Codex assist with creating code.
Mathematics: ChatGPT can perform mathematical calculations including algebra, calculus, geometry, number theory and probability and statistics. Its mathematical capabilities were updated on January 30, 2023.
Medical Education: Glass AI creates treatment plans based on diagnostic data that is entered into the system.
What if I want to incorporate ChatGPT or other AI tool into my assignments?
AI tools like ChatGPT that are used in a course must go through Penn State’s Courseware Review process. In February 2023,  a request for university-wide use was submitted for ChatGPT, but it may take several months to get the necessary approvals. Once approved, instructors who use ChatGPT may want to include a module or tutorial in the first week of their course that addresses AI literacy to assist students in using the tool effectively and evaluating the information that it provides. Expectations for the use of the AI tool must be clearly explained to the student in the syllabus and on the first day of class.

Can I use online proctoring solutions to prevent students from inappropriately using AI technology?
While this is one option available to instructors, there are many considerations that you should be aware of prior to adopting online proctoring. Penn State has provided a document outlining these considerations. In summary, online proctoring is not the only or even the best approach to ensuring academic integrity and it should be used judiciously, and generally limited to high-stakes exams.",This is described as a FAQ but is general guidelines for faculty,Unknown
Kiley,60,Santa Clara University,Q992830,"9,015 (fall 2019)","5,438","3,296",M1,"37.349166666667,-121.93805555556",Santa Clara,CA,,http://www.scu.edu,https://www.scu.edu/provost/teaching-and-learning/faculty-collaborative-for-teaching-innovation/digital-resources-for-teaching-drt/supporting-students/ai-in-the-classroom-and-what-about-academic-integrity/,"AI in the Classroom (and what about Academic Integrity?)

The use of AI in academic contexts has become a popular topic of discussion in recent months – even though AI as a field has existed since the 1950s and more complicated tools have been in existence since the 1970s. AI in general may not be a new field, but how students leverage AI in their coursework is changing in substantial ways. Like it or not, AI tools are here to stay and will continue to change. So what can we as faculty do with and about AI? And how can we help our students use it wisely?

Teaching Strategies from SCU Faculty
We’re navigating recent developments with AI in our classrooms at SCU. The Collaborative for Teaching Innovation hosted three demonstrations and conversations about Generative AI in Winter and Spring Quarters of 2023. Here’s a summary of a few key points from these discussions. 

1) AI Tools Generate Information (but they don't analyze it, it may not be accurate, and they make stuff up)

AI tools like ChatGPT are large language models (LLMs) that scour the internet for information and make connections between words and phrases. ChatGPT, in particular, has been used to complete writing tasks, generate Excel-related prompts, draft lab reports, and do coding assignments quickly. They often lack the sophistication, details, critical perspective, and specificity to course content and contexts that we expect in excellent student writing. For example, it can explain theoretical concepts well (because of an abundance of texts on the topic online), but it cannot generate a deeper analysis of the theory. It also cannot draw information from non-text modalities like videos, nor can it, obviously, draw from non-internet sources, such as the nuanced and particular discussions that you and your students are having in class. Oftentimes, ChatGPT-generated writing is recognizable because of its use of repetitive phrases or stylistic constructions, and a lack of specific contexts. It’s worth talking with students about what ChatGPT can and cannot do: students are still learning about the affordances and limits of these technologies. So are we. So is everyone. 

2) AI and Digital Pedagogy

When you are inclined to worry that the world of knowledge (and teaching and learning) is coming to an end, think about the use of new AI tools as an extension of already existing digital pedagogical practices. Digital pedagogy can look different depending on the instructor and their discipline: lectures may be paired with online research conducted by students to create a digital artifact. Faculty may use the “Designer” feature on PowerPoint to make their presentations more visually appealing without a lot of work. Faculty may allow or even encourage students to use Grammarly and other similar tools to check their writing. Faculty may also use such tools themselves. With the abundance of available digital tools faculty can set boundaries and expectations by talking to students about their use of generative AI tools. How do you expect students to use these tools (if at all)? Why? Can you imagine using generative AI tools to support student learning? Many faculty here and elsewhere are addressing these issues in syllabus statements and assignment prompts. As with many technological tools, talk to your students. Don’t assume they know what’s expected or appropriate. 

3) Focus on Increasing Access and Equity

ChatGPT and similar AI tools can increase access to learning resources for students with diverse learning needs. Captioning, audio description, text-to-speech, and speech-to-text are examples of generative AI tools that support inclusive learning. Additionally, students who struggle with writing may benefit from using ChatGPT to help them develop their ideas or improve their writing. Collaboratively writing with ChatGPT, for example, may provide an initial structure for students to strengthen their positions, enhance and specify their arguments, integrate their own voice and perspective,  or edit for clarity. Generative AI may help students reduce stress and spend more time engaging with course content to deepen their learning. Although Generative AI, including ChatGPT, may offer important assists to student learning, it cannot offer personalized support, mentorship, and the relational aspect of teaching and learning, which is fundamental to what we all do at SCU.

4) But What about Misinformation, Bias, and Academic Integrity?

As with any teaching tool, generative AI delivers both benefits and challenges. The latter includes issues of privacy, racism, and sexism (information is scoured from the internet after all), misinformation, fake citations, and even malicious content. Decades of research on academic integrity (apart from the use of AI) directs us away from an exclusively punitive approach and toward pedagogical practices that acknowledge what academic integrity looks like in our discipline. As a start, include specific statements in your syllabus and assignments about using AI (some examples are linked below).",I didn't include the resources at the end,FACULTY COLLABORATIVE FOR TEACHING INNOVATION
Kiley,60,"University of California, Merced",Q1728622,"9,148 (Fall 2023)","8,373 (Fall 2023)",775 (Fall 2023),R2,"37.366,-120.4235",Merced,CA,,https://www.ucmerced.edu/,,"Addressing AI in Courses & Syllabi
This content was co-developed with the UC Santa Barbara and UC Berkeley Centers for Teaching and Learning and ChatGPT (OpenAI's GPT-3.5, 2023). At UC Merced, feel free to get in touch with faculty who use ChatGPT in their classes, like Sylvain Masclin at smasclin@ucmerced.edu. 

ChatGPT and other forms of generative artificial intelligence (AI) are booming. As instructors, it is important to consider what generative AI means for your teaching. What are the expectations for the use of AI in your course and how are you communicating that to your students? In other words, what is your AI usage philosophy? This guide is an evolving set of resources curated for you to help support your discussion of generative AI, like ChatGPT, in your class.

Main Takeaway: Communicate Expectations to Students
Whatever your position and subsequent decision about the use of AI that you make as an instructor, explain why you made this decision for your class. Have an open and honest conversation with your students about using generative AI responsibly and ethically.
Encourage students to consider the limitations and implications of these technologies.
Start with the syllabus.
What is Generative AI
Generative AI such as ChatGPT, GPT-4, Bard, DALL-E 2, and Midjourney, is a rapidly evolving content creation technology. As these tools advance, we can expect to see more innovative applications that leverage the creativity and generative capabilities of AI. However, it is crucial to approach the use of generative AI responsibly and address ethical considerations surrounding its applications, such as misinformation and potential biases.

Need more guidance on AI's opportunities and threats in the classroom? Consider this document which links a series of talks from a symposium at UC San Diego covering academic integrity, assessment, and a student panel. 

Example of How to Discuss Generative AI in the Syllabus
The syllabus is the best place to provide guidance to students about the appropriate use of generative AI. There are myriad ways to approach addressing generative AI in your syllabus.  First, consider what you would like to factor into your AI policy. The following list contains some examples of topics you may consider including that aim to emphasize the responsible use of these technologies to maintain ethical standards in the learning environment.

Emphasize Critical Thinking: This course places a strong emphasis on developing critical thinking skills when exploring and utilizing artificial intelligence (AI) technologies. Students are encouraged to approach AI with a critical and analytical mindset, evaluating its applications, implications, and limitations to make informed decisions and contributions.
Originality of Work: All submitted work must be the original creation of the student, unless otherwise specified in the assignment guidelines. The use of generative AI models, such as language models or content generators, to produce entire or significant portions of the work is not allowed unless explicitly stated otherwise by the instructor.
Attribution and Citations: When incorporating content generated by AI models, proper attribution and citation practices must be followed. If you use generative AI to assist in research, writing, or content creation, acknowledge the AI's contribution and include appropriate references for the underlying data or models used.
ChatGPT Citation: OpenAI. (2023). ChatGPT (Mar 14 version) [Large language model]. https://chat.openai.com/chat
Plagiarism and Academic Integrity: Plagiarism, including copying and submitting work created by others (including AI-generated content), without proper attribution, is strictly prohibited. Any instance of plagiarism will be subject to the institution's academic integrity policy, which may include penalties ranging from failing the assignment to academic disciplinary actions. [See UC Merced's policies on plagiarism and academic honesty here]
Assessment Guidelines: If AI-generated content is permitted in specific assignments or projects, explicit guidelines will be provided. Follow these guidelines carefully to ensure that you comply with the requirements for responsible use of AI in academic work.
Understanding AI Limitations: Recognize that generative AI models have limitations, such as potential biases in the training data and the inability to fully grasp context and nuance. Avoid relying solely on AI-generated content for critical or sensitive assignments that require human understanding and judgment.
Data Privacy and Security: When using generative AI tools or platforms, prioritize the privacy and security of your data. Be cautious about sharing sensitive or personal information while experimenting with AI technologies.
Peer Collaboration: Collaboration with peers is encouraged, but when using generative AI tools, each student must contribute their original ideas and content to the project. Do not share AI-generated content as your sole contribution to group work.
Course-Specific Guidelines: Some assignments or courses may have specific rules or exceptions regarding the use of AI-generated content. Always review and follow the instructor's guidelines for individual projects.
Instructor's Discretion: The instructor reserves the right to use plagiarism detection software or other methods to verify the originality of students' work, including the identification of AI-generated content.
Example Syllabi Content by Faculty
This living document with over 50 examples of classroom policies submitted by faculty across institutions from the Higher Ed Facebook collective and curated by Lance Eaton. For the Spanish version of these policies, check out the work curated by Tatiana Torres Zapata. Continue reading for a few more examples:

Amanda Goldberg syllabus policy

In this course, students may use AI tools such as grammar checkers and citation generators to assist with their writing. These tools can be helpful in identifying errors and formatting citations, but they should not be relied upon completely. It is the responsibility of the student to proofread and carefully review their work before submission to ensure that it meets the standards of academic writing.

Students should also be aware that AI tools may not always produce accurate or appropriate results, and they should not blindly trust or blindly follow their suggestions. It is important to critically evaluate the output of AI tools and use your own judgement and knowledge of the subject matter to make informed decisions about your writing.

Plagiarism is strictly prohibited in this course. Students should not use AI tools to generate original content or to rewrite existing content in an attempt to avoid plagiarism. Students should also be aware that AI tools may not always accurately detect plagiarism, and it is their responsibility to properly cite all sources used in their writing.

By using AI tools in this course, students acknowledge and accept the limitations and risks associated with their use. The instructor and the college are not responsible for any errors or issues that may arise from the use of AI tools.

 

Inara Scott, OSU College of Business

I expect you to generate your own work in this class. When you submit any kind of work (including projects, exams, quizzes, or discussions), you are asserting that you have generated and written the text unless you indicate otherwise by the use of quotation marks and proper attribution for the source. Submitting content that has been generated by someone other than you, or was created or assisted by a computer application or tool, including artificial intelligence (AI) tools such as ChatGPT is cheating and constitutes a violation of the Student Conduct Code. You may use simple word processing tools to update spelling and grammar in your assignments, but you may not use AI tools to draft your work, even if you edit, revise, or paraphrase it. There may be opportunities for you to use AI tools in this class. Where they exist, I will clearly specify when and in what capacity it is permissible for you to use these tools.

Example of guidance on individual assignments:

I want to address the new AI-tools that you may be hearing about, such as ChatGPT, and their possible role in this project. I want you to be aware that ChatGPT is based on a large language model--it is basically crowdsourcing information and providing likely answers based on the vast amount of text in its database. While it can provide some helpful information, and may spur your thinking in some areas, it is not a reliable source and cannot provide citations or references to reliable data or evidence. (If you ask it for a citation, be aware that it makes things up and the information it's giving you is likely garbage!) 

So, can I use ChatGPT or other AI tools to help write this paper?

Things you can do: ask ChatGPT questions! I personally enjoy chatting with it about topics I'm interested in. For example, ""What are some current issues related to sustainability in the airline industry?"" When you read what it says, keep in mind that it's probably at least 60-70% correct, but perhaps not more than that. :-) Given that you're considering whatever it told you with a big grain of salt, you'll then need to do some research to find peer reviewed and reliable evidence that might corroborate (or disagree with!) what the AI tool told you. Use those articles to find other articles that consider the same question (review the citation list for other articles to read). Either before or after you ask ChatGPT a question, try a google search with the same sort of query and see what it turns up; also, try a search on the OSU library system. Review, compare, and investigate. Repeat this cycle, keeping in mind that what you're getting from AI is crowdsourced information, not the reliable product of research and assessment.

Things you cannot do: Do not use ChatGPT to draft your paper. Do not use ChatGPT to give you citations. I am saying this both for purposes of coming up with reliable evidence and also from an academic integrity (i.e, cheating) standpoint. If you didn't write it, don't put your name on it and claim that you wrote it. Don't modify a few words here and there and claim you wrote it either. Close the window before you start drafting and put the real evidence and articles you've found into your own words. Do your own analysis and critical thinking.

 

Kristy Kelly, OSU College of Liberal Arts

Strong writing and research relies on the appropriate attribution of sources. In this class, we’ll have many conversations about what counts as a source, and how to draw clear lines around where your ideas begin and others’ end. This question is complicated by the ubiquity of tools like ChatGPT, Grammarly, Chegg, and even Google’s autocomplete function that are increasingly embedded in students’ writing and study practices. As part of our learning about digital literacy and the appropriate attribution of sources, we'll discuss what counts as “original” writing with the increasing presence of this network of tools, so we’ll talk about how to use those tools appropriately without over-relying on them or threatening the originality of your work.

Since the OSU Student Conduct Code defines Academic Misconduct as “an act of deception in which a Student seeks to claim credit for the work or effort of another person, or uses unauthorized materials or fabricated information in any academic work or research,” we’ll be mindful to avoid claiming the “work or effort” of a machine or AI, as well as another person, as our own. The baseline expectation for our class is that all of the work you submit is your own original writing. I may invite you to use other tools as part of an assignment process, but until then, you’ll be expected to use your brain as the primary tool for creation.

 

 

Diving Deeper: Utilizing AI in Activities and Assessments
When using AI in assessment, consider the following policies to promote fairness, accuracy, and ethical considerations among your students:

Establish transparency and communication with your students.
Establish clear learning outcomes with your students (INTERNAL LINK) and consider how generative AI can support the learning outcomes.
Test generative AI yourself before using them in any assignment and share your experience with your students.
Create assignments where students use AI to get inspiration or build upon ideas
Download 101 Creative idea to us AI in education (Nerantzi et al., 2023 #creativeHE collection)
Consider some practical assignments using AI:
Identify misconceptions or errors in AI-generated text
Recognize the limitations in the response of AI
Evaluate AI algorithms and applications
Examine biases and fairness
Discuss ethical considerations of AI
Explore AI and social impact
Develop an AI application
Design and assess writing assignments with AI in mind
Consider co-grading with AI",,Teaching Commons
Kiley,67,George Washington University,Q432637,"26,457 (2021)","11,502 (2021)","14,955 (2021)",R1,"38.900833333333,-77.050833333333",Washington,DC,,https://www.gwu.edu,https://provost.gwu.edu/sites/g/files/zaxdzs5926/files/2023-04/generative-artificial-intelligence-guidelines-april-2023.pdf,"Guidelines for Using Generative Artificial Intelligence at the George Washington University April 2023  The Promise of Generative Artificial Intelligence  The wide availability of Generative Artificial Intelligence (GAI) tools, such as ChatGPT and other large language models, is driving an ongoing conversation about their academic uses.  GAI tools represent an exciting addition to the learning process that can be deployed in innovative ways to advance learning objectives.  This document provides some guidelines for the use of GAI in connection with academic work at the University.  The Office of the Provost encourages the entire University community to embrace these technologies through creative uses and applications.  Faculty are invited to make thoughtful use of GAI tools in their teaching and research.  Used properly, GAI tools can enhance the design of lessons, assignments, and assessments.  Our students will use GAI tools for the rest of their lives.  There are many productive ways in which they might use them as students, consistent with stated course policies and objectives.  Examples include: brainstorming ideas; summarizing and translating content; explaining new concepts to aid comprehension; generating counter-arguments; suggesting titles; debugging code; gathering sources; and formatting references.  Designing Assignments  Even as we are learning ourselves, we must teach our students to use GAI tools effectively and responsibly: to draft appropriate prompts; to think critically about the proper use of the tools and their possible effects on society; to evaluate their outputs with respect to accuracy, bias, and equity.  Exercises might include having students formulate effective prompts; identify superficial rhetoric in GAI-generated content; and evaluate GAI-generated arguments for soundness and  logical validity.  Students could be asked to fact-check, criticize, and/or edit GAI-generated content for credit.  The Instructional Core in the Division of Libraries and Academic Innovation has provided useful guidance for instructors on Responding to Generative Artificial Intelligence (AI) Tools.  The Office of the Provost encourages instructors to consult these resources.  Encouraging Responsible Use  For all their promise, GAI tools misused could interfere with learning objectives and impair the development of students’ writing, analytical, and technical skills.  There are also legitimate concerns about academic ethics, accuracy, citation of sources, and cheating.  The Office of Student Rights & Responsibilities maintains a Faculty Guide to Clarifying Academic Expectations that directly addresses the use of GAI tools in connection with academic work.  An instructor who suspects an academic integrity violation should consider submitting a Charge of Academic Dishonesty.  Note: the Law School and the School of Medicine and Health Sciences maintain their own codes of academic integrity, so the guidance contained in this document does not apply to these schools.  The Office of the Provost encourages instructors to state explicitly and affirmatively their expectations regarding student use of GAI tools.  Instructors should specify in writing the permitted and prohibited uses of GAI tools in their courses.  Instructors might 1) generally permit the use of GAI tools; 2) generally forbid their use; or 3) permit their use for certain purposes on certain assignments, but not others.  If an instructor wishes to permit certain uses of GAI tools, such uses must be set forth explicitly in the course syllabus and/or assignment instructions.  Below is some model language for the three permission options: General Permission Generative Artificial Intelligence (GAI) tools such as ChatGPT are becoming important resources in many fields and industries.  Accordingly, you are permitted to use such tools to generate content submitted for evaluation in this course, including [papers; take-home examinations; specified other assignments].  You remain responsible for all content you submit for evaluation. [Instructors might also wish to include language regarding pitfalls, such as the following:] You may use GAI tools to help generate ideas and brainstorm.  However, you should note that the material generated by these tools may be inaccurate, incomplete, or otherwise problematic.  Beware that use may also stifle your own independent thinking and creativity. [Instructors might also wish to include language regarding citation, such as the following:] If you include content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) in work submitted for evaluation in this course, you must document and credit your source.  For example, text generated using ChatGPT-4 should include a citation such as: “ChatGPT-4. (YYYY, Month DD of query). ‘Text of your query.’ Generated using OpenAI. https://chat.openai.com/.” Material generated using other tools should be cited accordingly.  Failure to do so in this course constitutes failure to attribute under the George Washington University Code of Academic Integrity. General Prohibition By submitting work for evaluation in this course, you represent it as your own intellectual product.  You may not submit for evaluation any content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models).  Doing so in this course constitutes cheating under the George Washington University Code of Academic Integrity. [Instructors might also wish to include language regarding papers about AI:] Although this course generally prohibits use of GAI tools, the instructor may choose to grant an exception if you propose to write a paper about [some aspect of artificial intelligence].  Such an exception must be granted in writing (e.g., email) to avoid the danger of misunderstanding. Selective Permission By submitting work for evaluation in this course, you represent it as your own intellectual product.  You may not submit for evaluation any content (e.g., ideas, text, code, images) that was generated, in whole or in part, by Generative Artificial Intelligence tools (including, but not limited to, ChatGPT and other large language models) unless the instructor has explicitly granted permission to do so.  Your instructor will explain to you the uses of GAI tools that are permitted or prohibited in this course, including on what specific assignments use of GAI tools is permitted.  Submitting content for evaluation that was produced in whole or in part by GAI tools, except for the specific purpose(s) and assignment(s) discussed and authorized by the instructor, constitutes cheating in this course under the George Washington University Code of Academic Integrity. [Instructors choosing the Selective Permission option might also wish to include some or all of the additional potential language from the General Permission and General Prohibition options, above.]   Default Rules  In the absence of explicit directions to the contrary from instructors, the following default rules apply at the University.  1. Work submitted for evaluation is represented as the student’s own intellectual product.  Students may not submit content (e.g., ideas, text, code, images) for evaluation that was generated, in whole or in part, by Generative Artificial Intelligence tools (such as ChatGPT and other large language models).  Doing so without instructor’s explicit permission constitutes cheating under the Code of Academic Integrity and is therefore prohibited.  Examples (illustrative only) of conduct that is prohibited unless explicitly permitted by the instructor: • A student types a prompt into a GAI tool and pastes all or part of the generated content into their answer on an out-of-class assessment or test. • A student types a prompt into a GAI tool and incorporates all or part of the generated content into an essay submitted for evaluation, without proper attribution to the GAI tool.  2. Students are permitted to use GAI tools to generate content that is not submitted to an instructor for evaluation.  For example, using GAI tools to study for examinations, tests, and quizzes is permitted.  Likewise, on assignments where the use of the Internet is not otherwise prohibited by the instructor, GAI tools may be used for learning, studying, and brainstorming.  Examples (illustrative only) of permitted conduct: • A student types a prompt into a GAI tool and reviews the generated content to help them study for a test. • A student types a prompt into a GAI tool and uses the generated content to help them brainstorm ideas for a term paper or research project.  3. Unless the instructor explicitly states otherwise in advance and in writing, the use of GAI tools during any assessment (e.g., examination, test, quiz) whether taken in the classroom or elsewhere, constitutes cheating under the Code of Academic Integrity and is therefore prohibited.  This prohibition includes assessments for which the use of the Internet is otherwise permitted.  Examples (illustrative only) of conduct that is prohibited unless explicitly permitted by the instructor: • While taking an out-of-class (“take-home”) test on which Internet use is generally permitted, a student types a prompt into a GAI tool and incorporates some or all of the generated content into their submitted answer. • While taking an in-class quiz on which Internet use is generally permitted, a student types a prompt into a GAI tool and incorporates some of the ideas generated into their submitted answer. • Before taking an in-class quiz on which Internet use is generally permitted, a student types a prompt into a GAI tool, saves the generated content to a document, and pastes some or all of the text into their submitted answer while taking the quiz.  Final Thoughts  As we enter this new technological era, the Office of the Provost encourages instructors to confer with the Instructional Core if they have questions about best practices for course design, pedagogy, assessment, and with the Office of Student Rights and Responsibilities for questions about academic integrity.  Further, we appreciate that this technology is evolving, and we will update this guidance as circumstances require. ",,Provost
Kiley,67,"University of Massachusetts, Amherst",Q15142,"32,045 (Fall 2021)","24,231 (Fall 2021)","7,814 (Fall 2021)",R1,"42.388888888889,-72.527777777778",Amherst,MA,,https://www.umass.edu/,https://www.umass.edu/ctl/how-do-i-consider-impact-ai-tools-chatgpt-my-courses,"How Do I Consider the Impact of AI Tools like ChatGPT in My Courses? 

Students may be interested in using artificial Intelligence (AI) tools like ChatGPT (an AI tool that can immediately craft text in response to users’ prompts) to enhance their own writing. Conversely, faculty may be apprehensive of students’ AI use in demonstrations of their learning, such as through writing research papers, creating code and scripts, or solving problem sets. However, some instructors may even be interested in using AI tools in their courses to create novel learning experiences. As you navigate where you might fall on this continuum, we encourage you to review the following strategies and examples when considering AI use in your courses. 

STRATEGIES & EXAMPLES
Examine Privacy and Data Collection Practices. Before encouraging or asking students to use AI tools in their work, investigate how these tools collect personal information and data, including reading any privacy policy linked with a particular AI tool. Some tools may use log-in data, tracking, and other analytics (Center for New Designs in Learning and Scholarship, 2023). This same advice applies to you, as a faculty member. Should you want to input your existing writing prompts into an AI tool to see the output, consider your own data privacy. Instructors who may want to make use of ChatGPT or other technology tools in their classes should note that, as with any online service that is not contracted by the university, in view of FERPA regulations, students must not be required to identify themselves to third parties.
Write a syllabus statement that clarifies the expectations of AI use. The Faculty Senate Rules Committee has made a determination that, absent any guidance from the instructor, the use of ChatCPT and similar AI text generators is prohibited according to the Academic Honesty Policy. Instructors do have the discretion to allow for the use of such tools, however, and must do so explicitly if they want to allow it. Stating what you expect students to use (or not use) in their work helps to answer any questions around what extra support is permittable. Depending on your pedagogical values and course expectations, consider adopting or revising one of the statements below for your syllabus. (All statements adapted from Artificial Intelligence Tools and Teaching by Iowa University’s Office of Teaching, Learning, & Technology.) 
AI is prohibited: [This course] assumes that all work submitted by students will be generated by the students themselves, working individually or in groups. Students should not have another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write assignments and using artificial intelligence tools like ChatGPT.  

AI is allowed with attribution: Use of AI tools, including ChatGPT, is permitted in this course for students who wish to use them. To adhere to our scholarly values, students must cite any AI-generated material that informed their work (this includes in-text citations and/or use of quotations, and in your reference list). Using an AI tool to generate content without proper attribution qualifies as academic dishonesty.  

AI is encouraged with certain tasks and with attribution: You can choose to use AI tools to help brainstorm assignments or projects or to revise existing work you have written. When you submit your assignment, I expect you to clearly attribute what text was generated by the AI tool (e.g., AI-generated text appears in a different colored font, quoted directly in the text, or use an in-text parenthetical citation). 

Communicate your perspective about AI use. In addition to syllabus statements, consider talking with your students about AI tools like ChatGPT.  
Different levels of familiarity: As an emerging technology, students will have differing levels of familiarity with these tools. For instance, while ChatGPT can write a grammatically correct paper or appear to solve a math problem, it may be unreliable and limited in scope. Discuss with students the uses and limitations of AI tools more broadly in addition to your perspective on their use in your class. 

Connect to critical thinking skills: AI tools have many implications beyond the classroom. Consider talking with students about how to be engaged consumers of AI content (e.g., how to identify trusted sources, reading critically, privacy concerns).  

Adapt assessments. AI tools are emerging and it can be incredibly difficult to make any assessment completely free from AI interference. Beyond a syllabus statement, you may also consider adapting your assessments to help reduce the usefulness of AI products. However before revising any assignment, it’s helpful to reflect on what exactly you want students to get out of the experience and share your expectations with your students. Is it just the end product, or does the process of creating the product play a significant role? 
Create assessments that allow students to develop ideas over time. Depending on your class size, consider scaffolding assessments to be completed in small components (e.g., proposal, annotated bibliography, outline, first draft, revised drafts). 

Ask students to connect their writing to specific course materials or current events. Students can draw from the course textbook, additional readings on the LMS, and even class discussion boards or in-class discussions.  

Incorporate personal experiences and reflections. Provide students with opportunities to connect what they are learning to their own lives and experiences—stories unique to each individual. 

Incorporate Multimedia Assessments. Consider developing or adapting assessments to include multimedia submissions (e.g., audio or video components). VoiceThread is a UMass Amherst supported tool that allows students to leave audio, visual, and video content.  Also, consider UMass-supported social annotation tools like Perusall or Google Docs for students to use when responding to assigned readings or other materials.  

Use class time. Ask students to complete writing assignments during class time (e.g. complete reading reflections at the beginning of class, or use exit tickets). Asking students to organize their ideas by writing during class may also support student engagement in other class activities such as discussions and group work.  

For questions on your LMS, Google, and other educational technology contact IDEAS at instruct@umass.edu",,Center for Teaching & Learning
Vedant,67,University of Pittsburgh,Q235034,"28,391","19,200","9,191",R1,"40.444565,-79.953274",Pittsburgh,PA,School of Computing and Information,https://www.pitt.edu,,"AI Academic Integrity Policy Suggestions
University of Pittsburgh Writing Institute Workshop on AI and the
Teaching of Writing*
Background to the generative AI suggested policy (for teachers):
As you may be aware, generative AI tools such as ChatGPT have made it possible to automate some
aspects of the writing process, including: brainstorming; analysis of texts; developing arguments or
counterarguments; improving organization; adjusting style and grammar to fit stylistic conventions; and
even wholesale generation of essays. What ChatGPT and other large language models (LLMs) write often
contains falsehoods, made-up quotes, and predictable writing. But their output is also often polishedsounding writing. As generative AI platforms evolve, they will get better at accurate quoting, writing
more interesting prose, and higher level research activities.
Students in our composition courses are aware of ChatGPT and other free, accessible AI writing tools
and some are already using them in our courses. Pitt's Academic Integrity Code prohibits
unacknowledged use of the writing of others, which addresses issues of plagiarism and contract
cheating. We believe that the code already implicitly covers the potential use of generative AI, although
the policy may be updated soon to explicitly address AI use.
The Composition Program encourages instructors to exercise their own judgment about student use of
these technologies within their courses. Composition courses can employ generative AI productively
and, if you would like to integrate such technologies in your course, we encourage you to seek out ways
to do so while adhering to composition course goals. We suggest that you make your policies clear by
including one of the following policies on your syllabus and Canvas site. Instructors should also discuss
their policies with students, make clear which uses of AI are allowed and which are not, and why.
If you have questions about the use of AI in your course, we encourage you to reach out to our
Academic Integrity Dean and Teaching Professor of English, Jeff Aziz.
Suggested syllabus statement if you want to prohibit the use of generative AI in your course:
The use of generative AI writing tools (such as ChatGPT, GrammarlyGO, GPT-3, GPT-4, BERT, or
others) is prohibited in this class. Assignments for the course have been designed to help you
develop as a writer without the use of these technologies. You will generate ideas, read, revise,
and write on your own and/or in consultation with peers, me, or Writing Center tutors and you
will not use AI at any stage of your writing process. You are the author of your work for the
course and authorship means you take responsibility for your words and claims. Any use of AI
technologies in your work will be considered an academic integrity violation and addressed
accordingly: https://www.as.pitt.edu/faculty/policies-and-procedures/academic-integrity-code .
Please see me if you have any questions about this policy.
Suggested syllabus statement if you want to allow limited uses of generative AI in your course:
The use of generative AI writing tools (such as ChatGPT, GrammarlyGO, GPT-3, GPT-4, BERT, or
others) is allowed in this class within specific contexts and only if such use is properly
acknowledged. Assignments for the course have been designed to help you develop as a writer,
and some of them may call on you to practice writing with the help of such tools. As your
instructor, I will assume that any use of these tools will be only within the contexts the
assignment allows (for instance, you can use ChatGPT for brainstorming if the assignment asks
you to do so). You must acknowledge the use of AI in your assignment in an ""Acknowledgement
of AI Use"" statement that:
· Specifies which technology was used and on what date (ChatGPT, GPT-3, etc)
· Includes explicit descriptions of how the information was generated
· Identifies the prompts used
· Explains how the output was used in your work
Examples of such Acknowledgments of AI Use can be found on Monash University's website.
The use of AI outside of contexts where the instructor specifies its use, or failure to
acknowledge any use of AI technologies in your work will be considered an academic integrity
violation and addressed according to Pitt’s Academic Integrity policies:
https://www.as.pitt.edu/faculty/policies-and-procedures/academic-integrity-code . You are the
author of your work for the course and authorship means you take responsibility for your words
and claims, regardless of which tools you use. Please see me if you have any questions about
this policy",,
Vedant,67,Villanova University,Q1466001,"9,800 (spring 2023)","6,791","3,108",R3,"40.03771,-75.33755",Villanova,PA,,http://www1.villanova.edu/content/main.html,,"Guidance on AI Text Generators
New open-access AI technologies are emerging (such as ChatGPT) that are able to generate written text based on user prompts.  Several at Villanova have put this new software through its paces, and we believe that most of the time, it does not generate college-level text suitable for paper submission.

At this time, AI-generated responses to broad essay-style questions lack detail and analytical depth. However, the technology will generate a starting point that a student could potentially edit and expand upon to generate an essay that will meet college standards. Furthermore, it can generate short answers that could be suitable for blogs, discussion boards, and more. 

We have put together some recommendations that faculty may consider implementing in the coming semester and in the future as AI tools become readily available and more sophisticated. 

Grading and Discussing with Students
Department/Program Actions
Best Practices/Help for Beating AI
Class Activities/Assignments

July 2023 Updates
Dear Faculty Colleagues,

We would like to update you with some information around A.I. generators such as ChatGPT in preparation for the fall semester.  A.I.’s capacity to generate text, computer code, graphics, music, and more is continuing to improve, and its use is becoming more commonplace. Most of our students are aware of these tools and have used them. While some time savings and useful information can be generated by A.I., substituting A.I. use for human cerebration can be detrimental to student learning in many situations, while in others it can be extremely helpful. It will continue to be up to the individual faculty member to allow or prohibit its use in their courses. We ask that you:

Become familiar with A.I. generators through readings and personal use.  Review our previous advice (see above) and VITAL’s newsletter on how to generate assignments in the age of A.I. Continue to have discussions in faculty meetings about this issue.  As this technology is here to stay and is steadily evolving, we need to continue to familiarize ourselves with its benefits and drawbacks and consider it within our disciplinary instructional contexts.  
Design instructional activities and assignments to directly support your courses’ learning goals. Should you find that A.I. use supports and/or enhances the students’ achievement of these goals, then consider its educational use. If not, you may consider prohibiting its use for that activity, assignment, or the entire course. Some forms of higher-order thinking, such as analytical reading, critical thinking, problem solving, evaluation, metacognition, and synthesis of knowledge, might not be mastered with the use of A.I.
Discuss with your classes the proper use (or prohibition) of A.I. and explain the reasoning behind your instructional decisions given the courses’ learning goals. Focus on the skills and cognitive capabilities your course intends to foster in your students and how A.I. can either support or inhibit building students’ competencies. Students must be able to utilize modern tools, develop abilities that exceed those of machines, evaluate the reliability of all information including that is generated by A.I., and meet the higher-order demands of engaged citizenship in an evever-changingcomplex world.
Set clear expectations and guidelines around appropriate A.I. use in your courses, and consider including one of the following statements for your syllabus/assignment that we have developed; you may adjust to reflect the nature of your course:
The use of A.I.-generated content is not permitted in this course/on this assignment. Its use will result in an academic integrity violation and a zero on the assignment.  
The use of A.I.-generated content is allowed in this course/on this assignment. Even if you have significantly edited A.I.-generated material, you must identify the A.I. tool used to assist in generating your work. You are required to provide the name of the tool, date used, and prompts used to generate the output. As you may be required to submit the original A.I. output you must keep a copy of the original output and provide it when requested.  If questions arise about the authorship of submitted work, you are responsible for authenticating your authorship. You should save evidence of your authorial process, such as drafts, notes, version histories, and complete transcripts of A.I. assistance.
The use of A.I.-generated content is permitted as follows (a) for generating a first draft or (b) for generating an outline or (c) for generating XXX.  Even if you have significantly edited A.I.-generated material, you must identify the A.I. tool used to assist in generating your work. You are required to provide the name of the tool, date used, and prompts used to generate the output. As you may be required to submit the original A.I. output, you must keep a copy of the original output and provide it when requested.  If questions arise about the authorship of submitted work, you are responsible for authenticating your authorship. You should save evidence of your authorial process, such as drafts, notes, version histories, and complete transcripts of A.I. assistance.
Understand it is challenging to detect A.I.-generated material.  A.I. detectors are not fully reliable, and there are apps available whose specific intent is to render A.I.-generated content undetectable.  
When filing an academic integrity violation for a student’s work, point to specific parts of the Code of Academic Integrity and, when possible, focus on aspects of the student’s work that are indisputably problematic. The Academic Policy Committee is in the process of updating the Code to address A.I. more explicitly with an expected announcement early in the fall semester. Some relevant sections of the Code (which you may decide to include in your syllabus) are:
""When completing an individual class assessment (i.e. assignment, quiz, lab report, exam, etc.) students shall rely on their own mastery of the subject and not attempt to receive help in any way not explicitly approved by the instructor.”
“Plagiarism is defined as the appropriation of another's work and the unacknowledged submission or incorporation of that work as one's own offered for credit.”  This statement applies to A.I.-generated material even if edited.
“Students shall not falsify, invent, or use in a deliberately misleading way any information, data, or citations in any assignment.”  Often A.I. will generate false citations or actual citations that do not correspond with the material.
Consider providing the attached “Guidelines for Students Regarding Responsible Academic Use of A.I.-Generated Material” to the students in your course and/or discuss topics contained within it.
 ",,
Vedant,73,Binghamton University (SUNY),Q863813,"18,148 (Spring 2022)","14,333 (2022)","3,815 (2022)",R1,"42.08925,-75.96989",Binghamton,NY,,https://www.binghamton.edu/,,"Suggested Syllabus Language for Generative AI
August 2023
Special thanks to the University of Toronto for allowing us to use much of their content.
We recognize that some instructors may want to allow, or even encourage, their students to use
generative AI technologies, and others may want to prohibit their use. The following suggested
statements are intended to help you shape the message you provide to your students on a course
syllabus and/or on assignment instructions to reinforce a shared understanding of what is, and is not,
allowed. This is an important conversation to have with your students so that everyone is clear about
their use in your class. If you are allowing or disallowing AI tools in your course, clarify for the students
why you made this decision and how AI assistance supports or, alternatively, negatively impacts the
pedagogical goals of the course or assignment.
Can use Generative AI tools
In indicating on a syllabus and/or assignment instructions that students may use generative AI, the
instructor should decide to what degree and on which assignments the students may use these tools.
This is similar to indicating to students when they may collaborate, and to what degree, with their
classmates, and when an assignment should be solely their own work.
Here are some suggested example statements that might be used, combined, or adapted for your course
or assignments:
• Students are encouraged to make use of technology, including generative AI tools, to contribute
to their understanding of course materials.
• Students may use artificial intelligence tools, including generative AI, in this course as learning
aids or to help produce assignments. However, students are ultimately accountable for the work
they submit.
• Students must submit, as an appendix with their assignments, any content produced by an
artificial intelligence tool, and the prompt used to generate the content.
• Any content produced by an artificial intelligence tool must be cited appropriately. Many
organizations that publish standard citation formats now provide information on citing
generative AI (e.g., MLA: https://style.mla.org/citing-generative-ai/ ).
• Students may choose to use generative AI tools as they work through the assignments in this
course; this use must be documented in an appendix for each assignment. The documentation
should include what tool(s) were used, how they were used, and how the results from the AI
were incorporated into the submitted work.
Note: Some generative AI applications may require a subscription fee. Please consider offering students
a choice to opt-out of using a system if they have concerns about the cost, privacy, security or other
issues related to the technology.
If you are indicating that students can using generative AI tools, it is important to include in your syllabus
statement two reminders to students about these tools:
? AI can lead to incorrect information and non-existent sources, so it is important that you verify
any information you use from an AI tool is correct and cite it.
? Do not input any personal information into AI tools.
Can use Generative AI in certain instances or specific ways
It is important to be very specific about the boundaries and limitations of artificial intelligence use in
completing course work, if there are boundaries you want to set. Please consider the difficulty for
students, who are trying to navigate AI rules in multiple courses before setting up elaborate limitations in
your course. However, there are reasons why you may want, or need, students to engage with generative
AI tools in a specific way or on a specific assignment.
Here are some suggested example statements that might be used, combined, or adapted for your course
or assignments:
• Students may use artificial intelligence tools for creating an outline for an assignment, but the
final submitted assignment must be original work produced by the individual student alone.
• Students may not use artificial intelligence tools for taking tests, writing research papers,
creating computer code, or completing major course assignments. However, these tools may be
useful when gathering information from across sources and assimilating it for understanding.
• Students may not use artificial intelligence tools for taking tests in this course, but students may
use generative AI tools for other assignments.
• Students may use the following, and only these, generative artificial intelligence tools in
completing their assignments for this course: …. No other generative AI technologies are
allowed to be used for assessments in this course. If you have any question about the use of AI
applications for course work, please speak with the instructor.
If you are indicating that students can using generative AI tools, it is important to include in your syllabus
statement two reminders to students about these tools:
? AI can lead to incorrect information and non-existent sources, so it is important that you verify
any information you use from an AI tool is correct and cite it.
? Do not input any personal information into AI tools.
Cannot use Generative AI
In indicating on a syllabus that students may not use generative artificial intelligence, the instructor
should decide to what degree and on which assignments the students may not use these tools. This is
similar to indicating to students when they may, or may not, collaborate with classmates and to what
degree. Note that as AI tools become incorporated into commonly used systems (e.g., Google docs), it
will become increasingly important to be clear about what functionalities are allowed or disallowed in
your course.
Here are some suggested example statements that might be used, combined, or adapted for your course
or assignments:
• The use of generative AI tools or apps for assignments in this course, including tools like ChatGPT
and other AI writing or coding assistants, is prohibited.
• The knowing use of generative AI tools, including ChatGPT and other AI writing and coding
assistants, for the completion of, or to support the completion of, an examination, term test,
assignment, or any other form of academic assessment, may be considered an academic offense
in this course.
• Representing as one’s own an idea, or expression of an idea, that was AI-generated may be
considered an academic offense in this course.
• Students may not copy or paraphrase from any generative artificial intelligence applications,
including ChatGPT and other AI writing and coding assistants, for the purpose of completing
assignments in this course.
• The use of generative artificial intelligence tools and apps is strictly prohibited in all course
assignments unless explicitly stated otherwise by the instructor in this course. This includes
ChatGPT and other AI writing and coding assistants. Use of generative AI in this course may be
considered use of an unauthorized aid, which is a form of cheating.
• This course policy is designed to promote your learning and intellectual development and to help
you reach course learning outcomes.",,
Vedant,73,"Indiana University, Bloomington",Q1079140,"45,328 (2021)","34,253 (2021)","11,075 (2021)",R1,"39.167222,-86.521389",Bloomington,IN,School of Informatics and Computing,https://www.indiana.edu/,,"Precautions
Generative AI has potential applications across a wide range of industries, including art, writing, and software development. However, there are also concerns about the potential misuse of these tools and any data shared with the services. When you provide information to these tools, such as queries, student essays, grant proposals, source code, or datasets, it is the same as posting the information on a public website.

Indiana University encourages its affiliates to experiment with using these generative AI services, as long as no institutional data is submitted to them without approval.

At IU, Microsoft Bing Chat Enterprise is available for use by IU faculty and staff, and it is the recommended way to use generative AI within the IU environment. As part of the university's enterprise agreement with Microsoft, Bing Chat Enterprise is approved to interact with data classified up to and including University-Internal data . To use Bing Chat Enterprise, you must be logged in with your Microsoft 365 at IU  account (your @iu.edu email address and your IU passphrase). For information about browser and app compatibility, see About Bing Chat Enterprise at IU.

To date, no other generative AI tools have been approved for data beyond Public classification, and these have not been through the Software and Services Selection Process (SSSP). Prior to the sharing of any institutional data, these services will need to go through review to ensure the necessary contracts and safeguards are in place to protect the data submitted and to ensure the algorithms in use are ethical, transparent, and beneficial to the IU community.

Types of institutional data that should not be submitted to public versions of generative AI tools, even when anonymized, include:

Data classified as University-Internal or higher (for examples, visit the Data Classification Matrix )
Any data that may be considered student, faculty, or staff intellectual property, unless the individual submitting that intellectual property created it
Specific examples that are not appropriate for the public versions of generative AI tools include:

Sharing names and information about a real student, employee, research participant, or patient
Asking an AI service to summarize and grade a student paper or assignment
Sharing employee-related data such as performance or benefit information for communication drafting or analysis
Asking an AI service to generate code for IU systems protecting institutional data or sharing IU source code for editing
Sharing grant proposals still under review
Acceptable uses
With these precautions in mind, there are numerous ways to use generative AI tools without submitting university data or intellectual property. Using general queries to generate content to pull information from the AI resources is a good way to engage with the products.

Students should use generative AI in ways that align with university academic integrity policies and communicate with their instructors before using generative AI in their coursework. Schools and departments may elect to further restrict generative AI.

From a data management perspective, examples of acceptable uses of generative AI include:

Syllabus and lesson planning: Instructors can use generative AI to help outline course syllabi and lesson plans, getting suggestions for learning objectives, teaching strategies, and assessment methods. Course materials that the instructor has authored (such as course notes) may be submitted by the instructor.
Correspondence when no student or employee information is provided: Students, faculty, or staff may use fake information (such as an invented name for the recipient of an email message) to generate drafts of correspondence using AI tools, as long as they are using general queries and do not include institutional data.
Professional development and training presentations: Faculty and staff can use AI to draft materials for potential professional development opportunities, including workshops, conferences, and online courses related to their field.
Event planning: AI can assist in drafting event plans, including suggesting themes, activities, timelines, and checklists.
Reviewing publicly accessible content: AI can help you draft a review, analyze publicly accessible content (for example, proposals, papers and articles) to aid in drafting summaries, or pull together ideas.
Even if you use generative AI tools for activities that do not share personal or institutional data, you should still check the tool's output for accuracy. Since these tools have been known to produce inaccurate content (sometimes called ""hallucinations""), verify any factual information generated by an AI tool, and make sure to reference the tool as you would any other source.",,
Vedant,73,Tulane University,Q1193547,"14,062","8,610","5,452",R1,"29.935344,-90.122687",New Orleans,LA,,http://tulane.edu,,"Guiding Principles 
Learning-Centered: Like any tool students might use to engage in the work of a course—from library books to research databases to internet search engines—GenAI systems present opportunities for students to learn important skills, including creativity, critical thinking, ethical decision-making, and discerning use of resources, among others. We encourage instructors to talk explicitly with students about the pluses and minuses of GenAI systems as they help or hinder learning in a course.  In this way, regardless of one's view of GenAI or concern with its implications, the emphasis is on learning and what might assist or inhibit the learning process.

For example, if a course learning objective is to help students develop their own voices and perspectives through reflective writing, then use of GenAI systems to produce text that ostensibly represents ""personal voice"" or ""perspective"" might undermine that objective. Alternatively, if a learning objective is to help students structure and refine the presentation of arguments in writing, a GenAI system might play a supportive role in the process of students' learning how to organize an argument in written form (which is not the same as a student trying to use a GenAI system to formulate the substantive claim—with accompanying reasoning and evidence—that is being structured in written form). 

Transparent: Faculty and GE instructors have flexibility in how they approach newly available GenAI tools, and this means students will shift among a variety of approaches in their different courses.  It is therefore essential for instructors to prioritize transparency so that students know clearly the GenAI policy in each individual class and the expectations for specific course activities and assignments, especially if the parameters for GenAI use vary among the latter.  In addition to including a GenAI course policy on the syllabus and explicit details about GenAI use in assignment instructions, we encourage faculty and GE instructors to talk explicitly with students about the rationale for their policy and expectations, including the relationship to students’ learning, as suggested above.

Course Policies
We strongly encourage instructors to have an explicit policy about GenAI in their course syllabus, including any relevant distinctions between GenAI use (as process) and GenAI content (as product). We also encourage instructors to reinforce their expectations in assignment instructions and in conversation with students. Based on helpful considerations articulated by Josef Brandauer and Melissa Forbes at Gettysburg College, the sample course policies below indicate a range of options that instructors can adapt depending on their specific course context and student learning goals.

Sample Course Policies
We also encourage exploration of the Syllabus Resources from the Sentient Syllabus Project, which offers a variety of considerations and example policies that can be used or adapted. Instructors can also find dozens of sample course policies at this crowdsourced document, including an option to search by discipline and course topic.
Course Assignments and Class Activities
Instructors are experimenting with a variety of ways to include use of GenAI systems in their courses, including both classroom activities and assignments. Below are a few ideas to consider, including examples from UO instructors, and we also list a few ideas for those wanting to mitigate use of GenAI.",,
Vedant,76,Colorado School of Mines,Q1111367,"7,101 (fall 2023)","5,443 (fall 2023)","1,658 (fall 2023)",R1,"39.751111111111,-105.2225",Golden,CO,,https://www.mines.edu,,"TEACHING RESPONSIBLE USE OF GENAI
AS GENAI TOOLS DEVELOP, IT WILL BE IMPORTANT FOR STUDENTS AND FACULTY TO CONTINUE TO BUILD THEIR RELATIONSHIPS WITH EACH OTHER AND WITH TECHNOLOGY. FACULTY ARE ENCOURAGED TO USE A PROACTIVE, HOLISTIC APPROACH IN USING GENAI THAT DISINCENTIVIZES CHEATING AND UNETHICAL USE OF GENAI BY EMPLOYING THE “TEACH” ACRONYM: BUILDING TRUST AND COMMUNITY WITH STUDENTS, INTERROGATING THE ETHICS OF GENAI, ENSURING EQUITABLE ACCESS TO GENAI, EXPLICITLY COMMUNICATING EXPECTATIONS AND RELEVANCE OF COURSE TASKS, AND (RE)DESIGNING ASSIGNMENTS TO CENTER THE HUMAN IN LEARNING BY TAPPING INTO STUDENT MOTIVATION, ITERATION, AGENCY, LIVED EXPERIENCE, AND CREATIVITY. 
TRUST
Students and faculty are encouraged to work together to determine how genAI will be used transparently in their coursework, and should learn from each other on how both students and instructors are using genAI in and out of the classroom.

ETHICS
GenAI tools are being developed at an incredible pace. Though promising, genAI tools may be inaccurate, inefficient, or biased. GenAI outputs may come from questionable sources: a network of unknown and known genAI tools, sources that limit or remove human credit, sources that limit or remove pay to humans, etc. It is critical to interrogate both genAI inputs and outputs so as to comply with ethical and legal standards.

ACCESS
Mines is committed to supporting an accessible digital environment for all members of our community. Faculty should proactively investigate how their incorporation of genAI tools might unintentionally build barriers to student learning. When bringing genAI tools into the classroom, faculty should provide equitable access and ensure that they are offering accessible avenues for diverse learners. Financial considerations should be made as not all genAI models are free, and the most updated versions of genAI tools usually come at a cost. Additionally, instructors should build a shared literacy around selected genAI tools by assessing and/or teaching students how to use a tool before significant use is expected. 

CONTACT IT TO REVIEW PROSPECTIVE AI TOOLS
DISABILITY SUPPORT SERVICES
COMMUNICATION
Faculty are expected to state explicitly and affirmatively their expectations regarding student use of genAI tools. Instructors should specify in writing the permitted and prohibited uses of genAI tools in their courses, and should seek to clarify any expectations if they differ from one assessment to another. For additional support, see this faculty guide for clarifying resource use on individual assignments. 

HUMAN
All Mines community members play a role in adding heart to genAI use. Humans will always have unique lived experience and agency to make their own decisions on how to use, implement, and apply genAI tools in the classroom. It is recommended to center student iteration and growth, and to help students develop critical and metacognitive thinking skills to identify where genAI complements, supplants, or fails to replace human contributions. Faculty and students are expected to be creative in their ethical use of genAI tools in ways that center human experience and evaluation.

ADDITIONAL STRATEGIES FOR TEACHING WITH GENAI, PLEASE VISIT THIS RESOURCE ON EFFECTIVE TEACHING & GENERATIVE AI PROVIDED BY THE TREFNY CENTER.
Guidance is provided below for faculty to consider adding a statement to their syllabus to aid in the communication about genAI tools in their courses. These guidelines have been adapted from George Washington University by a group of Mines faculty, students, and staff. There are three suggested paths for faculty to consider.

Instructors might 1) generally permit the use of genAI tools; 2) generally forbid their use; or 3) permit their use for certain purposes on certain assignments, but not others. If an instructor wishes to permit certain uses of genAI tools, such uses must be set forth explicitly in the course syllabus and/or assignment instructions. Below is some model language for the three permission options.

SAMPLE SYLLABUS LANGUAGE
GENERAL PERMISSION
GENERAL PROHIBITION
SELECTIVE PERMISSION
DOWNLOAD THE OFFICIAL MINES SYLLABUS TEMPLATE
FACULTY GUIDE FOR CLARIFYING GENAI USE
GUIDE FOR CITATION (ARTHUR LAKES LIBRARY)
DEFAULT GUIDELINES
In the absence of explicit directions in writing to the contrary from instructors (instructors: see above for several examples of alternative course policies), the following provisions provide default guidance for the use of genAI under the Academic Integrity/Misconduct Policy.

Students must represent any work (e.g., ideas, text, code, images) submitted for evaluation as the student’s own intellectual product, created in accordance with established course and university policies. Unless instructors provide explicit permission or instruction to the contrary, students may not submit content for evaluation that was generated, in whole or in part, by genAI tools. Using genAI tools when unauthorized would be considered a potential violation of Mines’ policy on academic integrity.
Illustrative, but not exhaustive, examples of prohibited conduct include:

A student types a prompt into a genAI tool and copies all or part of the generated content (e.g., text, code, images, solutions) into their answer on an assessment, essay, test, or other assignment submitted for evaluation (whether in-class or out-of-class).
A student provides a genAI tool with all or part of the prompt for an out-of-class assignment (e.g., essay or coding assignment) and paraphrases or otherwise adapts all or part of the generated content into their response without proper attribution of the genAI tool.
2. Students are permitted and encouraged to explore the use of genAI tools to generate (and critically examine) content that is not submitted to an instructor for evaluation. Unless instructors provide explicit direction to the contrary, students may use genAI tools for learning, studying, proofreading, and brainstorming.

Illustrative, but not exhaustive, examples of permitted conduct include:

A student uses genAI tools to study for exams, quizzes, and other assessments by providing prompts to the genAI and reviewing the output.
A student uses genAI tools to explore the capabilities and potential uses of genAI in an area of their interest.
A student copies the text of a writing assignment into a genAI to ask for proofreading, while making sure to adhere to established university policies.
ACADEMIC INTEGRITY CONCERNS
Mines community members have a responsibility for establishing, maintaining, and fostering an understanding and appreciation for academic integrity. For all their promise, genAI tools misused could interfere with learning outcomes and impair the development of students’ skills in and outside of the classroom, as well as harm the substance of degrees awarded at Mines.

An instructor who suspects an academic integrity violation based on genAI tools should follow Mines Academic Integrity/Misconduct Policy, following guidance from the Office of Community Standards. This website is designed to provide both students and faculty policy and procedure access to best understand rights and responsibilities of all involved parties when academic misconduct is suspected. The Office of Community Standards staff can also be points of contact for questions regarding policy implementation as it relates to genAI tools.

At times, faculty may be interested in using AI-detection resources provided by Mines (e.g. Turnitin) to assess academic integrity in student work. These tools may not be available for faculty to rely on institutionally, and should not be the only tool used to make decisions on academic integrity. The Office of Community Standards staff can provide additional considerations to support faculty in making decisions which honor the rights and responsibilities of faculty and students in the academic integrity/misconduct process.

Each academic department is encouraged to prepare for the increased use of genAI tools within their departments in some of the following ways: to discuss academic integrity expectations within each department, to (re)design assignments to focus on process over output, to create clear expectations of genAI use on specific course tasks, and to develop ways to build trust between students and faculty. All of these examples are proactive ways to reduce academic misconduct in coursework. The Office of Community Standards and the Trefny Center have worked together to provide a resource to help instructors proactively disincentivize academic misconduct.

DESIGNING ASSIGNMENTS
The continued development of ChatGPT and genAI offers us an opportunity to revisit course assignments and assessments to ensure that they spark curiosity, foster intrinsic motivation, and promote high-level critical and ethical thinking as described in the TEACHing with genAI section above. We encourage instructors to design assignments that center the learning process as much as (if not more than) output through evaluative critique, self-reflection, and opportunities for revision. Here are a few ideas for exercises and prompts that explicitly encourage students to critically engage with genAI: 

Ask students to describe how they utilized genAI for any given assignment and reflect on their process—what they learned, the challenges and frustrations they faced, what they did to overcome those challenges, their takeaways from the experience, etc.
Ask students to practice formulating effective prompts.
Ask students to critically evaluate AI-generated outputs with respect to accuracy, persuasiveness, bias, equity, quality, and so forth.
Ask students to investigate, fact-check, criticize, or edit AI-generated content.
Ask students to practice metacognition and reflect on where genAI complements, supplants, or fails to replace their own contributions. This can be done by asking students to compare their own work with AI-generated work and to identify their own strengths, tendencies, and perspectives. 
In addition to designing assignments that ask students to critically engage with genAI, there are a number of design strategies instructors can use to disincentivize cheating, center the human in assignments, build trust with students, and foster intrinsic motivation:

Set clear expectations (in writing) and communicate the relevance of any given assignment or activity.
Offer opportunities for low-stakes, collaborative practice. Give students a chance to identify mistakes, make corrections and revisions, discuss strategies with their peers, and re-present their thinking in low-stakes contexts. 
Incorporate unpenalized opportunities for students to revise and resubmit their work. Consider grading policies that reward student perseverance and growth.
Encourage multimodal communication and creativity (voice memos, podcasts, videos, brochures, infographics, websites, classroom presentations, etc).
Ask students to connect course content, classroom conversations, and their own lived experience. Not only will this create tasks that are more difficult for genAI, but it can help foster intrinsic motivation through personal connections with course content.
Ask students to set personal learning goals and reflect on their progress throughout the semester. Cultivating a “growth mindset” will help students to see the value of their learning process and take ownership over their work.",,
Vedant,76,Pepperdine University,Q117876,"10,030 (Fall 2022)","3,662 (Fall 2022)","6,368 (Fall 2022)",R3,"34.038683333333,-118.70758055556",Malibu,CA,,https://www.pepperdine.edu/,,"Recommendations entering this new space
Establish in the beginning of the class on whether or not AI is allowed. If it is permitted, clearly state guidelines and policies. We highly encourage all faculty to state the use of AI in the classroom within their syllabus. 
Monitor the use of AI regardless if it is permitted or not in the course. Use the softwares available by the University to help oversee this area, but once again, using a monitoring software system does not guarantee the work is original.
Have an open mind about AI on how it can be an asset in the classroom. AI is capable of many features, such as text, video, image, and voice generation, which can add value to your class learning, assignments, and presentations. AI is already embedded within the marketplace and business world and will continue to grow in capability. For example, newer models (like Claude) are already sufficiently good at ""reading"" long (even book-length) documents and summarizing them effectively. ChatGPT's code interpreter is also reducing the barriers of entry into data analysis and visualization. AI does not appear to be going away and we need to equip ourselves going forward. Helpful tip, when using prompts, be clear, using sequential prompts and giving the model ""time to think"" by explaining the logical steps it follows. Effective prompting is an effective skill for both educators and students.
Continue to incorporate critical thinking within the course. As stated by ChatGPT above, ""human interaction, critical thinking, and hands-on experiences are essential components of higher education."" Here at Pepperdine, we value transformational learning and need to continue focusing on critical thinking and engagement from students.",,
Vedant,76,University at Buffalo (SUNY),Q681025,"32,347 (Fall 2020)","22,306 (Fall 2020)","10,041 (Fall 2020)",R1,"43,-78.78916666666667",Buffalo,NY,,https://www.buffalo.edu/,," Guidance for Artificial Intelligence
UB has no universal policy about student use of artificial intelligence tools (such as ChatGPT). As with any other technology, instructors have the academic freedom to determine what tools students can and cannot use in pursuit of meeting course learning objectives. As you make decisions about if and how to use AI in your classroom, you can find guidance for you and for your students on the Office of Academic Integrity website.

It is highly recommended that you provide clear guidance to your students about the parameters of artificial intelligence use in your course. Some sample language includes:

To prohibit any use of generative AI:

Any use of generative AI (e.g., ChatGPT) is prohibited in this class and will be considered a violation of UB’s academic integrity policy. Details of what resources are allowed will be provided for each assignment. If you are unsure if a resource or tool is allowable, be sure to ask. 

To allow use of generative AI:

This course allows use of generative AI tools (e.g., ChatGPT) on certain assignments within given guidelines. Failure to follow these guidelines may be considered a violation of UB’s academic integrity policy. If you are unsure of how and when generative AI can be used, be sure to ask. ",,
Vedant,76,University of Delaware,Q1068072,"24,120 (Fall 2018)","18,221 (Fall 2018)","4,164 (Fall 2018)",R1,"39.679111111111,-75.752166666667",Newark,DE,,https://www.udel.edu/,,"This is a rapidly-growing, quickly-changing topic; we will continue to add to this webpage as more information becomes available and our experience grows, particularly as more faculty develop and use these tools in assignments. We welcome your input, suggestions, and questions at CTAL-info@UDel.edu

Definition
 Advanced automated tools – artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2 that are sometimes described as “generative” or “autogenerative” tools – use sophisticated technology and very large data sets to create realistic writing, images, or other artifacts in response to natural language queries and prompts. They are very easy to use and some of their output is very difficult to distinguish from human-generated material.

Purpose
This website provides context and practical suggestions for faculty who are addressing the use of advanced automated tools in their course(s). Should students be allowed to use these tools? What are the most pressing issues – practical, pedagogical, and ethical – related to the use of these tools? How can we support their learning about these tools and the many complex, interesting, and rapidly-developing issues that surround them?

Context and impact
These tools are readily accessible to students and have the potential to significantly change some aspects of traditional Western higher education, especially assignments that require students to write or create other artifacts that can easily be created or changed using one of these tools. Although there is research that supports the notion that some automated tools can help students learn and improve skills such as writing (e.g., Wilson, Olinghouse, & Andrada, 2014), the extent of the impact of these newer tools is unknown and subject to debate. However, their accessibility and ease-of-use makes it likely that some students will at least experiment with these tools and be expected to use similar tools after graduation.

Just as faculty have previously adapted their teaching to accommodate the use of other tools and advancements in technology, faculty are encouraged to develop and share with students a coherent approach to the use or non-use of these tools in their courses.

Course policies
In each course, at least four possible approaches seem plausible in terms of student use of these tools:

Prohibit all use of these tools
Allow their use only with prior permission
Allow their use only with explicit acknowledgement
Freely allow their use
Each approach is discussed below with some thoughts and considerations; possible syllabus language for each approach is included in a separate section below. Regardless of the approach selected, faculty should explicitly discuss with students the approach and its underlying rationale.

If use of these tools is prohibited, limited, or must be documented, faculty should also consider if they should include an explicit reminder about plagiarism and whether use or misuse of these tools would be considered plagiarism",,
Vedant,82,"Rutgers University, Newark",Q7382780,"12,321","8,170","4,151",R2,"40.741,-74.174",Newark,NJ,,https://www.newark.rutgers.edu/,,"The International Center of Academic Integrity is dedicated to building and strengthening
cultures of honesty and integrity within academic settings and society at large (ICAI, nd, para 3).
We aim to respond to the trends and issues related to academic integrity standards and practices
(ICAI, nd, para 4). Given this aim, we want to respond to artificial intelligence tools' current and
dramatic rise across the educational sector. Focusing on our organizational aim, we offer the
following statement on artificial intelligence and academic integrity.
Artificial intelligence applications have the potential to support student learning and
development when used ethically and appropriately. However, inappropriate reliance can be a
barrier to student learning and may lead to academic integrity breaches. Institutional leaders,
faculty, staff, and students need to reflect on the differences between appropriate and
inappropriate use of these applications. Institutions and their staff should develop effective
guidance for their leaders, faculty, staff, and students that clearly outline responsibilities for these
applications' ethical, transparent use.
Responsible and ethical use means recognizing that these tools are digital assistants that do not
think but rather use algorithms to predict content. A scholarly skepticism for the use of these
applications is important for the following reasons:
? there are privacy and confidentially concerns about personal user data;
? there is a potential for bias in generated output;
? there is a potential for misinformation, disinformation, and false information related to
the generated output, including falsified references (referred to as hallucinations or
generation of phantom data);
? there are copyright and plagiarism concerns related to the generated output;
? there is a potential risk for lack of diverse voices in the generated output;
? there are access issues related to superior applications behind paywalls;
? there is a potential detriment to the environment due to the energy (and harmful
emissions) required to run these applications;
? there are potential concerns about its ethical use as it remains an unregulated industry.
The careful and thoughtful use of artificial intelligence in learning environments can enhance
and support student learning. As these applications continue to evolve, so should our
conversations with students, academics, and organizations",,
Vedant,82,"University of California, Santa Cruz",Q7895186,"19,161 (fall 2020)","17,207 (fall 2020)","1,954 (fall 2020)",R1,,Santa Cruz,CA,,http://svi.ucsc.edu/,,"Academic Integrity for Graduate Students
Official University Policy on Academic Integrity
for Graduate Students
I. Principles
Academic misconduct includes but is not limited to cheating, plagiarism, fabrication, falsification, research fraud, or facilitating academic dishonesty or as further specified in campus policies and regulations, including the Campus Policy on Research Integrity.
Instructors of record of a course shall make a reasonable effort to explain to students at the outset of the course the behavior expected of them when taking examinations or preparing and submitting other course work. Similarly, faculty or other academic appointees supervising graduate student research shall make a reasonable effort to explain to students at the outset of the research activity the behavior expected of them in properly conducting research. In the sequel, the term ""instructor"" refers to the instructor of record of a course (including courses numbered 297 and 299), to the chair of a qualifying examination committee, and to the chair of a graduate dissertation or thesis committee.
In the event that academic misconduct is suspected, due process shall be respected. The procedures described below shall be invoked to determine the facts of a case and to decide upon disciplinary sanctions where appropriate. All steps need to be carefully documented in writing and should be completed in a timely fashion [1].
All members of the university community who suspect academic dishonesty should report it to the instructor. If the instructor is unavailable, it should be reported to the chair of the department of the student.
The instructor is responsible for determining the academic consequences of academic dishonesty. These may include reduced scores on assignments or non-acceptance of work for satisfaction of course or degree requirements. The latter may entail failure in the course, failure of the qualifying exam, or non-acceptance of the thesis or dissertation.
Disciplinary sanctions are determined by the Dean of Graduate Studies or, if the student's case goes to a hearing, by the Graduate Academic Tribunal. The Dean of Graduate Studies may delegate the determination of disciplinary sanctions to the Associate Dean of Graduate Studies. In the sequel, the term ""Dean of Graduate Studies"" refers to the Dean of Graduate Studies or to the Associate Dean of Graduate Studies, if the case has been delegated to the latter.
The decision of the Graduate Academic Tribunal is final. Appeals to the Chancellor will be considered only for alleged violation of due process.
No grade notation or narrative evaluation will be issued until the process outlined is completed and a final decision is made on the charges and the disciplinary sanctions to be imposed. Likewise, the qualifying examination will not be certified as passed or failed, nor a thesis/dissertation accepted or rejected, until the process outlined is completed and the case has been adjudicated.
In cases in which academic misconduct has been determined to occur, sanctions may include dismissal, suspension for a specified period, and notation of academic misconduct on a student's transcript, including all external copies, for a specified period.
II. Process for disposition of academic dishonesty cases
An instructor or advisor who has evidence of academic misconduct of one of their students has discretion to decide whether that misconduct is sufficiently serious to warrant formal action.

The instructor initiates the process by making a formal request for a meeting with the student to discuss the charges, evidence of misconduct and the academic consequences. If the student refuses to meet with the instructor, that refusal shall be reported to the Dean of Graduate Studies, and shall be taken as prima facie evidence of guilt.

There can be one of three outcomes to the meeting of the instructor and the student:

The instructor and student mutually agree that there is no guilt. No report of the incident will be issued (A.1).
The student admits guilt (B.1). Then the faculty member must make a formal report of the incident by completing a standardized form (available on the UCSC Academic Integrity website and at departmental and Graduate Division offices), which summarizes the charges and the academic consequences (B.2). This form requires the signature of both the instructor and student and should be completed and submitted within 3 working days of the initial meeting between student and instructor. The charges letter must include:

The date(s) of the offense
The nature of the offense
The instructor's analysis
The nature of the physical evidence which supports the analysis, e.g., tests or assignment papers or fabricated data.
Academic consequences determined by the instructor and an explanation of the appropriateness of these consequences. In cases involving a qualifying examination, thesis, or dissertation committee, the charges letter must indicate whether the committee members are in agreement concerning the offense and its consequences, or not. The nature of any disagreements should be described.
The completed form and copies of any physical evidence are sent to the Dean of Graduate Studies who then determines what disciplinary sanctions may be appropriate (B.3). The Dean of Graduate Studies will summarize the charges, the academic consequences, and the disciplinary sanctions to be imposed, and will notify in writing:

The originator of the charges, the student, and the chair(s) of the student's academic department (B.3a.i). Only after receiving the formal report on the final disposition of the case will the instructor submit the final grade and narrative evaluation for the student. Certification of the outcome of a qualifying examination, or of acceptance or rejection of a thesis or dissertation will likewise not occur until the formal report is received (B.3a.ii).
The campus judiciary officer (B.3b), who will notify the registrar and request specific actions as appropriate.
The student does not admit guilt (C.1). The instructor must send a completed form to the Dean of Graduate Studies summarizing the charges and academic consequences, together with a summary of the specifics of the disagreement by the student (C.2). Both instructor and student must sign the completed form. This completed form should be submitted within 3 working days of the initial meeting between the student and the instructor.

Once the form is received, the Dean of Graduate Studies will schedule a meeting with the student to begin the formal hearing process (C.3). At this meeting the Dean of Graduate Studies serves as a procedural advisor to the student, advising the student of his or her options, the consequences of various options, and the procedures that are open to the student.

After meeting with the Dean of Graduate Studies, the student may agree to the charges (C.4a). In this case, the Dean of Graduate Studies will then determine disciplinary sanctions (B.3) and formally report the decision to the appropriate parties (B.3a-b).

If the student does not admit guilt (C.4b), the case is referred to a formal hearing by the Graduate Academic Tribunal (C.5). The Dean of Graduate Studies will schedule this hearing to occur in as timely a manner as possible. Every effort will be made to schedule this hearing within 30 days of the Dean's receipt of the initial charge. In cases involving multiple accused students, it will be up to the discretion of the Graduate Academic Tribunal whether to hear the cases separately or jointly. The Graduate Academic Tribunal will hear the originator and student's case [2], decide upon disciplinary sanctions, and report their decision to the Dean of Graduate Studies, who informs all other parties (B.3a-b) and the originator of the charge. The instructor may then impose the academic consequences originally determined or may modify these consequences based on the findings of the Graduate Academic Tribunal. The Graduate Council will appoint three standing members of the Graduate Council to serve on the ad-hoc Graduate Academic Tribunal; every effort will be made to ensure continuity in the composition of the Tribunal between different academic years. In addition, one of the current GSA reps on the GC will be appointed to the tribunal.

Procedural Appeal Process: The decision of the Graduate Academic Tribunal with respect to verdict and disciplinary sanctions will be final.

Appeals to the Chancellor or to the Chancellor's designee (C.6) will be considered only for alleged use of improper criteria as well as for procedural violations. Appeals shall be limited to the following:

Whether there is substantial evidence to support the finding(s) of violation of university policies or campus regulations for which the discipline was imposed;
Whether there is evidence which could not be adduced at the time of the original hearing and which is likely to change the result;
Whether there was procedural unfairness at the conduct of the hearing;
Whether the disciplinary sanctions imposed were inappropriate given the findings of fact. Any appeal to the Chancellor must be made in writing and received by the Chancellor's office within five business days after the student receives the decision of the Graduate Academic Tribunal. The decision of the Chancellor or Chancellor's designee is final and shall be conveyed to the student and the Dean of Graduate Studies, who will formally inform all other parties involved in the case (B3a-b).
III. Assignment of grade and submission of narrative evaluation, certification of outcome of qualifying examination, thesis, or dissertation
Until a final decision is made regarding the case and disciplinary sanctions to be imposed, the instructor will not assign the grade (or when the option becomes available from the registrar, they will assign the notation DG for deferred grade). Assignment of a final grade and submission of a narrative evaluation shall not occur until the case is finalized and the instructor has received a written summary of the case's outcome from the Dean of Graduate Studies. The same sequence of events shall govern certification of the outcome of a qualifying examination and acceptance or rejection of a thesis or dissertation. The student may appeal the academic consequences imposed by the instructor through the process outlined in the Graduate Student Handbook.

IV. Annual Report and Procedural Instruction
The Dean of Graduate Studies shall compile an annual public summary (omitting names and other material that would identify the concerned parties) of the disposition of cases that have been reported to that office. This report shall contain the Dean of Graduate Studies' summary and assessment of the effectiveness of the procedure. In particular, cases of multiple offenses shall be noted. The report will be sent to the Graduate Council, with copies to the Chancellor, the EVC, the Judicial Affairs Director, and all Academic Deans.

The Division of Graduate Studies is responsible for providing web-based and paper copy information to the campus community on all procedures regarding academic integrity for graduate students.

V. Sunset Provision
These procedures shall be assessed by the Graduate Council after being in effect for five years and a report of the assessment shall be made to the Academic Senate. At the end of the third year, the Dean of Graduate Studies shall prepare for the Graduate Council an addendum to the annual report with recommendations for any modifications to these procedures.

Footnotes:
All attempts should be made to complete cases within the academic quarter in which the incident occurred or within the quarter or summer immediately following the incident.
The procedures of the Graduate Academic Tribunal will provide for both the instructor and the student to be present and present their cases. Both parties will be allowed to bring appropriate witnesses if agreed in advance by the hearing of the Graduate Student Academic Tribunal. In addition, the Graduate Student Academic Tribunal may seek the testimony of additional appropriate witnesses",,
Vedant,82,"University of Illinois, Chicago",Q955764,"33,747 (fall 2022)","21,807 (fall 2022)","11,940 (fall 2022)",R1,"41.871889,-87.64925",Chicago,IL,,https://www.uic.edu/,,"University of Illinois System Generative AI Principles
Overview
Given the uncertainty surrounding the evolution of generative artificial intelligence (AI) in terms of technological advances, societal acceptance, and regulation, it is important for the University of Illinois System to adopt principles for generative AI governance.

The principles commit the University of Illinois System to the appropriate, responsible, and ethical development, adoption, and use of generative AI in alignment with the System’s guiding principles and mission: to transform lives and serve society by educating, creating knowledge, and putting knowledge to work on a large scale and with excellence.

When applying these principles and evaluating options, always consider the ethical implications of possible outcomes and act with the highest ethical standards. This includes complying with applicable University of Illinois System policies, industry standards, and laws and regulations.

The University of Illinois System principles governing generative AI are:

Accountability
Take responsibility for generative AI outcomes and establish clear lines of accountability within the System to address any issues or unintended consequences that may arise from the use of generative AI. Establish procedures for remediations, recourse, or redress in case of unintended consequences, discrimination, or privacy breaches. Maintain ongoing monitoring and evaluation of generative AI systems' performance, impact, and compliance with ethical standards to help identify and address any emerging issues or areas for improvement.

Inclusiveness
Consider all human races and experiences and use inclusive design practices when developing generative AI systems and applications to identify and address potential barriers and biases that could unintentionally exclude people from generative AI outcomes. Seek input from diverse parties during the development and deployment of generative AI systems. Engage with relevant stakeholders, such as the public, experts, and affected communities, to provide diverse perspectives and help address potential concerns or unintended consequences.

Reliability and Safety
Embed reliability and safety into generative AI development and use to foster trust in generative AI outcomes. Generative AI systems and applications must be resilient to resist intended or unintended manipulation and sufficiently flexible to address new situations safely and reliably.

Fairness
Build fairness into the development and use of generative AI through checks and balances that prevent unlawful discrimination against individuals or groups of individuals. Identify and mitigate biases that could be present in the data used to train generative AI systems.

Transparency
Foster transparency in the development and deployment of generative AI systems. Clearly communicate the capabilities and limitations of the technology to users, stakeholders, and the public, to support generative AI-based decisions and conclusions. Disclose any biases or potential ethical concerns associated with the generated content. Provide transparency in the underlying algorithms and their decision-making processes to enhance trust and accountability.

Privacy and Security
Protect data and ensure generative AI systems and applications incorporate privacy and security by design. Comply with applicable privacy laws and regulations and best practices when using personal data. Implement strong security measures to prevent unauthorized access to generative AI systems and the data they process.",,
Vedant,82,Worcester Polytechnic Institute,Q195046,"7,308","5,246 (2022)","2,062 (2022)",R2,"42.273488888889,-71.80735",Worcester,MA,,http://www.wpi.edu,,"Each year, to support the faculty policy on academic honesty, the Dean of Students Office tabulates data for reported
cases of academic dishonesty. This report includes data on specific academic dishonesty at WPI for the 2022-2023
academic year and general academic dishonesty cases over the past five years.
The Faculty Guide to Academic Integrity at WPI and the Student Guide to Academic Integrity at WPI are available
through these hyperlinks. These brochures are designed to remind faculty, staff, and students about the academic
honesty policy and to explain WPI’s procedure for adjudicating academic dishonesty cases.
As a reminder, the four constructs of Academic Dishonesty referenced in this report include:
PLAGIARISM:
Taking credit for work that is not yours (even if you worked closely with the owner).
Inaccurately or inadequately citing sources.
Paraphrasing (rewording other people’s ideas) without proper citation.
FABRICATION:
Inventing or changing laboratory data and/or research results.
Altering grades or other official records.
Citing a source that was not used.
Changing exam solutions after grading.
CHEATING:
Submitting purchased work or any academic work that isn’t yours, including the use of ChatGPT when not
explicitly permitted.
Using unauthorized materials (cheat sheets, programmed calculators, etc.).
Copying another student’s work.
Unauthorized communication during an exam.
FACILITATION:
Sharing test, homework, or lab information with other students.
Doing work for other students (homework, labs, tests, etc.).
Allowing other students to see or copy your work, even from past assignments.
Assisting in any act of academic dishonesty of another student.",,
Vedant,89,Fordham University,Q130965,"16,986","9,904","7,082",R2,"40.86282744039587,-73.88575234654104",New York,NY,,https://www.fordham.edu/,,"Sample Syllabus Statements to Clearly Communicate the Instructor’s Expectations around Usage of GAI
Faculty need to clearly communicate the degree to which it is unacceptable for students to use GAI tools in their course. The course syllabus is an ideal place to communicate such expectations. Below are example syllabus statements for courses where the instructor wishes to (a) ban the use of GAI in the course, (b) allow students to engage with GAI tools under certain conditions, or (c) signal how students may responsibly use GAI tools freely in the course.

a) For a ""No-AI"" approach:
""Generative AI tools are not permitted in this course. Students [or learners] must rely on their own originality, creativity and critical thinking skills to complete all assignments and engage with course material.""


b) For a ""Limited-AI"" approach:
""Limited usage of generative AI tools may be allowed for specific assignments in this course, enabling exploration of ideas, complex data analysis, and creative solution development, when explicitly permitted by the instructor. When using these tools, it is mandatory to clearly indicate the sections of your work that were generated using them for proper attribution and transparency, and indicate the prompts and software versions that were used. It is critical to adhere to ethical standards by refraining from activities like plagiarism or creating misleading content. Additional guidelines or restrictions will be provided for specific assignments.""


c) For a ""Full-AI"" approach:
""This course allows the use of generative AI tools to facilitate exploration of innovative ideas, complex data analysis, and creative solution development. Students must clearly indicate the sections of the work that were generated using generative AI tools for proper attribution and transparency, and indicate the prompts and software versions that were used. It is critical to adhere to ethical standards by refraining from activities like plagiarism or creating misleading content. Additional guidelines or restrictions will be provided for specific assignments.""",,
Vedant,89,Southern Methodist University,Q1536258,"12,053 (Fall 2022)","7,056 (Fall 2022)","4,997 (Fall 2022)",R2,"32.844114,-96.784874",Dallas,TX,,http://www.smu.edu,,"Faculty may have several worries about students using Chat GPT, including:

Academic Dishonesty: One of the biggest concerns is that students may use Chat GPT to cheat on assignments, tests, or exams by relying on it to generate answers for them. This can undermine the academic integrity of the institution and negatively impact the learning outcomes for both the students and the faculty.
Lack of Critical Thinking: Another worry is that students may become too reliant on Chat GPT and may not develop critical thinking skills or the ability to solve problems independently. This can limit their creativity, problem-solving skills, and ability to think for themselves, which are essential skills for success in many fields.
Limited Understanding: While Chat GPT can provide helpful information, it may not be able to provide the depth and breadth of knowledge that students need to fully understand complex topics or concepts. Relying too heavily on Chat GPT can lead to a limited understanding of the subject matter.
Bias: Another concern is that Chat GPT may reflect biases and stereotypes present in the data it was trained on, which could perpetuate inequalities or reinforce problematic beliefs or assumptions.
Privacy: Finally, faculty may worry about the privacy implications of using Chat GPT. Depending on how it is used and implemented, it may collect and store data about students and their interactions, which could be a potential privacy risk.",,
Vedant,89,Temple University,Q1420239,"37,365 (Fall 2020)","27,307 (Fall 2020)","10,058 (Fall 2020)",R1,"39.981388888889,-75.154444444444",Philadelphia,PA,,https://www.temple.edu/,https://teaching.temple.edu/faculty-guide-ai,"Provost Mandel made clear that, ultimately, it will be important for faculty to determine the appropriate use of AI tools in their classrooms. However, the letter also clarified the following policies for fall 2023:


“Because we will all need time to fully explore the capabilities of generative AI tools, for the Fall 2023 semester, Temple has established a blanket policy that the use of generative AI tools is prohibited for students, unless an instructor explicitly grants permission. Students will be informed of this policy before the start of the semester, and Temple’s revised Student Code of Conduct now explicitly defines unauthorized use of generative AI tools as academic misconduct. In addition, you should add a statement to your syllabus outlining your stance on students’ potential use of such tools and discuss your decision with your students. These model syllabus statements, which can be adopted or modified as needed, have been developed to assist you in articulating this decision.

Temple has acquired a university-wide license for Turnitin’s recently released tool designed to detect AI-generated text (this is a different tool than the plagiarism detection tool that Temple has used for years). This AI-detection tool should be used with an abundance of caution, as it is often inaccurate and does not allow the ability to verify its results. However, it appears to be better than other tools available at this time. For Fall 2023, Turnitin’s AI detector tool will be available for you if you wish to use it. You must request access and complete a brief asynchronous training before the tool will be enabled. The training will provide guidance on how to interpret inaccurate and ambiguous results. Please login at tuhelp.temple.edu and select the ""Turnitin AI Detector"" option to submit your request.”",,
Vedant,93,University of Iowa,Q182973,"30,015 (Fall 2022)","21,973 (Fall 2022)","6,156 (Fall 2022)",R1,"41.66166666666667,-91.5363888888889",Iowa City,IA,,https://uiowa.edu,https://teach.its.uiowa.edu/artificial-intelligence-tools-and-teaching,"Artificial Intelligence Tools and Teaching
Print 
The newest entry in a long line of technologies that promise to disrupt higher education are artificial intelligence (AI) tools like ChatGPT. In seconds, ChatGPT uses machine learning to generate human-like text in response to users’ prompts, from brief social media posts to essays.

The technology continues to advance, with features such as internet browser plugins that can generate answers to assessment and homework questions. Computer generated text does have its limitations. Users can’t rely on the tool to make qualitative judgments, such as determining if language is appropriate for a given context, or rely on the accuracy of content citations.

Students have the opportunity to engage with these tools, and instructors will need to consider carefully how to adapt to these developments. Amid discussions and academic integrity concerns about these tools, the Office of Teaching Learning, and Technology offers this brief guide to address some of the most frequently asked questions about ChatGPT and other AI tools.

These are new technologies, and the future is unknown as the availability and reliability of these tools are developing. If you have questions about AI’s impact on your teaching, you can request a one-on-one consultation with staff from the Office of Teaching, Learning, and Technology. 

For more information about AI at Iowa, review guidance published by the Office of the Provost. 

If you have questions about the risks related to security, privacy, and ethical considerations, or how to assess a given implementation of AI, please contact it-security@uiowa.edu.

FAQ
Frequently Asked Questions 
What do I put in my syllabus about AI-generated and other externally generated content?
You may need to discuss using AI tools in a variety of contexts, including learning materials and campus, collegiate, and course policies related to academic integrity. Consult with your collegiate leadership about specific policies. In any case, providing transparent information about expectations for student use of AI tools and how these expectations align with course goals and scholarly values is crucial.

Remember that with any policy in your syllabus, it’s important to have ongoing conversations throughout the semester. 

Some example language:  
When AI is prohibited. [This course] assumes that work submitted by students—all process work, drafts, low-stakes writing, final versions, and all other submissions—will be generated by the students themselves, working individually or in groups. This means that the following would be considered violations of academic integrity: a student has another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write essays and drafts and/or other assignments, research-based or otherwise, and using artificial intelligence affordances like ChatGPT. (Excerpted from ChatGPT by University of California: Irvine Division of Teaching Excellence and Innovation)
When AI is prohibited. Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student. Developing strong competencies in this area will prepare you for a competitive workplace. Therefore, AI-generated submissions are not permitted and will be treated as plagiarism. (Sample statement shared by Chrissann Sparks Ruehle, with permission for others to use, on Higher Ed Discussions of AI Writing Facebook Group on 1/6/2023, cited in ChatGPT Resources by Texas Tech University Teaching, Learning & Professional Development Center)
When AI is allowed with attribution. In all academic work, the ideas and contributions of others must be appropriately acknowledged and work that is presented as original must be, in fact, original. Using an AI-content generator (such as ChatGPT) to complete coursework without proper attribution or authorization is a form of academic dishonesty. If you are unsure about whether something may be plagiarism or academic dishonesty, please contact your instructor to discuss the issue. Faculty, students, and administrative staff all share the responsibility of ensuring the honesty and fairness of the intellectual environment. (Excerpted from Constructing a Syllabus: A Checklist by Washington University in St. Louis Center for Teaching and Learning)
When AI is allowed with attribution. Use of AI tools, including ChatGPT, is permitted in this course for students who wish to use them. To be consistent with our scholarly values, students must cite any AI-generated material that informed their work and use quotation marks or other appropriate indicators of quoted material when appropriate. Students should indicate how AI tools informed their process and the final product, including how you validated any AI-generated citations, which may be invented by the AI. Assignment guidelines will provide additional guidance as to how these tools might be part of your process for each assessment this semester and how to provide transparency about their use in your work.
 When AI use is encouraged with certain tasks. Students are invited to use AI platforms to help prepare for assignments and projects (e.g., to help with brainstorming or to see what a completed essay might look like). I also welcome you to use AI tools to help revise and edit your work (e.g., to help identify flaws in reasoning, spot confusing or underdeveloped paragraphs, or to simply fix citations). When submitting work, students must clearly identify any writing, text, or media generated by AI. This can be done in a variety of ways. In this course, parts of essays generated by AI should appear in a different colored font, and the relationship between those sections and student contributions should be discussed in cover letters that accompany the essay submission. (Based on Course Policies related to ChatGPT and other AI Tools by Joel Gladd) ",,
Vedant,98,Drexel University,Q603034,"24,205","15,346","8,859",R1,"39.956441,-75.188686",Philadelphia,PA,College of Computing and Informatics,https://drexel.edu/,https://drexel.edu/provost/policies-calendars/policies/academic_integrity_artificial_intelligence/,"STATEMENT OF POLICY AND PROCEDURE
A. BACKGROUND
The following points are background for these policy:

1) VALUE OF ARTIFICIAL INTELLIGENCE IN EDUCATION.
Artificial Intelligence Tools are, and will increasingly be, a vital component of many academic and professional disciplines. Academic units have integrated Artificial Intelligence tools into their courses and curricula, and the University as a whole recognizes the enormous pedagogical value of these tools.

2) RAPIDLY CHANGING LANDSCAPE OF ARTIFICIAL INTELLIGENCE TOOLS.
The landscape of Artificial Intelligence Tools is rapidly changing, including the names of the most commonly used tools, the wide variety of contexts and uses in which the tools are employed, and the scope and efficacy of these tools. Consequently, no attempt is made to catalog or taxonomize this landscape. Rather, Appendix A “AI Tools: Scope and Use at Drexel University” presents a partial snapshot of the current landscape, with the recognition this snapshot will require regular updates to maintain relevance.

3) PRIMACY OF INSTRUCTOR PREFERENCE ON SUITABLE USE OF ARTIFICIAL INTELLIGENCE TOOLS IN COURSEWORK.
The broad scope of the University's education and learning enterprise, including breadth of topic, level, pedagogy, and mode, as well as the broad scope of use of Artificial Intelligence Tools in education and learning naturally lead to a broad scope for the suitable use of these tools in the classroom. Consequently, a foundational component of this policy is that instructors have broad discretion to define the suitable use of Artificial Intelligence Tools in the classroom.

B. POLICY ON FACULTY RIGHTS AND RESPONSIBILITIES FOR USE OF AI.
1) RIGHT OF INSTRUCTOR TO SPECIFY PERMITTED USE OF AI.
It is the right of the course instructor to decide whether AI Tools are to be permitted in the course and, if they are allowed, to describe precisely the conditions and criteria for use. Certain limited exceptions may apply for students with disabilities consistent with ODR-1 and applicable law. Students requesting or requiring accommodations are encouraged to contact the Office of Disability Resources.

2) RESPONSIBILITY OF INSTRUCTOR TO COMMUNICATE PERMITTED USE OF AI.
It is the responsibility of the instructor to include in the course syllabus a clearly written description of the permitted use of AI tools. It is strongly recommended that the instructor discuss this written description at the beginning of the course and as needed throughout the term. It is also expected that faculty describe precisely how students are expected to attribute / cite the use of AI tools. To assist faculty with communicating which AI tools are permitted in a course, Appendix A (“AI Tools: Scope and Use at Drexel University”) provides a checklist of common AI tools that faculty can customize based on their preference and include in the syllabus, and Appendix C (“AI Tools: Sample Syllabi Language”) provides language instructors may use in their syllabus to summarize their course policy on the use of AI.

3) DEFAULT POLICIES ON COURSE-RELATED USE OF AI.
Given the wide and dynamic landscape of AI Tools available for use by students in coursework, it is NOT required for the instructor to specify permitted use of every such tool. Instead, wherever there is a lack of clarity or specification, the default policy on the use of AI Tools in courses is that such tools are not permitted. Below are three important exceptions to this statement:

a. All databases and research tools provided by Drexel University Libraries are approved for use in coursework, unless such usage contradicts instructor guidance on coursework completion.

b. Appendix A (“AI Tools: Scope and Use at Drexel University”) includes a list of AI tools that are approved for use in coursework by default, unless such usage contradicts instructor guidance on coursework completion.

c. Certain exceptions will apply for students with disabilities. Students requesting or requiring accommodations are encouraged to contact the Office of Disability Resources.

C. POLICY ON STUDENT RIGHTS AND RESPONSIBILITIES FOR USE OF AI.
1) RESPONSIBILITY OF STUDENT TO ADHERE TO INSTRUCTOR SPECIFIED USE OF AI.
It is the responsibility of the student to read, understand, and seek clarification from the instructor where necessary on the instructor's written and verbal descriptions of the acceptable use of AI in the course. If usage of AI Tool(s) is permitted by the instructor, students are obligated to follow the instructor's guidance regarding the nature of that usage.

2) RESPONSIBILITY OF STUDENT TO CITE USAGE OF AI.
If usage of AI Tool(s) is permitted by the instructor, students are obligated to follow the instructor's guidance regarding if and how that usage is to be attributed/cited in the submitted work. If no attribution / citation guidance is given by the instructor, the students should adopt the style typically used in the discipline most closely aligned with the course. Appendix B (“AI Tools: Citation of Use at Drexel University”) aims to assist students with the various citation rules and styles for AI and AI Tools.

3) RESPONSIBILITY OF STUDENT FOR SUBMITTED WORK GENERATED BY AI.
If usage of AI Tool(s) is permitted by the instructor, the final work product that is submitted is nonetheless the responsibility of the student. It is important that students are aware that AI generated content may be false or biased, and students assume ownership of and responsibility for submitted work that may be violative of other university policies. Inappropriate or offensive content may be reported to Student Conduct & Care or other relevant university offices for consideration of appropriate next steps, if any.

D. USE AND MISUSE OF AI DETECTION TOOLS.
The purpose of this section is to clarify the use and potential misuse of so-called “AI Detection Tools.” These tools aim to assess whether or not AI or an AI Tool has been used to create a digital object (e.g., text, audio, image, video); in this context this applies to any and all student submitted work. There is no restriction on the use of such tools by the instructor, but the instructor is urged to be aware of the potential misuse or misinterpretation of the outputs presented by these tools.

1) FALLIBILITY OF AI DETECTION TOOLS.
Such tools are often fallible; they may both assert the digital object was created by AI when it was not or assert the digital object was not created by AI when it was. Such tools often advertise “error rates” to describe the frequency or likelihood of these mistakes.

a. Users of AI Detection Tools should be aware that the method used by these tools to determine if AI was used is particularly likely to yield false positives in the cases of neurodivergent students and students for whom English is not their first language.

b. Recent research also indicates that AI Detection Tools yield false negatives where students use paraphrasing tools or edit the document after using AI.

2) MISESTIMATION OF FALLIBILITY OF AI DETECTION TOOLS.
The creators and distributors of such tools may misestimate the fallibility of the tool. Instructors are advised to exercise caution regarding the asserted level of accuracy.

3) ABSENCE OF PROOF FOR AI DETECTION TOOLS.
In the context of plagiarism, for which the same or similar tools are often used, the tool is often able to provide “proof” to accompany an assertion of plagiarism in the form of demonstrating similarity between the object being assessed (i.e., the work submitted by a student) and another object (i.e., the reference work from which the student's submission was taken or adapted). In contrast, it is often the case that no such “proof” is available from AI Detection Tools.

4) GUIDANCE ON THE USE OF AI DETECTION TOOLS.
Due to the possible fallibility, possible misestimation of fallibility, and possible absence of proof to accompany assertions of AI use by AI Detection Tools, it is recommended that instructors use such tools with great caution and with full recognition of their limitations. In general, the “evidence” provided by such tools should be only part of an instructor’s case when accusing a student of academic misconduct.

E. STUDENT CONDUCT PROCESS FOR PERCEIVED VIOLATION OF USE OF AI.
1) APPLICABILITY OF THE STUDENT CONDUCT PROCESS.
The scope of applicability of the Academic Integrity Conduct Process is in no way limited by this policy.",,
Vedant,98,Illinois Institute of Technology,Q659706,"6,943 (Fall 2022)","3,125 (Fall 2022)","3,818 (Fall 2022)",R2,"41.834652777778,-87.628333333333",Chicago,IL,,http://www.iit.edu,https://www.iit.edu/sites/default/files/2023-08/Faculty%20Guide%20on%20Teaching%20and%20Generative%20AI.pdf,"Faculty Guide on Teaching and Generative AI
Illinois Tech’s mission lends itself to authentic, student-centered teaching. In alignment with that
mission, the Center for Learning Innovation guides instructors with best practices to prepare
Illinois Tech students to critically and productively engage with new and innovative
technologies–like generative AI–to become innovators and leaders of the future. CLI hosts
events like the Learning Innovation Symposium to bring faculty together from across the
university to discuss topics like generative AI and share successful strategies related to
teaching and learning.
Best practices include guidance and resources on student-centered teaching with AI, from
developing syllabus language, aligning the use of generative AI to course learning objectives
and developing assessments. Academic Affairs, the Galvin Library, CAC and the Center for
Ethics in the Profession also have a number of resources on generative AI. As generative AI
continues to evolve, CLI will continue to identify, curate, develop, and distribute best practices to
support instructors and student success at Illinois Tech.
Guiding principles to inform best practice in teaching with generative AI
? Clear and frequent communication with students is essential. Students need
transparency when it comes to instructor expectations and the use of AI in the
classroom. Expectations around AI use will vary from class to class. Frequent
discussions about instructor expectations, course learning objectives, and the
relationship between them with the related learning activities and student
work—including the use of generative AI—enhances student learning experiences and
minimizes opportunities for misunderstanding or misuse.
? Generative AI systems are technology tools. And like other technology tools like
Blackboard, Zoom, and even Google search, generative AI can be used to positively
support rigorous learning and enhance engaging learning experiences.
? The use of generative AI will evolve. Instructors and students should use generative
AI responsibly, purposefully, and ethically.
? If supported by the course learning objectives, instructors should design
assessments and learning activities in a way that students can use generative AI
as opportunities to learn. Students can better achieve their course learning objective
and learn more about the benefits and challenges when using generative AI.
1 of 4
Best practices to consider as you plan your course in light of generative AI
? Use backwards design concepts to align course learning objectives to assessments and
related active learning activities. Doing so can reveal how generative AI could be used to
make a positive impact in your course.
? Post clear expectations on the use of AI in your course syllabus. Academic Affairs
provides some brief examples of possible language. Externally, many additional
examples are being collected by Lance Eaton from instructors around the country and
posted in an open Google Doc; WCET highlighted 3:
No use of AI “All work submitted in this course must be your own. Contributions from anyone
or anything else- including AI sources, must be properly quoted and cited every
time they are used. Failure to do so constitutes an academic integrity violation,
and I will follow the institution’s policy to the letter in those instances.”
A theater course at a small liberal arts college.
Some use of AI “You might be permitted to use generative AI tools for specific assignments or
class activities. However, assignments created with AI should not exceed 25%
of the work submitted and must identify the AI-generated portions. Presenting
AI-generated work as your own will have consequences according to university
policies. Importantly, while AI programs like ChatGPT can help with idea
generation, they are not immune to inaccuracies and limitations. Further,
overreliance on AI can hinder independent thinking and creativity. Note that, in
the spirit of this policy, it was written in part by ChatGPT.”
A marketing course at a public university
Significant Use of AI “Within this course, you are welcome to use generative artificial intelligence (AI)
models (ChatGPT, DALL-E, GitHub Copilot, and anything after) with
acknowledgment. However, you should note that all large language models
have a tendency to make up incorrect facts and fake citations, they may
perpetuate biases, and image generation models can occasionally come up
with offensive products. You will be responsible for any inaccurate, biased,
offensive, or otherwise unethical content you submit regardless of whether it
originally comes from you or an AI model.
If you use an AI model, its contribution must be cited and discussed:
· What was your prompt?
· Did you revise the Ai model’s original output for your submission?
· Did you ask follow-up questions?
· What did you learn?
Having said all these disclaimers, the use of AI models is encouraged, as it may
make it possible for you to submit assignments and your work in the field with
higher quality and in less time.”
A graduate level library sciences course at a public university
? If you use text-matching software or anti-plagiarism checkers such as Blackboard’s
SafeAssign or IIT’s own chatGPT detector, first, be sure to post that notice in your
syllabus.Then encourage your students to use the tools as well. These tools compare
submitted work against work in its own databases (typically other users in the same
class or institution) and sometimes around the web. Tools may also report if the text
resembles or is generated by generative AI. Students can use the tool as a means to
ensure compliance with the class's policy on academic honesty and the use of
2 of 4
generative AI. Also mention to your students that in cases of possible infringement of
your course policy, a result from one or more of these tools may be used as
circumstantial evidence to report the student's infringement to the DDAD. Finally, do
keep in mind that the results of these tools only reveal similarity (ie, drafts and final
versions of work submitted may result in 100% similarity). Reports based on these tools
are typically the starting point for additional conversations before investigating actual
policy infringement or actual plagiarism.
? If you embrace the use of generative AI in your course, avoid making its use a
requirement, unless you plan to also offer alternative ways to achieve similar objectives.
It is a question making your course accessible: not all students may have the same
access or the ability to use generative AI. Offering alternatives is also a general principle
in universal design for learning, and can help make your course more accessible for all
learners.
? Integrating AI tools, or motivating compliance with relevant rationale, can be more
effective than banning, restricting and detecting AI-generated content with technology
tools.
? Like many other software/apps available on the web (free or not), access to AI-based
tools requires users to agree with specific terms of service and privacy policies.
Instructors and students should carefully read these documents and understand the risks
associated with their use, prior to accepting the term",,
Vedant,98,University of Oregon,Q766145,,,,R1,"44.044166666667,-123.07583333333",Eugene,OR,,http://www.uoregon.edu,https://teaching.uoregon.edu/teaching-and-generative-ai#activities,"Guiding Principles 
Learning-Centered: Like any tool students might use to engage in the work of a course—from library books to research databases to internet search engines—GenAI systems present opportunities for students to learn important skills, including creativity, critical thinking, ethical decision-making, and discerning use of resources, among others. We encourage instructors to talk explicitly with students about the pluses and minuses of GenAI systems as they help or hinder learning in a course.  In this way, regardless of one's view of GenAI or concern with its implications, the emphasis is on learning and what might assist or inhibit the learning process.

For example, if a course learning objective is to help students develop their own voices and perspectives through reflective writing, then use of GenAI systems to produce text that ostensibly represents ""personal voice"" or ""perspective"" might undermine that objective. Alternatively, if a learning objective is to help students structure and refine the presentation of arguments in writing, a GenAI system might play a supportive role in the process of students' learning how to organize an argument in written form (which is not the same as a student trying to use a GenAI system to formulate the substantive claim—with accompanying reasoning and evidence—that is being structured in written form). 

Transparent: Faculty and GE instructors have flexibility in how they approach newly available GenAI tools, and this means students will shift among a variety of approaches in their different courses.  It is therefore essential for instructors to prioritize transparency so that students know clearly the GenAI policy in each individual class and the expectations for specific course activities and assignments, especially if the parameters for GenAI use vary among the latter.  In addition to including a GenAI course policy on the syllabus and explicit details about GenAI use in assignment instructions, we encourage faculty and GE instructors to talk explicitly with students about the rationale for their policy and expectations, including the relationship to students’ learning, as suggested above.

Course Policies
We strongly encourage instructors to have an explicit policy about GenAI in their course syllabus, including any relevant distinctions between GenAI use (as process) and GenAI content (as product). We also encourage instructors to reinforce their expectations in assignment instructions and in conversation with students. Based on helpful considerations articulated by Josef Brandauer and Melissa Forbes at Gettysburg College, the sample course policies below indicate a range of options that instructors can adapt depending on their specific course context and student learning goals.

Sample Course Policies
We also encourage exploration of the Syllabus Resources from the Sentient Syllabus Project, which offers a variety of considerations and example policies that can be used or adapted. Instructors can also find dozens of sample course policies at this crowdsourced document, including an option to search by discipline and course topic.
Course Assignments and Class Activities
Instructors are experimenting with a variety of ways to include use of GenAI systems in their courses, including both classroom activities and assignments. Below are a few ideas to consider, including examples from UO instructors, and we also list a few ideas for those wanting to mitigate use of GenAI.

Examples from UO Instructors:
Promoting analysis and critical thinking:
Assisting research and brainstorming:
Contributing to writing and revision:
Facilitating icebreakers:
Mitigating use of GenAI:
Have students reference course-specific moments and materials such as lectures, discussions, labs, notes, handouts, or sources not otherwise available on the internet 
Have students respond to image-based or sound-based texts in your assignments, albeit be certain to include alt-text for accessibility.  
Ask students to apply or connect personal experience and knowledge in relation to key concepts or topics. 
Have students use social annotation tools such as Hypothes.is or Perusall to engage with texts. 
Consider having students submit audio files, a podcast, a video, infographics, or other multimedia texts instead of written essays. 
Chunk major assignments, such as essays, into multiple due dates for key steps, such as an outline, notes on sources, drafts, etc. 
You can find additional ideas for assignments and activities in this resource on ChatGPT Assignments to Use in Your Classroom Today, as well as this resource on Teaching with Text Generation Technologies.  Also consider this helpful decision tree for reviewing your course assignments, at the University of Michigan Course and Assignment (Re-)Design page.
Data Privacy and Security Considerations
UO Information Services reminds all faculty, staff, and students that: 

The University of Oregon does not have a contract with popular AI systems such as ChatGPT, Bard and DALL•E2, which would include the security compliance assessment of such systems to gauge whether they satisfy requirements associated with the university academic, operational, business and research needs. In addition, the privacy policies associated with the use of these systems, requires the users acknowledge that the data input into the AI system will remain a part of the environment. As such we recommend that users exercise caution when interacting with these systems, to avoid unintended release of intellectual property, copyrighted materials or trade secrets. 

We therefore strongly recommend that instructors who ask or encourage students to use any AI system remind students that they should avoid providing any personal or other sensitive data to AI prompts. We also advise that instructors consider making AI use voluntary or, if AI use is part of a required course assignment or activity, include an opt-out alternative for students who do not want to create an account with an AI system or interact with them. Such a recommendation is in alignment with UO’s policy on external vendor digital tools, for instance use of social media such as blogs as part of course assignments, which must include an option for students to keep their information and identities private.  ",,
